<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RedLeavesBlog</title>
  
  
  <link href="http://www.red0819.top/atom.xml" rel="self"/>
  
  <link href="http://www.red0819.top/"/>
  <updated>2020-09-14T22:16:15.007Z</updated>
  <id>http://www.red0819.top/</id>
  
  <author>
    <name>Snow Monster</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Redis主从复制机制</title>
    <link href="http://www.red0819.top/2020/09/15/Redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%9C%BA%E5%88%B6/"/>
    <id>http://www.red0819.top/2020/09/15/Redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%9C%BA%E5%88%B6/</id>
    <published>2020-09-14T18:19:27.000Z</published>
    <updated>2020-09-14T22:16:15.007Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>test1</title>
    <link href="http://www.red0819.top/2020/09/14/test1/"/>
    <id>http://www.red0819.top/2020/09/14/test1/</id>
    <published>2020-09-14T10:25:30.000Z</published>
    <updated>2020-09-14T19:56:05.430Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 简单示例 (id, server, type)  --&gt;</span><br><span class="line">&#123;% meting &quot;60198&quot; &quot;netease&quot; &quot;playlist&quot; %&#125;</span><br></pre></td></tr></table></figure><!-- 进阶示例 -->    <div id="aplayer-lByhHfAm" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="1303019637" data-server="netease" data-type="playlist" data-mode="circulation" data-autoplay="true" data-mutex="false" data-listmaxheight="340px" data-preload="none" data-theme="#ad7a86"></div><p>我らが征くは星の大海</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>主流ETL清洗工具</title>
    <link href="http://www.red0819.top/2020/09/12/%E4%B8%BB%E6%B5%81ETL%E6%B8%85%E6%B4%97%E5%B7%A5%E5%85%B7/"/>
    <id>http://www.red0819.top/2020/09/12/%E4%B8%BB%E6%B5%81ETL%E6%B8%85%E6%B4%97%E5%B7%A5%E5%85%B7/</id>
    <published>2020-09-12T05:55:24.000Z</published>
    <updated>2020-09-14T21:08:40.149Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>OLAP和OLTP</title>
    <link href="http://www.red0819.top/2020/09/12/OLAP%E5%92%8COLTP/"/>
    <id>http://www.red0819.top/2020/09/12/OLAP%E5%92%8COLTP/</id>
    <published>2020-09-12T05:55:06.000Z</published>
    <updated>2020-09-14T21:08:07.106Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>主流OLAP系统对比总结</title>
    <link href="http://www.red0819.top/2020/09/12/%E4%B8%BB%E6%B5%81OLAP%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93/"/>
    <id>http://www.red0819.top/2020/09/12/%E4%B8%BB%E6%B5%81OLAP%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93/</id>
    <published>2020-09-12T05:53:37.000Z</published>
    <updated>2020-09-14T21:08:27.268Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>基础SQL题（来自牛客网）</title>
    <link href="http://www.red0819.top/2020/09/09/%E5%9F%BA%E7%A1%80SQL%E9%A2%98%EF%BC%88%E6%9D%A5%E8%87%AA%E7%89%9B%E5%AE%A2%E7%BD%91%EF%BC%89/"/>
    <id>http://www.red0819.top/2020/09/09/%E5%9F%BA%E7%A1%80SQL%E9%A2%98%EF%BC%88%E6%9D%A5%E8%87%AA%E7%89%9B%E5%AE%A2%E7%BD%91%EF%BC%89/</id>
    <published>2020-09-09T13:37:10.000Z</published>
    <updated>2020-09-14T21:08:55.885Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="基础SQL"><a href="#基础SQL" class="headerlink" title="基础SQL"></a>基础SQL</h1><h2 id="查找最晚入职员工的所有信息"><a href="#查找最晚入职员工的所有信息" class="headerlink" title="查找最晚入职员工的所有信息"></a>查找最晚入职员工的所有信息</h2><p>查找最晚入职员工的所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天(sqlite里面的注释为–,mysql为comment)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE &#96;employees&#96; (</span><br><span class="line">&#96;emp_no&#96; int(11) NOT NULL, -- &#39;员工编号&#39;</span><br><span class="line">&#96;birth_date&#96; date NOT NULL,</span><br><span class="line">&#96;first_name&#96; varchar(14) NOT NULL,</span><br><span class="line">&#96;last_name&#96; varchar(16) NOT NULL,</span><br><span class="line">&#96;gender&#96; char(1) NOT NULL,</span><br><span class="line">&#96;hire_date&#96; date NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;emp_no&#96;));</span><br></pre></td></tr></table></figure><h4 id="输入描述"><a href="#输入描述" class="headerlink" title="输入描述"></a>输入描述</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">无</span><br></pre></td></tr></table></figure><h4 id="输出描述"><a href="#输出描述" class="headerlink" title="输出描述"></a>输出描述</h4><p>示例</p><table><thead><tr><th>emp_no</th><th>birth_date</th><th>first_name</th><th>last_name</th><th>gender</th><th>hire_date</th></tr></thead><tbody><tr><td>10008</td><td>1958-02-19</td><td>Saniya</td><td>Kalloufi</td><td>M</td><td>1994-09-15</td></tr></tbody></table><h3 id="我的答案"><a href="#我的答案" class="headerlink" title="我的答案"></a>我的答案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from employees where hire_date &#x3D; (select max(hire_date) from employees );</span><br></pre></td></tr></table></figure><p>运行时间: 17ms 占用内存: 3320KB</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from employees order by hire_date desc limit 1</span><br></pre></td></tr></table></figure><p>运行时间: 11ms 占用内存: 3320KB</p><h3 id="最优答案"><a href="#最优答案" class="headerlink" title="最优答案"></a>最优答案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM employees</span><br><span class="line">WHERE hire_date &#x3D; (SELECT MAX(hire_date) FROM employees)</span><br></pre></td></tr></table></figure><p>用户：ktktktkt 运行时间: 10ms 占用内存: 3192KB</p><h2 id="查找入职员工时间排名倒数第三的员工所有信息"><a href="#查找入职员工时间排名倒数第三的员工所有信息" class="headerlink" title="查找入职员工时间排名倒数第三的员工所有信息"></a>查找入职员工时间排名倒数第三的员工所有信息</h2><p>查找入职员工时间排名倒数第三的员工所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE &#96;employees&#96; (</span><br><span class="line">&#96;emp_no&#96; int(11) NOT NULL,</span><br><span class="line">&#96;birth_date&#96; date NOT NULL,</span><br><span class="line">&#96;first_name&#96; varchar(14) NOT NULL,</span><br><span class="line">&#96;last_name&#96; varchar(16) NOT NULL,</span><br><span class="line">&#96;gender&#96; char(1) NOT NULL,</span><br><span class="line">&#96;hire_date&#96; date NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;emp_no&#96;));</span><br></pre></td></tr></table></figure><h4 id="输入描述-1"><a href="#输入描述-1" class="headerlink" title="输入描述"></a>输入描述</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">无</span><br></pre></td></tr></table></figure><h4 id="输出描述-1"><a href="#输出描述-1" class="headerlink" title="输出描述"></a>输出描述</h4><p>示例1</p><table><thead><tr><th>emp_no</th><th>birth_date</th><th>first_name</th><th>last_name</th><th>gender</th><th>hire_date</th></tr></thead><tbody><tr><td>10005</td><td>1955-01-21</td><td>Kyoichi</td><td>Maliniak</td><td>M</td><td>1989-09-12</td></tr></tbody></table><h3 id="我的答案-1"><a href="#我的答案-1" class="headerlink" title="我的答案"></a>我的答案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from employees </span><br><span class="line">where hire_date &#x3D; (select hire_date from employees order by hire_date desc limit 2,1</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>运行时间: 12ms 占用内存: 3364KB</p><h3 id="最优答案-1"><a href="#最优答案-1" class="headerlink" title="最优答案"></a>最优答案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select *</span><br><span class="line"> &#96;&#96;from employees</span><br><span class="line"> &#96;&#96;order by hire_date desc</span><br><span class="line"> &#96;&#96;limit &#96;&#96;2&#96;&#96;,&#96;&#96;1&#96;&#96;;</span><br></pre></td></tr></table></figure><p>用户：Howedata 运行时间: 10ms 占用内存: 3284KB</p><h2 id="查找当前薪水详情以及部门编号"><a href="#查找当前薪水详情以及部门编号" class="headerlink" title="查找当前薪水详情以及部门编号"></a>查找当前薪水详情以及部门编号</h2><p>查找最晚入职员工的所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天(sqlite里面的注释为–,mysql为comment)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE &#96;employees&#96; (</span><br><span class="line">&#96;emp_no&#96; int(11) NOT NULL, -- &#39;员工编号&#39;</span><br><span class="line">&#96;birth_date&#96; date NOT NULL,</span><br><span class="line">&#96;first_name&#96; varchar(14) NOT NULL,</span><br><span class="line">&#96;last_name&#96; varchar(16) NOT NULL,</span><br><span class="line">&#96;gender&#96; char(1) NOT NULL,</span><br><span class="line">&#96;hire_date&#96; date NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;emp_no&#96;));</span><br></pre></td></tr></table></figure><h4 id="输入描述-2"><a href="#输入描述-2" class="headerlink" title="输入描述"></a>输入描述</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">无</span><br></pre></td></tr></table></figure><h4 id="输出描述-2"><a href="#输出描述-2" class="headerlink" title="输出描述"></a>输出描述</h4><p>示例</p><table><thead><tr><th>emp_no</th><th>salary</th><th>from_date</th><th>to_date</th><th>dept_no</th></tr></thead><tbody><tr><td>10002</td><td>72527</td><td>2001-08-02</td><td>9999-01-01</td><td>d001</td></tr><tr><td>10004</td><td>74057</td><td>2001-11-27</td><td>9999-01-01</td><td>d004</td></tr><tr><td>10005</td><td>94692</td><td>2001-09-09</td><td>9999-01-01</td><td>d003</td></tr><tr><td>10006</td><td>43311</td><td>2001-08-02</td><td>9999-01-01</td><td>d002</td></tr><tr><td>10010</td><td>94409</td><td>2001-11-23</td><td>9999-01-01</td><td>d006</td></tr></tbody></table><h3 id="我的答案-2"><a href="#我的答案-2" class="headerlink" title="我的答案"></a>我的答案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select sa.emp_no , sa.salary ,sa.from_date ,sa.to_date ,de.dept_no</span><br><span class="line">from </span><br><span class="line">salaries sa join dept_manager de on </span><br><span class="line">sa.emp_no &#x3D; de.emp_no</span><br><span class="line">where</span><br><span class="line">sa.to_date&#x3D;&#39;9999-01-01&#39; and de.to_date&#x3D;&#39;9999-01-01&#39;</span><br><span class="line">order by </span><br><span class="line">sa.emp_no;</span><br></pre></td></tr></table></figure><p>运行时间:12ms 占用内存: 3344KB</p><h3 id="最优答案-2"><a href="#最优答案-2" class="headerlink" title="最优答案"></a>最优答案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select salaries.*, dept_manager.dept_no from </span><br><span class="line">salaries,dept_manager on salaries.emp_no &#x3D; dept_manager.emp_no </span><br><span class="line">where salaries.to_date&#x3D;&#39;9999-01-01&#39; and dept_manager.to_date&#x3D;&#39;9999-01-01&#39;;</span><br></pre></td></tr></table></figure><p>用户：牛客326549353号 运行时间: 11ms 占用内存: 3248KB</p><h2 id="查找所有已经分配部门的员工的last-name和first-name以及dept-no"><a href="#查找所有已经分配部门的员工的last-name和first-name以及dept-no" class="headerlink" title="查找所有已经分配部门的员工的last_name和first_name以及dept_no"></a>查找所有已经分配部门的员工的last_name和first_name以及dept_no</h2><p>查找最晚入职员工的所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天(sqlite里面的注释为–,mysql为comment)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE &#96;employees&#96; (</span><br><span class="line">&#96;emp_no&#96; int(11) NOT NULL, -- &#39;员工编号&#39;</span><br><span class="line">&#96;birth_date&#96; date NOT NULL,</span><br><span class="line">&#96;first_name&#96; varchar(14) NOT NULL,</span><br><span class="line">&#96;last_name&#96; varchar(16) NOT NULL,</span><br><span class="line">&#96;gender&#96; char(1) NOT NULL,</span><br><span class="line">&#96;hire_date&#96; date NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;emp_no&#96;));</span><br></pre></td></tr></table></figure><h4 id="输入描述-3"><a href="#输入描述-3" class="headerlink" title="输入描述"></a>输入描述</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">无</span><br></pre></td></tr></table></figure><h4 id="输出描述-3"><a href="#输出描述-3" class="headerlink" title="输出描述"></a>输出描述</h4><p>示例</p><table><thead><tr><th>last_name</th><th>first_name</th><th>dept_no</th></tr></thead><tbody><tr><td>Facello</td><td>Georgi</td><td>d001</td></tr><tr><td>省略</td><td>省略</td><td>省略</td></tr><tr><td>Piveteau</td><td>Duangkaew</td><td>d006</td></tr></tbody></table><h3 id="我的答案-3"><a href="#我的答案-3" class="headerlink" title="我的答案"></a>我的答案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select employees.last_name,employees.first_name,dept_emp.dept_no </span><br><span class="line">from </span><br><span class="line">employees inner join dept_emp </span><br><span class="line">on employees.emp_no&#x3D;dept_emp.emp_no</span><br></pre></td></tr></table></figure><p>运行时间:12ms 占用内存: 3300KB</p><h3 id="最优答案-3"><a href="#最优答案-3" class="headerlink" title="最优答案"></a>最优答案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select employees.last_name,employees.first_name,dept_emp.dept_no </span><br><span class="line">from employees inner join dept_emp</span><br><span class="line">on employees.emp_no&#x3D;dept_emp.emp_no</span><br></pre></td></tr></table></figure><p>用户：牛客180011609号 运行时间: 10ms 占用内存: 3300KB</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="SQL基础" scheme="http://www.red0819.top/tags/SQL%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Oozie</title>
    <link href="http://www.red0819.top/2020/09/06/Oozie/"/>
    <id>http://www.red0819.top/2020/09/06/Oozie/</id>
    <published>2020-09-06T06:44:01.087Z</published>
    <updated>2020-09-14T21:08:20.683Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="第1章-Oozie简介"><a href="#第1章-Oozie简介" class="headerlink" title="第1章 Oozie简介"></a>第1章 Oozie简介</h1><p>Oozie英文翻译为：驯象人。一个基于工作流引擎的开源框架，由Cloudera公司贡献给Apache，提供对Hadoop<br>MapReduce、Pig Jobs的任务调度与协调。Oozie需要部署到Java<br>Servlet容器中运行。主要用于定时调度任务，多任务可以按照执行的逻辑顺序调度。</p><h1 id="第2章-Oozie的功能模块介绍"><a href="#第2章-Oozie的功能模块介绍" class="headerlink" title="第2章 Oozie的功能模块介绍"></a>第2章 Oozie的功能模块介绍</h1><h2 id="2-1模块"><a href="#2-1模块" class="headerlink" title="2.1模块"></a>2.1模块</h2><p><strong>1) Workflow</strong></p><p>顺序执行流程节点，支持fork（分支多个节点），join（合并多个节点为一个）</p><p><strong>2) Coordinator</strong></p><p>定时触发workflow</p><p><strong>3) Bundle</strong></p><p>绑定多个Coordinator</p><h2 id="2-2-Workflow常用节点"><a href="#2-2-Workflow常用节点" class="headerlink" title="2.2 Workflow常用节点"></a>2.2 Workflow常用节点</h2><p><strong>1) 控制流节点（Control Flow Nodes）</strong></p><p>控制流节点一般都是定义在工作流开始或者结束的位置，比如start,end,kill等。以及提供工作流的执行路径机制，如decision，fork，join等。</p><p><strong>2) 动作节点（Action Nodes）</strong></p><p>负责执行具体动作的节点，比如：拷贝文件，执行某个Shell脚本等等。</p><h1 id="第3章-Oozie的部署"><a href="#第3章-Oozie的部署" class="headerlink" title="第3章 Oozie的部署"></a>第3章 Oozie的部署</h1><h2 id="3-1-部署Hadoop（CDH版本的）"><a href="#3-1-部署Hadoop（CDH版本的）" class="headerlink" title="3.1 部署Hadoop（CDH版本的）"></a>3.1 部署Hadoop（CDH版本的）</h2><h3 id="3-1-2-修改Hadoop配置"><a href="#3-1-2-修改Hadoop配置" class="headerlink" title="3.1.2 修改Hadoop配置"></a>3.1.2 修改Hadoop配置</h3><p><strong>core-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Oozie Server的Hostname --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.red.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 允许被Oozie代理的用户组 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.red.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>mapred-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>yarn-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 任务历史服务 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop102:19888/jobhistory/logs/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>完成后：记得scp同步到其他机器节点</p><h3 id="3-1-3-启动Hadoop集群"><a href="#3-1-3-启动Hadoop集群" class="headerlink" title="3.1.3 启动Hadoop集群"></a>3.1.3 启动Hadoop集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hadoop-2.5.0-cdh5.3.6]$ sbin&#x2F;start-dfs.sh</span><br><span class="line"></span><br><span class="line">[red@hadoop103 hadoop-2.5.0-cdh5.3.6]$ sbin&#x2F;start-yarn.sh</span><br><span class="line"></span><br><span class="line">[red@hadoop102 hadoop-2.5.0-cdh5.3.6]$</span><br><span class="line">sbin&#x2F;mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><p>注意：需要开启JobHistoryServer, 最好执行一个MR任务进行测试。</p><h2 id="3-2-部署Oozie"><a href="#3-2-部署Oozie" class="headerlink" title="3.2 部署Oozie"></a>3.2 部署Oozie</h2><h3 id="3-2-1-解压Oozie"><a href="#3-2-1-解压Oozie" class="headerlink" title="3.2.1 解压Oozie"></a>3.2.1 解压Oozie</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 software]$ tar -zxvf</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;cdh&#x2F;oozie-4.0.0-cdh5.3.6.tar.gz -C &#x2F;opt&#x2F;module</span><br></pre></td></tr></table></figure><h3 id="3-2-2-在oozie根目录下解压oozie-hadooplibs-4-0-0-cdh5-3-6-tar-gz"><a href="#3-2-2-在oozie根目录下解压oozie-hadooplibs-4-0-0-cdh5-3-6-tar-gz" class="headerlink" title="3.2.2 在oozie根目录下解压oozie-hadooplibs-4.0.0-cdh5.3.6.tar.gz"></a>3.2.2 在oozie根目录下解压oozie-hadooplibs-4.0.0-cdh5.3.6.tar.gz</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ tar -zxvf</span><br><span class="line">oozie-hadooplibs-4.0.0-cdh5.3.6.tar.gz -C ..&#x2F;</span><br></pre></td></tr></table></figure><p>完成后Oozie目录下会出现hadooplibs目录。</p><h3 id="3-2-3-在Oozie目录下创建libext目录"><a href="#3-2-3-在Oozie目录下创建libext目录" class="headerlink" title="3.2.3 在Oozie目录下创建libext目录"></a>3.2.3 在Oozie目录下创建libext目录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ mkdir libext&#x2F;</span><br></pre></td></tr></table></figure><h3 id="3-2-4-拷贝依赖的Jar包"><a href="#3-2-4-拷贝依赖的Jar包" class="headerlink" title="3.2.4 拷贝依赖的Jar包"></a>3.2.4 拷贝依赖的Jar包</h3><p>1）将hadooplibs里面的jar包，拷贝到libext目录下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ cp -ra</span><br><span class="line">hadooplibs&#x2F;hadooplib-2.5.0-cdh5.3.6.oozie-4.0.0-cdh5.3.6&#x2F;* libext&#x2F;</span><br></pre></td></tr></table></figure><p>2）拷贝Mysql驱动包到libext目录下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ cp -a</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;mysql-connector-java-5.1.27&#x2F;mysql-connector-java-5.1.27-bin.jar</span><br><span class="line">.&#x2F;libext&#x2F;</span><br></pre></td></tr></table></figure><h3 id="3-2-5-将ext-2-2-zip拷贝到libext-目录下"><a href="#3-2-5-将ext-2-2-zip拷贝到libext-目录下" class="headerlink" title="3.2.5 将ext-2.2.zip拷贝到libext/目录下"></a>3.2.5 将ext-2.2.zip拷贝到libext/目录下</h3><p>ext是一个js框架，用于展示oozie前端页面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ cp -a</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;cdh&#x2F;ext-2.2.zip libext&#x2F;</span><br></pre></td></tr></table></figure><h3 id="3-2-6-修改Oozie配置文件"><a href="#3-2-6-修改Oozie配置文件" class="headerlink" title="3.2.6 修改Oozie配置文件"></a>3.2.6 修改Oozie配置文件</h3><p><strong>oozie-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">属性：oozie.service.JPAService.jdbc.driver</span><br><span class="line"></span><br><span class="line">属性值：com.mysql.jdbc.Driver</span><br><span class="line"></span><br><span class="line">解释：JDBC的驱动</span><br><span class="line"></span><br><span class="line">属性：oozie.service.JPAService.jdbc.url</span><br><span class="line"></span><br><span class="line">属性值：jdbc:mysql://hadoop102:3306/oozie</span><br><span class="line"></span><br><span class="line">解释：oozie所需的数据库地址</span><br><span class="line"></span><br><span class="line">属性：oozie.service.JPAService.jdbc.username</span><br><span class="line"></span><br><span class="line">属性值：root</span><br><span class="line"></span><br><span class="line">解释：数据库用户名</span><br><span class="line"></span><br><span class="line">属性：oozie.service.JPAService.jdbc.password</span><br><span class="line"></span><br><span class="line">属性值：123456</span><br><span class="line"></span><br><span class="line">解释：数据库密码</span><br><span class="line"></span><br><span class="line">属性：oozie.service.HadoopAccessorService.hadoop.configurations</span><br><span class="line"></span><br><span class="line">属性值：*= /opt/module/cdh/hadoop-2.5.0-cdh5.3.6/etc/hadoop</span><br><span class="line"></span><br><span class="line">解释：让Oozie引用Hadoop的配置文件</span><br></pre></td></tr></table></figure><h3 id="3-2-7-在Mysql中创建Oozie的数据库"><a href="#3-2-7-在Mysql中创建Oozie的数据库" class="headerlink" title="3.2.7 在Mysql中创建Oozie的数据库"></a>3.2.7 在Mysql中创建Oozie的数据库</h3><p>进入Mysql并创建oozie数据库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -uroot -p123456</span><br><span class="line"></span><br><span class="line">mysql&gt; create database oozie;</span><br></pre></td></tr></table></figure><h3 id="3-2-8-初始化Oozie"><a href="#3-2-8-初始化Oozie" class="headerlink" title="3.2.8 初始化Oozie"></a>3.2.8 初始化Oozie</h3><p><strong>1) 上传Oozie目录下的yarn.tar.gz文件到HDFS：</strong></p><p>提示：yarn.tar.gz文件会自行解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozie-setup.sh</span><br><span class="line">sharelib create -fs hdfs:&#x2F;&#x2F;hadoop102:8020 -locallib</span><br><span class="line">oozie-sharelib-4.0.0-cdh5.3.6-yarn.tar.gz</span><br></pre></td></tr></table></figure><p>执行成功之后，去50070检查对应目录有没有文件生成。</p><p><strong>2) 创建oozie.sql文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;ooziedb.sh create -sqlfile oozie.sql -run</span><br></pre></td></tr></table></figure><p><strong>3) 打包项目，生成war包</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozie-setup.sh prepare-war</span><br></pre></td></tr></table></figure><h3 id="3-2-9-Oozie的启动与关闭"><a href="#3-2-9-Oozie的启动与关闭" class="headerlink" title="3.2.9 Oozie的启动与关闭"></a>3.2.9 Oozie的启动与关闭</h3><p>启动命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozied.sh start</span><br></pre></td></tr></table></figure><p>关闭命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozied.sh stop</span><br></pre></td></tr></table></figure><h3 id="3-2-10-访问Oozie的Web页面"><a href="#3-2-10-访问Oozie的Web页面" class="headerlink" title="3.2.10 访问Oozie的Web页面"></a>3.2.10 访问Oozie的Web页面</h3><p><a href="http://hadoop102:11000/oozie" target="_blank" rel="noopener">http://hadoop102:11000/oozie</a></p><h1 id="第4章-Oozie的使用"><a href="#第4章-Oozie的使用" class="headerlink" title="第4章 Oozie的使用"></a>第4章 Oozie的使用</h1><h2 id="4-1-案例一：Oozie调度shell脚本"><a href="#4-1-案例一：Oozie调度shell脚本" class="headerlink" title="4.1 案例一：Oozie调度shell脚本"></a>4.1 案例一：Oozie调度shell脚本</h2><p>目标：使用Oozie调度Shell脚本</p><p>分步实现：</p><p>1）创建工作目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ mkdir -p oozie-apps&#x2F;shell</span><br></pre></td></tr></table></figure><p>2）在oozie-apps/shell目录下创建两个文件——job.properties和workflow.xml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 shell]$ touch workflow.xml</span><br><span class="line"></span><br><span class="line">[red@hadoop102 shell]$ touch job.properties</span><br></pre></td></tr></table></figure><p>3）编辑job.properties和workflow.xml文件</p><p><strong>job.properties</strong></p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#HDFS地址</span></span><br><span class="line"></span><br><span class="line"><span class="attr">nameNode</span>=<span class="string">hdfs://hadoop102:8020</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ResourceManager地址</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobTracker</span>=<span class="string">hadoop103:8032</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#队列名称</span></span><br><span class="line"></span><br><span class="line"><span class="attr">queueName</span>=<span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">examplesRoot</span>=<span class="string">oozie-apps</span></span><br><span class="line"></span><br><span class="line"><span class="meta">oozie.wf.application.path</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/shell</span></span><br></pre></td></tr></table></figure><p><strong>workflow.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.4"</span> <span class="attr">name</span>=<span class="string">"shell-wf"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--开始节点--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"shell-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--动作节点--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"shell-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--shell动作--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--要执行的脚本--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>mkdir<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>/opt/software/d<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--kill节点--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Shell action failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--结束节点--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure><p>4）上传任务配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-2.5.0-cdh5.3.6&#x2F;bin&#x2F;hadoop fs -put oozie-apps&#x2F;</span><br><span class="line">&#x2F;user&#x2F;red</span><br></pre></td></tr></table></figure><p>5）执行任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozie job -oozie http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -config oozie-apps&#x2F;shell&#x2F;job.properties -run</span><br></pre></td></tr></table></figure><p>6）杀死某个任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozie job -oozie http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -kill 0000004-170425105153692-oozie-z-W</span><br></pre></td></tr></table></figure><h2 id="4-2-案例二：Oozie逻辑调度执行多个Job"><a href="#4-2-案例二：Oozie逻辑调度执行多个Job" class="headerlink" title="4.2 案例二：Oozie逻辑调度执行多个Job"></a>4.2 案例二：Oozie逻辑调度执行多个Job</h2><p>目标：使用Oozie执行多个Job调度</p><p>分步执行：</p><p>1）编辑job.properties和workflow.xml文件</p><p><strong>job.properties</strong></p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">nameNode</span>=<span class="string">hdfs://hadoop102:8020</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobTracker</span>=<span class="string">hadoop103:8032</span></span><br><span class="line"></span><br><span class="line"><span class="attr">queueName</span>=<span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">examplesRoot</span>=<span class="string">oozie-apps</span></span><br><span class="line"></span><br><span class="line"><span class="meta">oozie.wf.application.path</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/shells</span></span><br></pre></td></tr></table></figure><p><strong>workflow.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.4"</span> <span class="attr">name</span>=<span class="string">"shells-wf"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"p1-shell-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"p1-shell-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>mkdir<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>/opt/software/d1<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"p2-shell-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"p2-shell-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>mkdir<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>/opt/software/d2<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Shell action failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br><span class="line"></span><br><span class="line">补充：fork节点和join节点</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">fork</span> <span class="attr">name</span>=<span class="string">"forking"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">path</span> <span class="attr">start</span>=<span class="string">"firstparalleljob"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">path</span> <span class="attr">start</span>=<span class="string">"secondparalleljob"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">fork</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">join</span> <span class="attr">name</span>=<span class="string">"joining"</span> <span class="attr">to</span>=<span class="string">"nextaction"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>2）上传任务配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bin&#x2F;hadoop fs -rmr &#x2F;user&#x2F;red&#x2F;oozie-apps&#x2F;</span><br><span class="line"></span><br><span class="line">$ bin&#x2F;hadoop fs -put oozie-apps&#x2F;shells &#x2F;user&#x2F;red&#x2F;oozie-apps</span><br></pre></td></tr></table></figure><ol start="3"><li>执行任务</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozie job -oozie http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -config oozie-apps&#x2F;shells&#x2F;job.properties -run</span><br></pre></td></tr></table></figure><h2 id="4-3-案例三：Oozie调度MapReduce任务"><a href="#4-3-案例三：Oozie调度MapReduce任务" class="headerlink" title="4.3 案例三：Oozie调度MapReduce任务"></a>4.3 案例三：Oozie调度MapReduce任务</h2><p>目标：使用Oozie调度MapReduce任务</p><p>分步执行：</p><p>1）找到一个可以运行的mapreduce任务的jar包（可以用官方的，也可以是自己写的）</p><p>2）拷贝官方模板到oozie-apps</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ cp -r &#x2F;opt&#x2F;module&#x2F;oozie-4.0.0-cdh5.3.6&#x2F;examples&#x2F;apps&#x2F;map-reduce&#x2F; oozie-apps&#x2F;</span><br></pre></td></tr></table></figure><p><strong>3)测试一下wordcount在yarn中的运行</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-2.5.0-cdh5.3.6&#x2F;bin&#x2F;yarn jar</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-2.5.0-cdh5.3.6&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar</span><br><span class="line">wordcount &#x2F;input&#x2F; &#x2F;output&#x2F;</span><br></pre></td></tr></table></figure><p><strong>4) 配置map-reduce任务的job.properties以及workflow.xml</strong></p><p><strong>job.properties</strong></p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">nameNode</span>=<span class="string">hdfs://hadoop102:8020</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobTracker</span>=<span class="string">hadoop103:8032</span></span><br><span class="line"></span><br><span class="line"><span class="attr">queueName</span>=<span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">examplesRoot</span>=<span class="string">oozie-apps</span></span><br><span class="line"></span><br><span class="line"><span class="meta">oozie.wf.application.path</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/map-reduce/workflow.xml</span></span><br></pre></td></tr></table></figure><p><strong>workflow.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.2"</span> <span class="attr">name</span>=<span class="string">"map-reduce-wf"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"mr-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"mr-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">map-reduce</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">prepare</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">"$&#123;nameNode&#125;/output/"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">prepare</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置调度MR任务时，使用新的API --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.mapper.new-api<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.reducer.new-api<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Job Key输出类型 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.output.key.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.Text<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Job Value输出类型 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.output.value.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.IntWritable<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定输入路径 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.input.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/input/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定输出路径 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.output.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/output/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Map类 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.map.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.examples.WordCount$TokenizerMapper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Reduce类 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.reduce.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.examples.WordCount$IntSumReducer<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.tasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">map-reduce</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Map/Reduce failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure><p>5）拷贝待执行的jar包到map-reduce的lib目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ cp -a &#x2F;opt</span><br><span class="line">&#x2F;module&#x2F;cdh&#x2F;hadoop-2.5.0-cdh5.3.6&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar</span><br><span class="line">oozie-apps&#x2F;map-reduce&#x2F;lib</span><br></pre></td></tr></table></figure><p>6）上传配置好的app文件夹到HDFS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-2.5.0-cdh5.3.6&#x2F;bin&#x2F;hdfs dfs -put</span><br><span class="line">oozie-apps&#x2F;map-reduce&#x2F; &#x2F;user&#x2F;admin&#x2F;oozie-apps</span><br></pre></td></tr></table></figure><p>7）执行任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozie job -oozie</span><br><span class="line">http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -config</span><br><span class="line">oozie-apps&#x2F;map-reduce&#x2F;job.properties -run</span><br></pre></td></tr></table></figure><h2 id="4-4-案例四：Oozie定时任务-循环任务"><a href="#4-4-案例四：Oozie定时任务-循环任务" class="headerlink" title="4.4 案例四：Oozie定时任务/循环任务"></a>4.4 案例四：Oozie定时任务/循环任务</h2><p>目标：Coordinator定时的周期性调度任务</p><p>分步实现：</p><ol><li><p>配置Linux时区以及时间服务器</p></li><li><p>检查系统当前时区：</p></li></ol><blockquote><h1 id="date-R"><a href="#date-R" class="headerlink" title="date -R"></a>date -R</h1></blockquote><p>注意：如果显示的时区不是+0800，删除localtime文件夹后，再关联一个正确时区的链接过去，命令如下：</p><blockquote><h1 id="rm-rf-etc-localtime"><a href="#rm-rf-etc-localtime" class="headerlink" title="rm -rf /etc/localtime"></a>rm -rf /etc/localtime</h1><p>ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</p></blockquote><p>同步时间：</p><blockquote><p>ntpdate pool.ntp.org</p></blockquote><p>修改NTP配置文件：</p><blockquote><p>vi /etc/ntp.conf</p></blockquote><p>去掉下面这行前面的# ,并把网段修改成自己的网段：</p><blockquote><p>restrict 192.168.122.0 mask 255.255.255.0 nomodify notrap</p></blockquote><p>注释掉以下几行：</p><blockquote><p>#server 0.centos.pool.ntp.org</p><p>#server 1.centos.pool.ntp.org</p><p>#server 2.centos.pool.ntp.org</p></blockquote><p>把下面两行前面的#号去掉,如果没有这两行内容,需要手动添加</p><blockquote><p>server 127.127.1.0 # local clock</p><p>fudge 127.127.1.0 stratum 10</p></blockquote><p>重启NTP服务：</p><blockquote><p>systemctl start ntpd.service，</p></blockquote><p>注意，如果是centOS7以下的版本，使用命令：service ntpd start</p><blockquote><p>systemctl enable ntpd.service，</p></blockquote><p>注意，如果是centOS7以下的版本，使用命令：chkconfig ntpd on</p><p>集群其他节点去同步这台时间服务器时间：</p><p>首先需要关闭这两台计算机的ntp服务</p><blockquote><p>systemctl stop ntpd.service，</p></blockquote><p>centOS7以下，则：service ntpd stop</p><blockquote><p>systemctl disable ntpd.service，</p></blockquote><p>centOS7以下，则：chkconfig ntpd off</p><blockquote><p>systemctl status ntpd，查看ntp服务状态</p><p>pgrep ntpd，查看ntp服务进程id</p></blockquote><p>同步第一台服务器linux01的时间：</p><blockquote><p>ntpdate hadoop102</p></blockquote><p>使用root用户制定计划任务,周期性同步时间：</p><blockquote><p>crontab -e */10 * * * * /usr/sbin/ntpdate hadoop102</p></blockquote><p>重启定时任务：</p><blockquote><p>systemctl restart crond.service，</p></blockquote><p>centOS7以下使用：service crond restart，</p><p>其他台机器的配置同理。</p><ol start="3"><li>配置 oozie-site.xml文件</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">属性：oozie.processing.timezone</span><br><span class="line"></span><br><span class="line">属性值：GMT+0800</span><br></pre></td></tr></table></figure><p>解释：修改时区为东八区区时</p><p>注：该属性去oozie-default.xml中找到即可</p><ol start="4"><li>修改js框架中的关于时间设置的代码(页面修改以后，省略此步骤，建议在页面修改)</li></ol><blockquote><p>$ vi<br>/opt/module/oozie-4.0.0-cdh5.3.6/oozie-server/webapps/oozie/oozie-console.js</p></blockquote><p>修改如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getTimeZone</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">Ext.state.Manager.setProvider(<span class="keyword">new</span> Ext.state.CookieProvider());</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> Ext.state.Manager.get(<span class="string">"TimezoneId"</span>,<span class="string">"GMT"</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>5）重启oozie服务，并重启浏览器（一定要注意清除缓存）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozied.sh stop</span><br><span class="line"></span><br><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozied.sh start</span><br></pre></td></tr></table></figure><p>6）拷贝官方模板配置定时任务</p><blockquote><p>$ cp -r examples/apps/cron/ oozie-apps/</p></blockquote><p>7）修改模板job.properties和coordinator.xml以及workflow.xml</p><p><strong>job.properties</strong></p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">pnameNode</span>=<span class="string">hdfs://hadoop102:8020</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobTracker</span>=<span class="string">hadoop103:8032</span></span><br><span class="line"></span><br><span class="line"><span class="attr">queueName</span>=<span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">examplesRoot</span>=<span class="string">oozie-apps</span></span><br><span class="line"></span><br><span class="line"><span class="meta">oozie.coord.application.path</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#start：必须设置为未来时间，否则任务失败</span></span><br><span class="line"></span><br><span class="line"><span class="attr">start</span>=<span class="string">2019-11-01T11:05+0800</span></span><br><span class="line"></span><br><span class="line"><span class="attr">end</span>=<span class="string">2019-11-01T12:05+0800</span></span><br><span class="line"><span class="attr">workflowAppUri</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron</span></span><br></pre></td></tr></table></figure><p><strong>coordinator.xml（最小频率5分钟）</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">coordinator-app</span> <span class="attr">name</span>=<span class="string">"cron-coord"</span> <span class="attr">frequency</span>=<span class="string">"$&#123;coord:minutes(5)&#125;"</span></span></span><br><span class="line"><span class="tag"><span class="attr">start</span>=<span class="string">"$&#123;start&#125;"</span> <span class="attr">end</span>=<span class="string">"$&#123;end&#125;"</span> <span class="attr">timezone</span>=<span class="string">"GMT+0800"</span></span></span><br><span class="line"><span class="tag"><span class="attr">xmlns</span>=<span class="string">"uri:oozie:coordinator:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">workflow</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">app-path</span>&gt;</span>$&#123;workflowAppUri&#125;<span class="tag">&lt;/<span class="name">app-path</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>jobTracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>nameNode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>queueName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">coordinator-app</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>workflow.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.5"</span> <span class="attr">name</span>=<span class="string">"one-op-wf"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"shell-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"shell-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>p1.sh<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Shell action failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure><p>8）上传配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-2.5.0-cdh5.3.6&#x2F;bin&#x2F;hdfs dfs -put oozie-apps&#x2F;cron&#x2F;</span><br><span class="line">&#x2F;user&#x2F;red&#x2F;oozie-apps</span><br></pre></td></tr></table></figure><p>9）启动任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.3.6]$ bin&#x2F;oozie job -oozie</span><br><span class="line">http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -config oozie-apps&#x2F;cron&#x2F;job.properties -run</span><br></pre></td></tr></table></figure><p>注意：oozie允许的最小执行任务的频率是5分钟</p><h1 id="第5章-常见问题总结"><a href="#第5章-常见问题总结" class="headerlink" title="第5章 常见问题总结"></a>第5章 常见问题总结</h1><p>1）Mysql权限配置</p><p>授权所有主机可以使用root用户操作所有数据库和数据表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant all on *.* to root@&#39;%&#39; identified by &#39;123456;</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line"></span><br><span class="line">mysql&gt; exit;</span><br></pre></td></tr></table></figure><p>2）workflow.xml配置的时候不要忽略file属性</p><p>3）jps查看进程时，注意有没有bootstrap</p><p>4）关闭oozie</p><p>如果bin/oozied.sh stop无法关闭，则可以使用kill -9<br>[pid]，之后oozie-server/temp/xxx.pid文件一定要删除。</p><p>5）Oozie重新打包时，一定要注意先关闭进程，删除对应文件夹下面的pid文件。（可以参考第4条目）</p><p>6）配置文件一定要生效</p><p>起始标签和结束标签无对应则不生效，配置文件的属性写错了，那么则执行默认的属性。</p><p>7）libext下边的jar存放于某个文件夹中，导致share/lib创建不成功。</p><p>8）调度任务时，找不到指定的脚本，可能是oozie-site.xml里面的Hadoop配置文件没有关联上。</p><p>9）修改Hadoop配置文件，需要重启集群。一定要记得scp到其他节点。</p><p>10）JobHistoryServer必须开启，集群要重启的。</p><p>11）Mysql配置如果没有生效的话，默认使用derby数据库。</p><p>12）在本地修改完成的job配置，必须重新上传到HDFS。</p><p>13）将HDFS中上传的oozie配置文件下载下来查看是否有错误。</p><p>14）Linux用户名和Hadoop的用户名不一致。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>my test blog</title>
    <link href="http://www.red0819.top/2020/09/06/my-test-blog/"/>
    <id>http://www.red0819.top/2020/09/06/my-test-blog/</id>
    <published>2020-09-05T17:19:45.000Z</published>
    <updated>2020-09-05T17:22:42.601Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>就是试一下联网 一直都是单机23333333</p><p>看看有没有成功！</p><p>————刚刚tags忘空格了，再来一次————</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="尝试" scheme="http://www.red0819.top/tags/%E5%B0%9D%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Socket入门介绍</title>
    <link href="http://www.red0819.top/2020/08/26/Sockecket%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"/>
    <id>http://www.red0819.top/2020/08/26/Sockecket%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/</id>
    <published>2020-08-26T04:38:42.000Z</published>
    <updated>2020-09-14T21:07:50.615Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>下面进行MS-Windows、HP-Unix网络编程的实践过程中总结出来的一些经验，仅供大家参考。本文所谈到的Socket函数如果没有特别说明，都是指的Windows Socket API。</p><h1 id="Socket-API简介"><a href="#Socket-API简介" class="headerlink" title="Socket API简介"></a>Socket API简介</h1><p>Windows Socket是从UNIX Socket继承发展而来，最新的版本是2.2。进行Windows网络编程，需要在程序中包含WINSOCK2.H或MSWSOCK.H，同时需要添加引入库WS2_32. LIB或WSOCK32.LIB。</p><p>网络中用一个三元组可以在全局唯一标志一个进程： （协议，本地地址，本地端口号） ，这样一个三元组，叫做一个半相关（half-association），它指定连接的每半部分。 </p><h1 id="套接字类型"><a href="#套接字类型" class="headerlink" title="套接字类型"></a>套接字类型</h1><p>TCP/IP的socket提供下列三种类型套接字。 </p><h2 id="流式套接字（SOCK-STREAM）"><a href="#流式套接字（SOCK-STREAM）" class="headerlink" title="流式套接字（SOCK_STREAM）"></a>流式套接字（SOCK_STREAM）</h2><p>提供了一个面向连接、可靠的数据传输服务，数据无差错、无重复地发送，且按发送顺序接收。内设流量控制，避免数据流超限；数据被看作是字节流，无长度限制。文件传送协议（FTP）即使用流式套接字。 </p><h2 id="数据报式套接字（SOCK-DGRAM）"><a href="#数据报式套接字（SOCK-DGRAM）" class="headerlink" title="数据报式套接字（SOCK_DGRAM）"></a>数据报式套接字（SOCK_DGRAM）</h2><p>提供了一个无连接服务。数据包以独立包形式被发送，不提供无错保证，数据可能丢失或重复，并且接收顺序混乱。网络文件系统（NFS）使用数据报式套接字。 </p><h2 id="原始式套接字（SOCK-RAW）"><a href="#原始式套接字（SOCK-RAW）" class="headerlink" title="原始式套接字（SOCK_RAW）"></a>原始式套接字（SOCK_RAW）</h2><p>该接口允许对较低层协议，如IP、ICMP直接访问。常用于检验新的协议实现或访问现有服务中配置的新设备。 </p><h3 id="一、WSAStartup函数"><a href="#一、WSAStartup函数" class="headerlink" title="一、WSAStartup函数"></a>一、WSAStartup函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">WSAStartup</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">WORD wVersionRequested,         <span class="comment">//使用的Socket版本</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">LPWSADATA lpWSAData         <span class="comment">//返回请求的Socket的版本信息</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>使用Socket的程序在使用Socket之前必须调用WSAStartup函数。该函数的第一个参数指明程序请求使用的Socket版本，其中高位字节指明副版本、低位字节指明主版本；操作系统利用第二个参数返回请求的Socket的版本信息。当一个应用程序调用WSAStartup函数时，操作系统根据请求的Socket版本来搜索相应的Socket库，然后绑定找到的Socket库到该应用程序中。以后应用程序就可以调用所请求的Socket库中的其它Socket函数了。该函数执行成功后返回0。</p><p>例：假如一个程序要使用2.1版本的Socket,那么程序代码如下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wVersionRequested = MAKEWORD( <span class="number">2</span>, <span class="number">1</span> ); </span><br><span class="line"></span><br><span class="line">err = WSAStartup( wVersionRequested, &amp;wsaData );</span><br></pre></td></tr></table></figure><h3 id="二、WSACleanup函数"><a href="#二、WSACleanup函数" class="headerlink" title="二、WSACleanup函数"></a>二、WSACleanup函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">WSACleanup</span> <span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure><p>应用程序在完成对请求的Socket库的使用后，要调用WSACleanup函数来解除与Socket库的绑定并且释放Socket库所占用的系统资源。 </p><h3 id="三、socket函数"><a href="#三、socket函数" class="headerlink" title="三、socket函数"></a>三、socket函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">SOCKET <span class="title">socket</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> af,          <span class="comment">//指定应用程序使用的通信协议的协议族，对于TCP/IP协议族，该参数置AF_INET; UNIX系统支持的地址族有：AF_UNIX、AF_INET、AF_NS等，而DOS、WINDOWS中仅支持AF_INET，它是网际网区域。</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> type,         <span class="comment">//套接字类型</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> protocol        <span class="comment">//应用程序所使用的通信协议</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>应用程序调用socket函数来创建一个能够进行网络通信的套接字。第一个参数指定应用程序使用的通信协议的协议族，对于TCP/IP协议族，该参数置PF_INET;第二个参数指定要创建的套接字类型，流套接字类型为SOCK_STREAM、数据报套接字类型为SOCK_DGRAM；第三个参数指定应用程序所使用的通信协议。该函数如果调用成功就返回新创建的套接字的描述符，如果失败就返回INVALID_SOCKET。套接字描述符是一个整数类型的值。每个进程的进程空间里都有一个套接字描述符表，该表中存放着套接字描述符和套接字数据结构的对应关系。该表中有一个字段存放新创建的套接字的描述符，另一个字段存放套接字数据结构的地址，因此根据套接字描述符就可以找到其对应的套接字数据结构。每个进程在自己的进程空间里都有一个套接字描述符表但是套接字数据结构都是在操作系统的内核缓冲里。下面是一个创建流套接字的例子：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">protoent</span> *<span class="title">ppe</span>;</span> </span><br><span class="line"></span><br><span class="line">ppe=getprotobyname(<span class="string">"tcp"</span>); </span><br><span class="line"></span><br><span class="line">SOCKET ListenSocket=socket(PF_INET,SOCK_STREAM,ppe-&gt;p_proto);</span><br></pre></td></tr></table></figure><h3 id="四、closesocket函数"><a href="#四、closesocket函数" class="headerlink" title="四、closesocket函数"></a>四、closesocket函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">closesocket</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>closesocket函数用来关闭一个描述符为s的套接字。由于每个进程中都有一个套接字描述符表，表中的每个套接字描述符都对应了一个位于操作系统缓冲区中的套接字数据结构，因此有可能有几个套接字描述符指向同一个套接字数据结构。套接字数据结构中专门有一个字段存放该结构的被引用次数，即有多少个套接字描述符指向该结构。当调用closesocket函数时，操作系统先检查套接字数据结构中的该字段的值，如果为1，就表明只有一个套接字描述符指向它，因此操作系统就先把s在套接字描述符表中对应的那条表项清除，并且释放s对应的套接字数据结构；如果该字段大于1，那么操作系统仅仅清除s在套接字描述符表中的对应表项，并且把s对应的套接字数据结构的引用次数减1。 </p><p>closesocket函数如果执行成功就返回0，否则返回SOCKET_ERROR。 </p><h3 id="五、send函数"><a href="#五、send函数" class="headerlink" title="五、send函数"></a>五、send函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">send</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s,                   <span class="comment">//指定发送端套接字描述符</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">const</span> <span class="keyword">char</span> FAR *buf,         <span class="comment">//一个存放应用程序要发送数据的缓冲区</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> len,              <span class="comment">//实际要发送的数据的字节数</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> flags </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>不论是客户还是服务器应用程序都用send函数来向TCP连接的另一端发送数据。客户程序一般用send函数向服务器发送请求，而服务器则通常用send函数来向客户程序发送应答。该函数的第一个参数指定发送端套接字描述符；第二个参数指明一个存放应用程序要发送数据的缓冲区；第三个参数指明实际要发送的数据的字节数；第四个参数一般置0。这里只描述同步Socket的send函数的执行流程。当调用该函数时，send先比较待发送数据的长度len和套接字s的发送缓冲区的长度，如果len大于s的发送缓冲区的长度，该函数返回SOCKET_ERROR；如果len小于或者等于s的发送缓冲区的长度，那么send先检查协议是否正在发送s的发送缓冲中的数据，如果是就等待协议把数据发送完，如果协议还没有开始发送s的发送缓冲中的数据或者s的发送缓冲中没有数据，那么send就比较s的发送缓冲区的剩余空间和len，如果len大于剩余空间大小send就一直等待协议把s的发送缓冲中的数据发送完，如果len小于剩余空间大小send就仅仅把buf中的数据copy到剩余空间里（注意并不是send把s的发送缓冲中的数据传到连接的另一端的，而是协议传的，send仅仅是把buf中的数据copy到s的发送缓冲区的剩余空间里）。如果send函数copy数据成功，就返回实际copy的字节数，如果send在copy数据时出现错误，那么send就返回SOCKET_ERROR；如果send在等待协议传送数据时网络断开的话，那么send函数也返回SOCKET_ERROR。要注意send函数把buf中的数据成功copy到s的发送缓冲的剩余空间里后它就返回了，但是此时这些数据并不一定马上被传到连接的另一端。如果协议在后续的传送过程中出现网络错误的话，那么下一个Socket函数就会返回SOCKET_ERROR。（每一个除send外的Socket函数在执行的最开始总要先等待套接字的发送缓冲中的数据被协议传送完毕才能继续，如果在等待时出现网络错误，那么该Socket函数就返回SOCKET_ERROR）<br>注意：在Unix系统下，如果send在等待协议传送数据时网络断开的话，调用send的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。 </p><h3 id="六、recv函数"><a href="#六、recv函数" class="headerlink" title="六、recv函数"></a>六、recv函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">recv</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">char</span> FAR *buf, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> len, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> flags </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>不论是客户还是服务器应用程序都用recv函数从TCP连接的另一端接收数据。该函数的第一个参数指定接收端套接字描述符；第二个参数指明一个缓冲区，该缓冲区用来存放recv函数接收到的数据；第三个参数指明buf的长度；第四个参数一般置0。这里只描述同步Socket的recv函数的执行流程。当应用程序调用recv函数时，recv先等待s的发送缓冲中的数据被协议传送完毕，如果协议在传送s的发送缓冲中的数据时出现网络错误，那么recv函数返回SOCKET_ERROR，如果s的发送缓冲中没有数据或者数据被协议成功发送完毕后，recv先检查套接字s的接收缓冲区，如果s接收缓冲区中没有数据或者协议正在接收数据，那么recv就一直等待，只到协议把数据接收完毕。当协议把数据接收完毕，recv函数就把s的接收缓冲中的数据copy到buf中（注意协议接收到的数据可能大于buf的长度，所以在这种情况下要调用几次recv函数才能把s的接收缓冲中的数据copy完。recv函数仅仅是copy数据，真正的接收数据是协议来完成的），recv函数返回其实际copy的字节数。如果recv在copy时出错，那么它返回SOCKET_ERROR；如果recv函数在等待协议接收数据时网络中断了，那么它返回0。 </p><p>注意：在Unix系统下，如果recv函数在等待协议接收数据时网络断开了，那么调用recv的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。 </p><h3 id="七、bind函数"><a href="#七、bind函数" class="headerlink" title="七、bind函数"></a>七、bind函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bind</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">const</span> struct sockaddr FAR *name, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> namelen </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>当创建了一个Socket以后，套接字数据结构中有一个默认的IP地址和默认的端口号。一个服务程序必须调用bind函数来给其绑定一个IP地址和一个特定的端口号。客户程序一般不必调用bind函数来为其Socket绑定IP地址和断口号。该函数的第一个参数指定待绑定的Socket描述符；第二个参数指定一个sockaddr结构，该结构是这样定义的： </p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr</span> &#123;</span></span><br><span class="line"></span><br><span class="line">u_short sa_family;            <span class="comment">//地址族 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> sa_data[<span class="number">14</span>];             <span class="comment">// 14字节协议地址</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>sa_family指定地址族，对于TCP/IP协议族的套接字，给其置AF_INET。当对TCP/IP协议族的套接字进行绑定时，我们通常使用另一个地址结构：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> &#123;</span>          <span class="comment">// ("in" 代表 "Internet"。)</span></span><br><span class="line"></span><br><span class="line">short sin_family; </span><br><span class="line"></span><br><span class="line">u_short sin_port;         <span class="comment">// 16位端口号，网络字节顺序</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">in_addr</span> <span class="title">sin_addr</span>;</span>       <span class="comment">// 32位IP地址，网络字节顺序</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> sin_zero[<span class="number">8</span>];          <span class="comment">//保留</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>其中sin_family置AF_INET；sin_port指明端口号；sin_addr结构体中只有一个唯一的字段s_addr，表示IP地址，该字段是一个整数，一般用函数inet_addr（）把字符串形式的IP地址转换成unsigned long型的整数值后再置给s_addr。有的服务器是多宿主机，至少有两个网卡，那么运行在这样的服务器上的服务程序在为其Socket绑定IP地址时可以把htonl(INADDR_ANY)置给s_addr，这样做的好处是不论哪个网段上的客户程序都能与该服务程序通信；如果只给运行在多宿主机上的服务程序的Socket绑定一个固定的IP地址，那么就只有与该IP地址处于同一个网段上的客户程序才能与该服务程序通信。我们用0来填充sin_zero数组，目的是让sockaddr_in结构的大小与sockaddr结构的大小一致。下面是一个bind函数调用的例子： </p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">saddr</span>； </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">saddr</span>.<span class="title">sin_family</span> = <span class="title">AF_INET</span>;</span> </span><br><span class="line"></span><br><span class="line">saddr.sin_port = htons(<span class="number">8888</span>);     <span class="comment">// Host to Network Short</span></span><br><span class="line"></span><br><span class="line">saddr.sin_addr.s_addr = htonl(INADDR_ANY); <span class="comment">//使用htonl将IP地址转换为网络格式，INADDR_ANY自动填上它所运行的机器的 IP 地址</span></span><br><span class="line"></span><br><span class="line">bind(ListenSocket,(struct sockaddr *)&amp;saddr,<span class="keyword">sizeof</span>(saddr))；</span><br></pre></td></tr></table></figure><p>注意：不同的计算机存放多字节值的顺序不同，有的机器在起始地址存放低位字节（低价先存），有的存高位字节（高价先存）。为保证数据的正确性，在网络协议中须指定网络字节顺序。TCP/IP协议使用16位整数和32位整数的高价先存格式，它们均含在协议头文件中。</p><p>在调用 bind() 的时候，要小心的另一件事情是：不要采用小于 1024的端口号。所有小于1024的端口号都被系统保留！可以选择从1024 到65535的端口(如果它们没有被别的程序使用的话)。</p><h3 id="八、listen函数"><a href="#八、listen函数" class="headerlink" title="八、listen函数"></a>八、listen函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">listen</span><span class="params">( SOCKET s, <span class="keyword">int</span> backlog )</span></span>;</span><br></pre></td></tr></table></figure><p>服务程序可以调用listen函数使其流套接字s处于监听状态。处于监听状态的流套接字s将维护一个客户连接请求队列，该队列最多容纳backlog个客户连接请求。假如该函数执行成功，则返回0；如果执行失败，则返回SOCKET_ERROR。 </p><h3 id="九、accept函数"><a href="#九、accept函数" class="headerlink" title="九、accept函数"></a>九、accept函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">SOCKET <span class="title">accept</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">struct sockaddr FAR *addr,          <span class="comment">//返回新创建的套接字的地址结构</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> FAR *addrlen              <span class="comment">//新创建的套接字的地址结构的长度</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>服务程序调用accept函数从处于监听状态的流套接字s的客户连接请求队列中取出排在最前的一个客户请求，并且创建一个新的套接字来与客户套接字创建连接通道，如果连接成功，就返回新创建的套接字的描述符，以后与客户套接字交换数据的是新创建的套接字；如果失败就返回INVALID_SOCKET。该函数的第一个参数指定处于监听状态的流套接字；操作系统利用第二个参数来返回新创建的套接字的地址结构；操作系统利用第三个参数来返回新创建的套接字的地址结构的长度。下面是一个调用accept的例子： </p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">ServerSocketAddr</span>;</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> addrlen; </span><br><span class="line"></span><br><span class="line">addrlen=<span class="keyword">sizeof</span>(ServerSocketAddr); </span><br><span class="line"></span><br><span class="line">ServerSocket=accept(ListenSocket,(struct sockaddr *)&amp;ServerSocketAddr,&amp;addrlen);</span><br></pre></td></tr></table></figure><h3 id="十、connect函数"><a href="#十、connect函数" class="headerlink" title="十、connect函数"></a>十、connect函数</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">connect</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">const</span> struct sockaddr FAR *name,</span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> namelen </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><p>客户程序调用connect函数来使客户Socket s与监听于name所指定的计算机的特定端口上的服务Socket进行连接。如果连接成功，connect返回0；如果失败则返回SOCKET_ERROR。下面是一个例子：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">daddr</span>;</span> </span><br><span class="line"></span><br><span class="line"><span class="built_in">memset</span>((<span class="keyword">void</span> *)&amp;daddr,<span class="number">0</span>,<span class="keyword">sizeof</span>(daddr)); </span><br><span class="line"></span><br><span class="line">daddr.sin_family=AF_INET; </span><br><span class="line"></span><br><span class="line">daddr.sin_port=htons(<span class="number">8888</span>); </span><br><span class="line"></span><br><span class="line">daddr.sin_addr.s_addr=inet_addr(<span class="string">"133.197.22.4"</span>);  <span class="comment">//函数inet_addr(),将IP地址从 点数格式转换成无符号长整型， inet_addr()返回的地址已经是网络字节格式</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">connect</span>(ClientSocket,(struct sockaddr *)&amp;daddr,<span class="keyword">sizeof</span>(daddr));</span><br></pre></td></tr></table></figure><h1 id="实例基本步骤"><a href="#实例基本步骤" class="headerlink" title="实例基本步骤"></a>实例基本步骤</h1><p>设计一个基本的网络服务器有以下几个步骤：</p><p>1、初始化Windows Socket</p><p>2、创建一个监听的Socket</p><p>3、设置服务器地址信息，并将监听端口绑定到这个地址上</p><p>4、开始监听</p><p>5、接受客户端连接</p><p>6、和客户端通信</p><p>7、结束服务并清理Windows Socket和相关数据，或者返回第4步</p><h1 id="入门代码"><a href="#入门代码" class="headerlink" title="入门代码"></a>入门代码</h1><p>下面是简单的服务器和客户端源代码。（阻塞模式下的，供初学者理解）</p><h3 id="TCPServer"><a href="#TCPServer" class="headerlink" title="TCPServer"></a>TCPServer</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;winsock2.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">  WSADATA        wsaData;</span><br><span class="line"></span><br><span class="line">  SOCKET        ListeningSocket;</span><br><span class="line"></span><br><span class="line">  SOCKET        NewConnection;</span><br><span class="line"></span><br><span class="line">  SOCKADDR_IN      ServerAddr;</span><br><span class="line"></span><br><span class="line">  SOCKADDR_IN      ClientAddr;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>          Port = <span class="number">5150</span>;</span><br><span class="line"></span><br><span class="line">  WSAStartup(MAKEWORD(<span class="number">2</span>,<span class="number">2</span>), &amp;wsaData); <span class="comment">// 初始化Windows Socket 2.2</span></span><br><span class="line"></span><br><span class="line">  ListeningSocket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP); <span class="comment">// 创建一个新的Socket来响应客户端的连接请求</span></span><br><span class="line"></span><br><span class="line">  ServerAddr.sin_family = AF_INET; <span class="comment">// 填写服务器地址信息</span></span><br><span class="line"></span><br><span class="line">  ServerAddr.sin_port = htons(Port);   <span class="comment">// 端口为5150</span></span><br><span class="line"></span><br><span class="line">  ServerAddr.sin_addr.s_addr = htonl(INADDR_ANY); <span class="comment">// IP地址为INADDR_ANY，注意使用htonl将IP地址转换为网络格式</span></span><br><span class="line"></span><br><span class="line">  bind(ListeningSocket, (SOCKADDR *)&amp;ServerAddr, <span class="keyword">sizeof</span>(ServerAddr)); <span class="comment">// 绑定监听端口 listen(ListeningSocket, 5); // 开始监听，指定最大同时连接数为5 </span></span><br><span class="line"></span><br><span class="line">  NewConnection = accept(ListeningSocket, (SOCKADDR *) &amp;ClientAddr,&amp;ClientAddrLen)); <span class="comment">// 接受新的连接</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 新的连接建立后，就可以互相通信了，在这个简单的例子中，我们直接关闭连接，</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 并关闭监听Socket，然后退出应用程序</span></span><br><span class="line"></span><br><span class="line">  closesocket(NewConnection);</span><br><span class="line"></span><br><span class="line">  closesocket(ListeningSocket);</span><br><span class="line"></span><br><span class="line">  WSACleanup();<span class="comment">// 释放Windows Socket DLL的相关资源</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TCPClient"><a href="#TCPClient" class="headerlink" title="TCPClient"></a>TCPClient</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;winsock2.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">  WSADATA        wsaData;</span><br><span class="line"></span><br><span class="line">  SOCKET        s;</span><br><span class="line"></span><br><span class="line">  SOCKADDR_IN      ServerAddr;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>          Port = <span class="number">5150</span>;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  WSAStartup(MAKEWORD(<span class="number">2</span>,<span class="number">2</span>), &amp;wsaData);  <span class="comment">//初始化Windows Socket 2.2</span></span><br><span class="line"></span><br><span class="line">  s = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);  <span class="comment">// 创建一个新的Socket来连接服务器</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 填写客户端地址信息</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 端口为5150</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 服务器IP地址为"136.149.3.29"，注意使用inet_addr将IP地址转换为网络格式</span></span><br><span class="line"></span><br><span class="line">  ServerAddr.sin_family = AF_INET;</span><br><span class="line"></span><br><span class="line">   ServerAddr.sin_port = htons(Port);   </span><br><span class="line"></span><br><span class="line">   ServerAddr.sin_addr.s_addr = inet_addr(<span class="string">"136.149.3.29"</span>);</span><br><span class="line"></span><br><span class="line">   <span class="built_in">connect</span>(s, (SOCKADDR *) &amp;ServerAddr, <span class="keyword">sizeof</span>(ServerAddr));    <span class="comment">// 向服务器发出连接请求</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 新的连接建立后，就可以互相通信了，在这个简单的例子中，我们直接关闭连接，</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 并关闭监听Socket，然后退出应用程序</span></span><br><span class="line"></span><br><span class="line">   closesocket(s);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 释放Windows Socket DLL的相关资源</span></span><br><span class="line"></span><br><span class="line">   WSACleanup();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="计算机网络" scheme="http://www.red0819.top/categories/computernetwork/"/>
    
    
    <category term="socket" scheme="http://www.red0819.top/tags/socket/"/>
    
  </entry>
  
  <entry>
    <title>SparkStreaming</title>
    <link href="http://www.red0819.top/2020/03/11/SparkStreaming/"/>
    <id>http://www.red0819.top/2020/03/11/SparkStreaming/</id>
    <published>2020-03-11T08:45:00.000Z</published>
    <updated>2020-09-14T22:11:58.807Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img src="/2020/03/11/SparkStreaming/SparkStreaming-logo.jpg" alt="SparkStreaming-logo"></p><p>[TOC]</p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Spark Streaming是微批次处理方式，批处理间隔是Spark Streaming是的核心概念和关键参数。</p><p>Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行</p><h2 id="Spark-Streaming是什么"><a href="#Spark-Streaming是什么" class="headerlink" title="Spark Streaming是什么"></a>Spark Streaming是什么</h2><p>Spark流使得构建可扩展的容错流应用程序变得更加容易。</p><p>Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。</p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming%E5%9B%BE.jpg" alt="SparkStreaming图"></p><p>和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。<strong>在内部，每个时间区间收到的数据都作为 RDD 存在，而DStream是由这些RDD所组成的序列(因此得名“离散化”)。</strong></p><p>离散流反义词就是连续流。</p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming%E5%9B%BE%E8%A7%A3.png" alt="SparkStreaming图解"></p><h2 id="Spark-Streaming的特点"><a href="#Spark-Streaming的特点" class="headerlink" title="Spark Streaming的特点"></a>Spark Streaming的特点</h2><p><strong>易用</strong></p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming5.jpg" alt="SparkStreaming5"></p><p><strong>容错</strong></p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming6.jpg" alt="SparkStreaming6"></p><p> <strong>易整合到Spark体系</strong><img src="/2020/03/11/SparkStreaming/SparkStreaming7.jpg" alt="SparkStreaming7"></p><h2 id="Spark-Streaming架构"><a href="#Spark-Streaming架构" class="headerlink" title="Spark Streaming架构"></a>Spark Streaming架构</h2><p>最基本的架构：底层就是spark-core。数据采集和封装之后传给Driver，Driver拿到相应的RDD，再形成一个一个Task然后传给Executor执行。。</p><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p> 整体架构图</p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming8.jpg" alt="SparkStreaming8"></p><p>​                                        spark1.5之前</p><p><img src="/2020/03/11/SparkStreaming/spark1.5%E4%B9%8B%E5%90%8E%E6%9E%B6%E6%9E%84.png" alt="spark1.5之后架构"></p><p>​                                       spark1.5之后</p><p>SparkStreaming架构图</p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming9.jpg" alt="SparkStreaming9"></p><h3 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h3><p>背压(back pressure)机制主要用于解决流处理系统中，业务流量在短时间内剧增，造成巨大的流量毛刺，数据流入速度远高于数据处理速度，对流处理系统构成巨大的负载压力的问题。</p><p>如果不能处理流量毛刺或者持续的数据过高速率输入，可能导致Executor端出现OOM的情况或者任务崩溃。</p><h4 id="Spark-1-5以前版本"><a href="#Spark-1-5以前版本" class="headerlink" title="Spark 1.5以前版本"></a>Spark 1.5以前版本</h4><p>用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现（限制每个receiver没每秒最大可以接收的数据量）。此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。</p><p>direct-approach方式接收数据，可以配置 “spark.streaming.kafka.maxRatePerPartition”参数来限制每个kafka分区最多读取的数据量。</p><p>缺点：</p><p>​          1、实现需要进行压测，来设置最大值。参数的设置必须合理，如果集群处理能力高于配置的速率，则会造成资源的浪费。</p><p>​          2、参数需要手动设置，设置过后必须重启streaming服务。</p><h4 id="Spark-1-5以后版本"><a href="#Spark-1-5以后版本" class="headerlink" title="Spark 1.5以后版本"></a>Spark 1.5以后版本</h4><p>为了更好的协调数据接收速率与资源处理能力，1.5版本开始Spark Streaming可以动态控制数据接收速率来适配集群数据处理能力（能够根据当前数据量以及集群状态来预估下个批次最优速率）。背压机制（即Spark Streaming Backpressure）: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。</p><p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。</p><p><strong><u>以下背压机制spark1.5之后流程以及配置均摘抄自：<a href="https://blog.csdn.net/may_fly/article/details/103922862" target="_blank" rel="noopener">https://blog.csdn.net/may_fly/article/details/103922862</a></u></strong></p><p>新版具体流程如下：<img src="/2020/03/11/SparkStreaming/SparkStreaming3.png" alt="SparkStreaming3"></p><p>新版的背压机制主要通过<code>RateController</code>组件来实现。<code>RateController</code>继承了接口<code>StreamingListener</code>并实现了<code>onBatchCompleted</code>方法。</p><p>结合direct-approach方式的源码来理解</p><ol><li>首先创建一个kafka流。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>,<span class="type">String</span>,<span class="type">StringDecoder</span>,<span class="type">StringDecoder</span>,(<span class="type">String</span>,<span class="type">String</span>)](streamingContext, kafkaParams, getOffsets(topics,kc,kafkaParams),messageHandler)</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure><ol><li>createDirectStream方法创建并返回一个DirectKafkaInputDStream对象</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create an input stream that directly pulls messages from Kafka Brokers</span></span><br><span class="line"><span class="comment">   * without using any receiver. This stream can guarantee that each message</span></span><br><span class="line"><span class="comment">   * from Kafka is included in transformations exactly once (see points below).</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Points to note:</span></span><br><span class="line"><span class="comment">   *  - No receivers: This stream does not use any receiver. It directly queries Kafka</span></span><br><span class="line"><span class="comment">   *  - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked</span></span><br><span class="line"><span class="comment">   *    by the stream itself. For interoperability with Kafka monitoring tools that depend on</span></span><br><span class="line"><span class="comment">   *    Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.</span></span><br><span class="line"><span class="comment">   *    You can access the offsets used in each batch from the generated RDDs (see</span></span><br><span class="line"><span class="comment">   *    [[org.apache.spark.streaming.kafka.HasOffsetRanges]]).</span></span><br><span class="line"><span class="comment">   *  - Failure Recovery: To recover from driver failures, you have to enable checkpointing</span></span><br><span class="line"><span class="comment">   *    in the `StreamingContext`. The information on consumed offset can be</span></span><br><span class="line"><span class="comment">   *    recovered from the checkpoint. See the programming guide for details (constraints, etc.).</span></span><br><span class="line"><span class="comment">   *  - End-to-end semantics: This stream ensures that every records is effectively received and</span></span><br><span class="line"><span class="comment">   *    transformed exactly once, but gives no guarantees on whether the transformed data are</span></span><br><span class="line"><span class="comment">   *    outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure</span></span><br><span class="line"><span class="comment">   *    that the output operation is idempotent, or use transactions to output records atomically.</span></span><br><span class="line"><span class="comment">   *    See the programming guide for more details.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param ssc StreamingContext object</span></span><br><span class="line"><span class="comment">   * @param kafkaParams Kafka &lt;a href="http://kafka.apache.org/documentation.html#configuration"&gt;</span></span><br><span class="line"><span class="comment">   *    configuration parameters&lt;/a&gt;. Requires "metadata.broker.list" or "bootstrap.servers"</span></span><br><span class="line"><span class="comment">   *    to be set with Kafka broker(s) (NOT zookeeper servers) specified in</span></span><br><span class="line"><span class="comment">   *    host1:port1,host2:port2 form.</span></span><br><span class="line"><span class="comment">   * @param fromOffsets Per-topic/partition Kafka offsets defining the (inclusive)</span></span><br><span class="line"><span class="comment">   *    starting point of the stream</span></span><br><span class="line"><span class="comment">   * @param messageHandler Function for translating each message and metadata into the desired type</span></span><br><span class="line"><span class="comment">   * @tparam K type of Kafka message key</span></span><br><span class="line"><span class="comment">   * @tparam V type of Kafka message value</span></span><br><span class="line"><span class="comment">   * @tparam KD type of Kafka message key decoder</span></span><br><span class="line"><span class="comment">   * @tparam VD type of Kafka message value decoder</span></span><br><span class="line"><span class="comment">   * @tparam R type returned by messageHandler</span></span><br><span class="line"><span class="comment">   * @return DStream of R</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDirectStream</span></span>[</span><br><span class="line">    <span class="type">K</span>: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">V</span>: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">KD</span> &lt;: <span class="type">Decoder</span>[<span class="type">K</span>]: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">VD</span> &lt;: <span class="type">Decoder</span>[<span class="type">V</span>]: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">R</span>: <span class="type">ClassTag</span>] (</span><br><span class="line">      ssc: <span class="type">StreamingContext</span>,</span><br><span class="line">      kafkaParams: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">      fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>],</span><br><span class="line">      messageHandler: <span class="type">MessageAndMetadata</span>[<span class="type">K</span>, <span class="type">V</span>] =&gt; <span class="type">R</span></span><br><span class="line">  ): <span class="type">InputDStream</span>[<span class="type">R</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanedHandler = ssc.sc.clean(messageHandler)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DirectKafkaInputDStream</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">KD</span>, <span class="type">VD</span>, <span class="type">R</span>](</span><br><span class="line">      ssc, kafkaParams, fromOffsets, cleanedHandler)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152</span></span><br></pre></td></tr></table></figure><ol><li>DirectKafkaInputDStream类继承了抽象类InputDStream，并重载了rateController方法。创建了DirectKafkaRateController类，并传入了一个速率估计类。如果设置RateController.isBackPressureEnabled为true也就是开启背压则开始计算下一次的最优速率</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Asynchronously maintains &amp; sends new rate limits to the receiver through the receiver tracker.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span>[streaming] <span class="keyword">val</span> rateController: <span class="type">Option</span>[<span class="type">RateController</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">RateController</span>.isBackPressureEnabled(ssc.conf)) &#123;</span><br><span class="line">      <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">DirectKafkaRateController</span>(id,</span><br><span class="line">        <span class="type">RateEstimator</span>.create(ssc.conf, context.graph.batchDuration)))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">1234567891011</span></span><br></pre></td></tr></table></figure><ol><li>DirectKafkaRateController内部实现了一个私有类来计算速率，publish方法使用lambda表达式调用了RateController中唯一一个公有的方法onBatchCompleted获取计算结果</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A RateController to retrieve the rate from RateEstimator.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[streaming] <span class="class"><span class="keyword">class</span> <span class="title">DirectKafkaRateController</span>(<span class="params">id: <span class="type">Int</span>, estimator: <span class="type">RateEstimator</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">RateController</span>(<span class="params">id, estimator</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">publish</span></span>(rate: <span class="type">Long</span>): <span class="type">Unit</span> = ()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">1234567</span></span><br></pre></td></tr></table></figure><ol><li>onBatchCompleted获取三个时间一个数据量：处理结束时间，处理时间，等待时间，当前处理数据量，并调用computeAndPublish方法计算下次最优的数据量</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBatchCompleted</span></span>(batchCompleted: <span class="type">StreamingListenerBatchCompleted</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> elements = batchCompleted.batchInfo.streamIdToInputInfo</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">      processingEnd &lt;- batchCompleted.batchInfo.processingEndTime</span><br><span class="line">      workDelay &lt;- batchCompleted.batchInfo.processingDelay</span><br><span class="line">      waitDelay &lt;- batchCompleted.batchInfo.schedulingDelay</span><br><span class="line">      elems &lt;- elements.get(streamUID).map(_.numRecords)</span><br><span class="line">    &#125; computeAndPublish(processingEnd, elems, workDelay, waitDelay)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">12345678910</span></span><br></pre></td></tr></table></figure><ol><li>computeAndPublish调用rateEstimator.compute方法计算速率</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Compute the new rate limit and publish it asynchronously.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">computeAndPublish</span></span>(time: <span class="type">Long</span>, elems: <span class="type">Long</span>, workDelay: <span class="type">Long</span>, waitDelay: <span class="type">Long</span>): <span class="type">Unit</span> =</span><br><span class="line">    <span class="type">Future</span>[<span class="type">Unit</span>] &#123;</span><br><span class="line">      <span class="keyword">val</span> newRate = rateEstimator.compute(time, elems, workDelay, waitDelay)</span><br><span class="line">      newRate.foreach &#123; s =&gt;</span><br><span class="line">        rateLimit.set(s.toLong)</span><br><span class="line">        publish(getLatestRate())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Computes the number of records the stream attached to this `RateEstimator`</span></span><br><span class="line"><span class="comment">   * should ingest per second, given an update on the size and completion</span></span><br><span class="line"><span class="comment">   * times of the latest batch.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param time The timestamp of the current batch interval that just finished</span></span><br><span class="line"><span class="comment">   * @param elements The number of records that were processed in this batch</span></span><br><span class="line"><span class="comment">   * @param processingDelay The time in ms that took for the job to complete</span></span><br><span class="line"><span class="comment">   * @param schedulingDelay The time in ms that the job spent in the scheduling queue</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>,</span><br><span class="line">      elements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>,</span><br><span class="line">      schedulingDelay: <span class="type">Long</span>): <span class="type">Option</span>[<span class="type">Double</span>]</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627</span></span><br></pre></td></tr></table></figure><ol><li>compute方法的具体实现，需要来看3中<code>RateEstimator.create(ssc.conf, context.graph.batchDuration)))</code>传入的RateEstimator类。由源码可知，默认调用pid速率估计器，是 <code>RateEstimator</code>的唯一实现 ，具体计算逻辑要看pid速率估计器的compute方法</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RateEstimator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new `RateEstimator` based on the value of</span></span><br><span class="line"><span class="comment">   * `spark.streaming.backpressure.rateEstimator`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * The only known and acceptable estimator right now is `pid`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return An instance of RateEstimator</span></span><br><span class="line"><span class="comment">   * @throws IllegalArgumentException if the configured RateEstimator is not `pid`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(conf: <span class="type">SparkConf</span>, batchInterval: <span class="type">Duration</span>): <span class="type">RateEstimator</span> =</span><br><span class="line">    conf.get(<span class="string">"spark.streaming.backpressure.rateEstimator"</span>, <span class="string">"pid"</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"pid"</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> proportional = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.proportional"</span>, <span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">val</span> integral = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.integral"</span>, <span class="number">0.2</span>)</span><br><span class="line">        <span class="keyword">val</span> derived = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.derived"</span>, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">val</span> minRate = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.minRate"</span>, <span class="number">100</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PIDRateEstimator</span>(batchInterval.milliseconds, proportional, integral, derived, minRate)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> estimator =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Unknown rate estimator: <span class="subst">$estimator</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="number">123456789101112131415161718192021222324</span></span><br></pre></td></tr></table></figure><ol><li>pid速率估计器的compute方法如下。具体流程不再细述，有时间举个例子推一下</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      numElements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      schedulingDelay: <span class="type">Long</span> <span class="comment">// in milliseconds</span></span><br><span class="line">    ): <span class="type">Option</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    logTrace(<span class="string">s"\ntime = <span class="subst">$time</span>, # records = <span class="subst">$numElements</span>, "</span> +</span><br><span class="line">      <span class="string">s"processing time = <span class="subst">$processingDelay</span>, scheduling delay = <span class="subst">$schedulingDelay</span>"</span>)</span><br><span class="line">    <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (time &gt; latestTime &amp;&amp; numElements &gt; <span class="number">0</span> &amp;&amp; processingDelay &gt; <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in seconds, should be close to batchDuration</span></span><br><span class="line">        <span class="keyword">val</span> delaySinceUpdate = (time - latestTime).toDouble / <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> processingRate = numElements.toDouble / processingDelay * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// In our system `error` is the difference between the desired rate and the measured rate</span></span><br><span class="line">        <span class="comment">// based on the latest batch information. We consider the desired rate to be latest rate,</span></span><br><span class="line">        <span class="comment">// which is what this estimator calculated for the previous batch.</span></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> error = latestRate - processingRate</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The error integral, based on schedulingDelay as an indicator for accumulated errors.</span></span><br><span class="line">        <span class="comment">// A scheduling delay s corresponds to s * processingRate overflowing elements. Those</span></span><br><span class="line">        <span class="comment">// are elements that couldn't be processed in previous batches, leading to this delay.</span></span><br><span class="line">        <span class="comment">// In the following, we assume the processingRate didn't change too much.</span></span><br><span class="line">        <span class="comment">// From the number of overflowing elements we can calculate the rate at which they would be</span></span><br><span class="line">        <span class="comment">// processed by dividing it by the batch interval. This rate is our "historical" error,</span></span><br><span class="line">        <span class="comment">// or integral part, since if we subtracted this rate from the previous "calculated rate",</span></span><br><span class="line">        <span class="comment">// there wouldn't have been any overflowing elements, and the scheduling delay would have</span></span><br><span class="line">        <span class="comment">// been zero.</span></span><br><span class="line">        <span class="comment">// (in elements/second)</span></span><br><span class="line">        <span class="keyword">val</span> historicalError = schedulingDelay.toDouble * processingRate / batchIntervalMillis</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/(second ^ 2)</span></span><br><span class="line">        <span class="keyword">val</span> dError = (error - latestError) / delaySinceUpdate</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> newRate = (latestRate - proportional * error -</span><br><span class="line">                                    integral * historicalError -</span><br><span class="line">                                    derivative * dError).max(minRate)</span><br><span class="line">        logTrace(<span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">            | latestRate = $latestRate, error = $error</span></span><br><span class="line"><span class="string">            | latestError = $latestError, historicalError = $historicalError</span></span><br><span class="line"><span class="string">            | delaySinceUpdate = $delaySinceUpdate, dError = $dError</span></span><br><span class="line"><span class="string">            "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        latestTime = time</span><br><span class="line">        <span class="keyword">if</span> (firstRun) &#123;</span><br><span class="line">          latestRate = processingRate</span><br><span class="line">          latestError = <span class="number">0</span>D</span><br><span class="line">          firstRun = <span class="literal">false</span></span><br><span class="line">          logTrace(<span class="string">"First run, rate estimation skipped"</span>)</span><br><span class="line">          <span class="type">None</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          latestRate = newRate</span><br><span class="line">          latestError = error</span><br><span class="line">          logTrace(<span class="string">s"New rate = <span class="subst">$newRate</span>"</span>)</span><br><span class="line">          <span class="type">Some</span>(newRate)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logTrace(<span class="string">"Rate estimation skipped"</span>)</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566</span></span><br></pre></td></tr></table></figure><ol><li>虽然通过这个公式计算出了一个速率，但最终的速率并不一定是计算出的结果。由代码可知，如果设置了参数spark.streaming.kafka.maxRatePerPartition，则每个分区所取数据最大量为计算出的结果以及设置参数的最小值，否则直接使用计算出的值</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span>[streaming] <span class="function"><span class="keyword">def</span> <span class="title">maxMessagesPerPartition</span></span>(</span><br><span class="line">      offsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>]): <span class="type">Option</span>[<span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> estimatedRateLimit = rateController.map(_.getLatestRate())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate a per-partition rate limit based on current lag</span></span><br><span class="line">    <span class="keyword">val</span> effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ &gt; <span class="number">0</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(rate) =&gt;</span><br><span class="line">        <span class="keyword">val</span> lagPerPartition = offsets.map &#123; <span class="keyword">case</span> (tp, offset) =&gt;</span><br><span class="line">          tp -&gt; <span class="type">Math</span>.max(offset - currentOffsets(tp), <span class="number">0</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> totalLag = lagPerPartition.values.sum</span><br><span class="line"></span><br><span class="line">        lagPerPartition.map &#123; <span class="keyword">case</span> (tp, lag) =&gt;</span><br><span class="line">          <span class="keyword">val</span> backpressureRate = <span class="type">Math</span>.round(lag / totalLag.toFloat * rate)</span><br><span class="line">          tp -&gt; (<span class="keyword">if</span> (maxRateLimitPerPartition &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">Math</span>.min(backpressureRate, maxRateLimitPerPartition)&#125; <span class="keyword">else</span> backpressureRate)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; offsets.map &#123; <span class="keyword">case</span> (tp, offset) =&gt; tp -&gt; maxRateLimitPerPartition &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> maxRateLimitPerPartition: <span class="type">Long</span> = context.sparkContext.getConf.getLong(</span><br><span class="line">      <span class="string">"spark.streaming.kafka.maxRatePerPartition"</span>, <span class="number">0</span>)</span><br><span class="line"><span class="number">12345678910111213141516171819202122</span></span><br></pre></td></tr></table></figure><p>一些相关的参数：</p><ol><li>开启背压机制：设置<strong>spark.streaming.backpressure.enabled</strong> 为true，默认为false</li><li>启用反压机制时每个接收器接收第一批数据的初始最大速率。默认值没有设置 <strong>spark.streaming.backpressure.initialRate</strong></li><li>速率估算器类，默认值为 pid ，目前 Spark 只支持这个，大家可以根据自己的需要实现 <strong>spark.streaming.backpressure.rateEstimator</strong></li><li>用于响应错误的权重（最后批次和当前批次之间的更改）。默认值为1，只能设置成非负值。<em>weight for response to “error” (change between last batch and this batch)</em> <strong>spark.streaming.backpressure.pid.proportional</strong></li><li>错误积累的响应权重，具有抑制作用（有效阻尼）。默认值为 0.2 ，只能设置成非负值。<em>weight for the response to the accumulation of error. This has a dampening effect.</em> <strong>spark.streaming.backpressure.pid.integral</strong></li><li>对错误趋势的响应权重。 这可能会引起 batch size 的波动，可以帮助快速增加/减少容量。默认值为0，只能设置成非负值。<em>weight for the response to the trend in error. This can cause arbitrary/noise-induced fluctuations in batch size, but can also help react quickly to increased/reduced capacity.</em> <strong>spark.streaming.backpressure.pid.derived</strong></li><li>可以估算的最低费率是多少。默认值为 100，只能设置成非负值。 <strong>spark.streaming.backpressure.pid.minRate</strong></li></ol><p>参考：</p><p><a href="https://blog.csdn.net/wangpei1949/article/details/90727805" target="_blank" rel="noopener">https://blog.csdn.net/wangpei1949/article/details/90727805</a></p><p><a href="https://blog.csdn.net/zengxiaosen/article/details/72822869" target="_blank" rel="noopener">https://blog.csdn.net/zengxiaosen/article/details/72822869</a></p><p><a href="https://www.cnblogs.com/barrenlake/p/5349949.html" target="_blank" rel="noopener">https://www.cnblogs.com/barrenlake/p/5349949.html</a></p><p><a href="https://www.iteblog.com/archives/2323.html?from=related" target="_blank" rel="noopener">https://www.iteblog.com/archives/2323.html?from=related</a></p><h1 id="Dstream入门"><a href="#Dstream入门" class="headerlink" title="Dstream入门"></a>Dstream入门</h1><h2 id="WordCount案例实操"><a href="#WordCount案例实操" class="headerlink" title="WordCount案例实操"></a>WordCount案例实操</h2><p>需求：使用netcat工具向9999端口不断的发送数据，通过SparkStreaming读取端口数据并统计不同单词出现的次数</p><p>1) 添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2) 编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Wordcount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Todo 1.配置对象</span></span><br><span class="line">    <span class="comment">//初始化Spark配置信息</span></span><br><span class="line">    <span class="comment">//Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行（local至少要两个节点）</span></span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Todo 2.环境对象</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    class StreamingContext private[streaming]加了包权限（私有） 所以主构不能用</span></span><br><span class="line"><span class="comment">    def this(sparkContext: SparkContext, batchDuration: Duration) 辅助构建方法可用（Spark配置信息，批处理持续时间）</span></span><br><span class="line"><span class="comment">    case class Duration (private val millis: Long)样例类 直接用 但是不方便 要自己算毫秒</span></span><br><span class="line"><span class="comment">    new StreamingContext(sparkconf , Duration(1000 * 3))</span></span><br><span class="line"><span class="comment">    所以直接使用伴生对象Seconds()</span></span><br><span class="line"><span class="comment">     object Seconds &#123;</span></span><br><span class="line"><span class="comment">        def apply(seconds: Long): Duration = new Duration(seconds * 1000)</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 初始化SparkStreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">3</span>)) <span class="comment">// 创建对象的第二个参数表示数据的采集周期</span></span><br><span class="line">    <span class="comment">//Todo 3.数据处理</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 从数据源采集数据</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 //内存和磁盘有两个序列化副本（socketStream默认的存储级别）</span></span><br><span class="line"><span class="comment">    ReceiverInputDStream[String]  Receiver:接收器 Input:输入  DStream:离散化流   */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span> )</span><br><span class="line">    <span class="comment">//TODO 将采集数据进行WordCount的处理</span></span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = socketDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在控制台上打印结果</span></span><br><span class="line"><span class="comment">/*    可以看出底层还是RDD</span></span><br><span class="line"><span class="comment">def print(num: Int): Unit = ssc.withScope &#123;</span></span><br><span class="line"><span class="comment">      def foreachFunc: (RDD[T], Time) =&gt; Unit = &#123;</span></span><br><span class="line"><span class="comment">        (rdd: RDD[T], time: Time) =&gt; &#123;</span></span><br><span class="line"><span class="comment">          val firstNum = rdd.take(num + 1)</span></span><br><span class="line"><span class="comment">          // scalastyle:off println</span></span><br><span class="line"><span class="comment">          println("-------------------------------------------")</span></span><br><span class="line"><span class="comment">          println(s"Time: $time")</span></span><br><span class="line"><span class="comment">          println("-------------------------------------------")</span></span><br><span class="line"><span class="comment">          firstNum.take(num).foreach(println)</span></span><br><span class="line"><span class="comment">          if (firstNum.length &gt; num) println("...")</span></span><br><span class="line"><span class="comment">          println()</span></span><br><span class="line"><span class="comment">          // scalastyle:on println</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Todo 4.开启连接环境</span></span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">    和spark、scala不同的是：最后并不关闭连接环境（除非程序升级或者出现故障的时候，因为数据采集是要7*24）</span></span><br><span class="line"><span class="comment">    并且不能让driver程序结束，需要让driver程序等待,等待数据处理的停止或异常时，才会继续执行</span></span><br><span class="line"><span class="comment">      def awaitTermination() &#123;</span></span><br><span class="line"><span class="comment">        waiter.waitForStopOrError()  //等待停止或者异常，如果没有停止和异常现象 程序不结束</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    ssc.start();</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3) 启动程序并通过netcat发送数据：</p><blockquote><p>nc -lk 9999</p><p>hello red</p><p>hello world</p></blockquote><h2 id="WordCount解析"><a href="#WordCount解析" class="headerlink" title="WordCount解析"></a>WordCount解析</h2><p>Discretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，<strong>DStream是一系列连续的RDD来表示</strong>。<u>每个RDD含有一段时间间隔内的数据（段时间内所有数据在一个RDD里）。</u><img src="/2020/03/11/SparkStreaming/SparkStreaming10.jpg" alt="SparkStreaming10"></p><p>对数据的操作也是按照RDD为单位来进行的</p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming11.jpg" alt="SparkStreaming11"></p><p>计算过程由Spark Engine来完成</p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming12.jpg" alt="SparkStreaming12"></p><h1 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h1><h2 id="RDD队列"><a href="#RDD队列" class="headerlink" title="RDD队列"></a>RDD队列</h2><h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue()"></a>Queue()</h3><h4 id="用法及说明"><a href="#用法及说明" class="headerlink" title="用法及说明"></a>用法及说明</h4><p><font color="red">测试过程</font>中，可以通过使用ssc.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。</p><h4 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h4><p>需求：循环创建几个RDD，将RDD放入队列。通过SparkStream创建Dstream，计算WordCount</p><p>1) 编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDStream</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.初始化Spark配置信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDDStream"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.初始化SparkStreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.创建RDD队列</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.创建QueueInputDStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.处理队列中的RDD数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.打印结果</span></span><br><span class="line"></span><br><span class="line">  reducedStream.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.启动任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//8.循环创建并向RDD队列中放入RDD</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line"></span><br><span class="line">   rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">   <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2) 结果展示</p><blockquote><p>-——————————————</p><p>Time: 1539075280000 ms</p><p>-——————————————</p><p>(4,60)</p><p>(0,60)</p><p>(6,60)</p><p>(8,60)</p><p>(2,60)</p><p>(1,60)</p><p>(3,60)</p><p>(7,60)</p><p>(9,60)</p><p>(5,60)</p><p>-——————————————</p><p>Time: 1539075284000 ms</p><p>-——————————————</p><p>(4,60)</p><p>(0,60)</p><p>(6,60)</p><p>(8,60)</p><p>(2,60)</p><p>(1,60)</p><p>(3,60)</p><p>(7,60)</p><p>(9,60)</p><p>(5,60)</p><p>-——————————————</p><p>Time: 1539075288000 ms</p><p>-——————————————</p><p>(4,30)</p><p>(0,30)</p><p>(6,30)</p><p>(8,30)</p><p>(2,30)</p><p>(1,30)</p><p>(3,30)</p><p>(7,30)</p><p>(9,30)</p><p>(5,30)</p><p>-——————————————</p><p>Time: 1539075292000 ms</p><p>-——————————————</p></blockquote><h3 id="file"><a href="#file" class="headerlink" title="file()"></a>file()</h3><h4 id="用法及说明-1"><a href="#用法及说明-1" class="headerlink" title="用法及说明"></a>用法及说明</h4><p><font color="red">测试过程</font>中，可以通过使用ssc.textFileStream(“in”)来创建DStream，监控文件夹的变化。</p><p>从文件夹中读取新的文件数据（拽过去的文件可能读不到），功能不稳定 ，所以不推荐使用</p><p>flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume</p><h4 id="案例实操-1"><a href="#案例实操-1" class="headerlink" title="案例实操"></a>案例实操</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming03_DStream_File</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 数据处理</span></span><br><span class="line">    <span class="comment">// 从文件夹中读取新的文件数据，功能不稳定 ，所以不推荐使用</span></span><br><span class="line">    <span class="comment">// flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume</span></span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">DStream</span>[<span class="type">String</span>] = ssc.textFileStream(<span class="string">"in"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map( (_, <span class="number">1</span>) )</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭连接环境</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h2><h3 id="用法及说明-2"><a href="#用法及说明-2" class="headerlink" title="用法及说明"></a>用法及说明</h3><p>需要继承Receiver，并实现onStart、onStop方法来自定义数据源采集。</p><h3 id="案例实操-2"><a href="#案例实操-2" class="headerlink" title="案例实操"></a>案例实操</h3><p>需求：自定义数据源，实现监控某个端口号，获取该端口号内容。</p><p>1) 自定义数据源</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  自定义数据采集器</span></span><br><span class="line"><span class="comment">    自定义数据采集器</span></span><br><span class="line"><span class="comment">    模仿spark自带的socket采集器</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  abstract class Receiver[T](val storageLevel: StorageLevel) extends Serializable</span></span><br><span class="line"><span class="comment">  StorageLevel :存储级别 MEMORY_ONLY DISK_ONLY MEMORY_AND_DISK</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  步骤： 1. 继承Receiver ,设定泛型（采集数据的类型）, 传递参数</span></span><br><span class="line"><span class="comment">         2. 重写方法</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyRecevier</span>(<span class="params">host : <span class="type">String</span> , port : <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>)</span>&#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   socketTextStream:  socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)</span></span><br><span class="line"><span class="comment">    socketStream：  new SocketInputDStream[T](this, hostname, port, converter, storageLevel)</span></span><br><span class="line"><span class="comment">    SocketInputDStream:  new SocketReceiver(host, port, bytesToObjects, storageLevel)</span></span><br><span class="line"><span class="comment">    SocketReceiver：extends Receiver[T](storageLevel) with Logging</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> socket: <span class="type">Socket</span> = _</span><br><span class="line">    <span class="comment">// SocketReceiver：socket.getInputStream()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">      <span class="comment">//我们需要字符串，所以将字节流转换为缓冲字符流</span></span><br><span class="line">      <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">InputStreamReader</span>(</span><br><span class="line">          <span class="comment">//获取从网络中传递来的数据（字节流）</span></span><br><span class="line">          socket.getInputStream,</span><br><span class="line">          <span class="string">"UTF-8"</span></span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      <span class="keyword">var</span> s:<span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">     <span class="comment">/*</span></span><br><span class="line"><span class="comment">      这个“s = reader.readLine())!= null”语句是错误的，因为在网络编程中获取的数据是没有null的概念</span></span><br><span class="line"><span class="comment">      文件读取时，如果读到结束的时候，获取的结果为null（文件读取这样是对的）</span></span><br><span class="line"><span class="comment">      但是在网络中我可以现在传递一些数据  过一段就再传一次，所以null是无法判断的</span></span><br><span class="line"><span class="comment">      网络编程中，需要明确告知服务器，客户端不再传数据，需要发送特殊的指令</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      <span class="keyword">while</span>(( s = reader.readLine())!= <span class="literal">null</span>)&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需要发送特殊的指令</span></span><br><span class="line">        <span class="keyword">if</span>(s != <span class="string">"-END-"</span>)&#123;</span><br><span class="line">          <span class="comment">//采集到数据后，进行封装(存储)</span></span><br><span class="line">              store(s)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          <span class="comment">// stop</span></span><br><span class="line">          <span class="comment">// close</span></span><br><span class="line">          <span class="comment">// 重启</span></span><br><span class="line">          <span class="comment">//restart("")</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 启动采集器</span></span><br><span class="line">    <span class="comment">// 采集 &amp; 封装</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      socket = <span class="keyword">new</span> <span class="type">Socket</span>(host , port)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"Socket Receiver"</span>) &#123;</span><br><span class="line">        setDaemon(<span class="literal">true</span>)<span class="comment">//守护线程</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">          receive() &#125;</span><br><span class="line">      &#125;.start()<span class="comment">//start()会回调run()方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (socket != <span class="literal">null</span>)&#123;</span><br><span class="line">        socket.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>2) 使用自定义的数据源采集数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamig_DIY</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span>  <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"DIY采集器"</span>)</span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// TODO 数据处理</span></span><br><span class="line">    <span class="comment">// 自定义数据采集器</span></span><br><span class="line">    <span class="keyword">val</span> myDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.receiverStream(<span class="keyword">new</span> <span class="type">MyRecevier</span>(<span class="string">"localhost"</span> , <span class="number">9999</span>))</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = myDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line">    <span class="comment">// TODO 开启连接环境</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h2><h3 id="版本选型"><a href="#版本选型" class="headerlink" title="版本选型"></a>版本选型</h3><p><strong>ReceiverAPI</strong>：需要一个专门的Executor去接收数据，然后发送给其他的Executor做计算。存在的问题，接收数据的Executor和计算的Executor速度会有所不同，特别在接收数据的Executor速度大于计算的Executor速度，会导致计算数据的节点内存溢出。</p><p><strong>DirectAPI</strong>：是由计算的Executor来主动消费Kafka的数据，速度由自身控制。</p><p> <img src="/2020/03/11/SparkStreaming/SparkStreaming13.jpg" alt="SparkStreaming13"></p><h3 id="Kafka-0-8-Receiver模式"><a href="#Kafka-0-8-Receiver模式" class="headerlink" title="Kafka 0-8 Receiver模式"></a>Kafka 0-8 Receiver模式</h3><p>1） 需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p><p>2）导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3）编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Kafka</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span>  <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"DIY采集器"</span>)</span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// TODO 数据处理 - 读取Kafka数据创建DStream(基于Receive方式)</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    def createStream(</span></span><br><span class="line"><span class="comment">                      ssc : org.apache.spark.streaming.StreamingContext,</span></span><br><span class="line"><span class="comment">                      zkQuorum : scala.Predef.String, //zookeeper</span></span><br><span class="line"><span class="comment">                      groupId : scala.Predef.String, //消费者组</span></span><br><span class="line"><span class="comment">                      topics : scala.Predef.Map[scala.Predef.String, scala.Int],//分区数</span></span><br><span class="line"><span class="comment">                      storageLevel : org.apache.spark.storage.StorageLevel = &#123;  compiled code  &#125;</span></span><br><span class="line"><span class="comment">                    ) : org.apache.spark.streaming.dstream.ReceiverInputDStream[scala.Tuple2[scala.Predef.String, scala.Predef.String]] = &#123;  compiled code  &#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaDS: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="string">"linux1:2181,linux2:2181,linux3:2181"</span>,</span><br><span class="line">      <span class="string">"red0819"</span>,</span><br><span class="line">      <span class="type">Map</span>(<span class="string">"red0819"</span> -&gt; <span class="number">3</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// Kafka消息传递的时候以k-v对</span></span><br><span class="line">    <span class="comment">// k - 传值的时候提供的，默认为null,主要用于分区</span></span><br><span class="line">    <span class="comment">// v - message</span></span><br><span class="line">    kafkaDS.map((_._2)).print()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Kafka-0-8-Direct模式"><a href="#Kafka-0-8-Direct模式" class="headerlink" title="Kafka 0-8 Direct模式"></a>Kafka 0-8 Direct模式</h3><p>1）需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p><p>2）导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3）编写代码</p><h4 id="自动维护offset"><a href="#自动维护offset" class="headerlink" title="自动维护offset"></a>自动维护offset</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIAuto02</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> getSSC1: () =&gt; <span class="type">StreamingContext</span> = () =&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">getSSC</span></span>: <span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//设置CK</span></span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck2"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.定义Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.读取Kafka数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc,</span><br><span class="line"></span><br><span class="line">   kafkaPara,</span><br><span class="line"></span><br><span class="line">   <span class="type">Set</span>(<span class="string">"red"</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.计算WordCount</span></span><br><span class="line"></span><br><span class="line">  kafkaDStream.map(_._2)</span><br><span class="line"></span><br><span class="line">   .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">   .print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.返回数据</span></span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取SSC</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck2"</span>, () =&gt; getSSC)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="手动维护offset"><a href="#手动维护offset" class="headerlink" title="手动维护offset"></a>手动维护offset</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.common.<span class="type">TopicAndPartition</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.message.<span class="type">MessageAndMetadata</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.&#123;<span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">OffsetRange</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIHandler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.获取上一次启动最后保留的Offset=&gt;getOffset(MySQL)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>] = <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>](<span class="type">TopicAndPartition</span>(<span class="string">"red"</span>, <span class="number">0</span>) -&gt; <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.读取Kafka数据创建DStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">String</span>] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>, <span class="type">String</span>](ssc,</span><br><span class="line"></span><br><span class="line">   kafkaPara,</span><br><span class="line"></span><br><span class="line">   fromOffsets,</span><br><span class="line"></span><br><span class="line">   (m: <span class="type">MessageAndMetadata</span>[<span class="type">String</span>, <span class="type">String</span>]) =&gt; m.message())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.创建一个数组用于存放当前消费数据的offset信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> offsetRanges = <span class="type">Array</span>.empty[<span class="type">OffsetRange</span>]</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.获取当前消费数据的offset信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToCountDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = kafkaDStream.transform &#123; rdd =&gt;</span><br><span class="line"></span><br><span class="line">   offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">   rdd</span><br><span class="line"></span><br><span class="line">  &#125;.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//8.打印Offset信息</span></span><br><span class="line"></span><br><span class="line">  wordToCountDStream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span>:<span class="subst">$&#123;o.partition&#125;</span>:<span class="subst">$&#123;o.fromOffset&#125;</span>:<span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//9.开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Kafka-0-10-Direct模式"><a href="#Kafka-0-10-Direct模式" class="headerlink" title="Kafka 0-10 Direct模式"></a>Kafka 0-10 Direct模式</h3><p>1）需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p><p>2）导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3）编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPI</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.定义Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.读取Kafka数据创建DStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line"></span><br><span class="line">   <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"red"</span>), kafkaPara))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.将每条消息的KV取出</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> valueDStream: <span class="type">DStream</span>[<span class="type">String</span>] = kafkaDStream.map(record =&gt; record.value())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.计算WordCount</span></span><br><span class="line"></span><br><span class="line">  valueDStream.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">   .print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="消费Kafka数据模式总结"><a href="#消费Kafka数据模式总结" class="headerlink" title="消费Kafka数据模式总结"></a>消费Kafka数据模式总结</h3><h4 id="0-8"><a href="#0-8" class="headerlink" title="0-8"></a>0-8</h4><h5 id="ReceiverAPI"><a href="#ReceiverAPI" class="headerlink" title="ReceiverAPI"></a>ReceiverAPI</h5><p>1) 专门的Executor读取数据，速度不统一</p><p>  数据丢失：预写日志开启</p><p>2) 跨机器传输数据</p><p>3) Executor读取数据通过多个线程的方式，想要增加并行度，则需要多个流union</p><p>4) offset存储在zookeeper中</p><h5 id="DirectAPI"><a href="#DirectAPI" class="headerlink" title="DirectAPI"></a>DirectAPI</h5><p>1) Executor读取数据并计算</p><p>2) 增加Executor个数来增加消费的并行度</p><p>3) offset存储</p><p>a. CheckPoint(getActiveOrCreate方式创建StreamingContext)</p><p>从checkpoint中读取数据偏移量（不推荐使用）</p><blockquote><p>理由：</p><p>​        checkpoint还保存了计算逻辑，不适合扩展功能<br>​                checkpoint会延续计算，但是可能会压垮内存<br>​                checkpoint一般的存储路径为HDFS，所以会导致小文件过多</p></blockquote><p>b. 手动维护(有事务的存储系统)</p><p>4) 获取offset必须在第一个调用的算子中：</p><p>offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</p><h4 id="0-10-DirectAPI"><a href="#0-10-DirectAPI" class="headerlink" title="0-10 DirectAPI"></a>0-10 DirectAPI</h4><p>1) Executor读取数据并计算</p><p>2) 增加Executor个数来增加消费的并行度</p><p>3) offset存储</p><p>a. __consumer_offsets系统主题中</p><p>b. 手动维护(有事务的存储系统)</p><h1 id="DStream转换"><a href="#DStream转换" class="headerlink" title="DStream转换"></a>DStream转换</h1><p>DStream上的操作与RDD的类似，分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。</p><h2 id="无状态转化操作"><a href="#无状态转化操作" class="headerlink" title="无状态转化操作"></a>无状态转化操作</h2><p>无状态转化操作就是把简单的RDD转化操作应用到每个批次上，也就是转化DStream中的每一个RDD。部分无状态转化操作列在了下表中。注意，针对键值对的DStream转化操作(比如 reduceByKey())要添加import StreamingContext._才能在Scala中使用。</p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming14.jpg" alt="SparkStreaming14"></p><p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个DStream在内部是由许多RDD（批次）组成，且无状态转化操作是分别应用到每个RDD上的。</p><p>例如：reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。</p><h3 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h3><p>Transform允许DStream上执行任意的RDD-to-RDD函数。即使这些函数并没有在DStream的API中暴露出来，通过该函数可以方便的扩展Spark API。该函数每一批次调度一次。其实也就是对DStream中的RDD应用转换。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DStream_WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"queue"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf, <span class="type">Seconds</span>(<span class="number">5</span>));</span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resultDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDS.transform(</span><br><span class="line">      rdd =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> flatRDD: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = flatRDD.map((_, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> reduceRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_ + _)</span><br><span class="line">        reduceRDD</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line">    resultDS.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>两个流之间的join需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的RDD进行join，与两个RDD的join效果相同。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JoinTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"JoinTest"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.从端口获取数据创建流</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lineDStream1: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lineDStream2: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux2"</span>, <span class="number">8888</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.将两个流转换为KV类型</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToOneDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lineDStream1.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToADStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lineDStream2.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="string">"a"</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.流的JOIN</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> joinDStream: <span class="type">DStream</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">String</span>))] = wordToOneDStream.join(wordToADStream)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.打印</span></span><br><span class="line"></span><br><span class="line">  joinDStream.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.启动任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="有状态转化操作"><a href="#有状态转化操作" class="headerlink" title="有状态转化操作"></a>有状态转化操作</h2><h3 id="UpdateStateByKey"><a href="#UpdateStateByKey" class="headerlink" title="UpdateStateByKey"></a>UpdateStateByKey</h3><p>UpdateStateByKey原语用于记录历史记录，有时，我们需要在DStream中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey()为我们提供了对一个状态变量的访问，用于键值对形式的DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。</p><p>updateStateByKey() 的结果会是一个新的DStream，其内部的RDD 序列是由每个时间区间对应的(键，状态)对组成的。</p><p>updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，需要做下面两步：</p><ol><li><p>定义状态，状态可以是一个任意的数据类型。</p></li><li><p>定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。</p></li></ol><p>使用updateStateByKey需要对检查点目录进行配置，会使用检查点来保存状态。</p><p>更新版的wordcount</p><p>1) 编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DSream_State</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = socketDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 使用有状态操作保存数据 updateStateByKey</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"scp"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordToOneDS.updateStateByKey[<span class="type">Long</span>](</span><br><span class="line">      <span class="comment">// TODO 第一个参数表示相同key的value数据集合</span></span><br><span class="line">      <span class="comment">// TODO 第二个参数表示相同key的缓冲区的数据</span></span><br><span class="line">      (seq: <span class="type">Seq</span>[<span class="type">Int</span>], opt: <span class="type">Option</span>[<span class="type">Long</span>]) =&gt; &#123;</span><br><span class="line">        <span class="comment">// TODO 返回值表示更新后的缓冲区的值</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> newValue = opt.getOrElse(<span class="number">0</span>L) + seq.sum</span><br><span class="line">        <span class="type">Option</span>(newValue)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2) 启动程序并向9999端口发送数据</p><blockquote><p>nc -lk 9999</p><p>Hello World</p><p>Hello Scala</p></blockquote><p>3) 结果展示</p><blockquote><p>-——————————————</p><p>Time: 1504685175000 ms</p><p>-——————————————</p><p>-——————————————</p><p>Time: 1504685181000 ms</p><p>-——————————————</p><p>(shi,1)</p><p>(shui,1)</p><p>(ni,1)</p><p>-——————————————</p><p>Time: 1504685187000 ms</p><p>-——————————————</p><p>(shi,1)</p><p>(ma,1)</p><p>(hao,1)</p><p>(shui,1)</p></blockquote><h3 id="WindowOperations"><a href="#WindowOperations" class="headerlink" title="WindowOperations"></a>WindowOperations</h3><p>Window Operations可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。</p><p>窗口时长：计算内容的时间范围；</p><p>滑动步长：隔多久触发一次计算。</p><p><font color="red"><strong>注意：这两者都必须为采集周期大小的整数倍。</strong></font></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WorldCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Split each line into words</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">// Count each word in each batch</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b),<span class="type">Seconds</span>(<span class="number">12</span>), <span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line"></span><br><span class="line">  wordCounts.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.start()       <span class="comment">// Start the computation</span></span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h4><p><img src="/2020/03/11/SparkStreaming/image-20200914120823766.png" alt="image-20200914120823766"></p><h4 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h4><p><img src="/2020/03/11/SparkStreaming/image-20200914120922340.png" alt="image-20200914120922340"></p><p>关于Window的操作还有如下方法：</p><p>（1）window(windowLength, slideInterval): 基于对源DStream窗化的批次进行计算返回一个新的Dstream；</p><p>（2）countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//countByWindow</span></span><br><span class="line">ssc.checkpoint(<span class="string">"scp"</span>)</span><br><span class="line"><span class="keyword">val</span> countDS: <span class="type">DStream</span>[<span class="type">Long</span>] = socketDS.countByWindow(<span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">print(countDS)</span><br></pre></td></tr></table></figure><p>（3）reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；</p><p>（4）reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的DStream上调用此函数，会返回一个新(K,V)对的DStream，此处通过对滑动窗口中批次数据使用reduce函数来整合每个key的value值。(func 两两聚合)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//reduceByKeyAndWindow</span></span><br><span class="line"><span class="keyword">val</span> wordToOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDS.flatMap(_.split(<span class="string">" "</span>)).map((_ , <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> reduceByKeyAndWindowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKeyAndWindow(</span><br><span class="line">  (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x + y, <span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">3</span>)</span><br><span class="line">)</span><br><span class="line">reduceByKeyAndWindowDS.print()</span><br></pre></td></tr></table></figure><p>（5）reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的reduce值都是通过用前一个窗的reduce值来递增计算。通过reduce进入到滑动窗口数据并”反向reduce”离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys的“加”“减”计数。<strong>通过前边介绍可以想到，这个函数只适用于”可逆的reduce函数”，也就是这些reduce函数有相应的”反reduce”函数(以参数invFunc形式传入)。</strong>如前述函数，reduce任务的数量通过可选参数来配置。</p><p><img src="/2020/03/11/SparkStreaming/SparkStreaming15.jpg" alt="SparkStreaming15"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map(logEntry =&gt; (logEntry.getIpAddress(), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> ipCountDStream = ipDStream.reduceByKeyAndWindow(</span><br><span class="line"> &#123;(x, y) =&gt; x + y&#125;,</span><br><span class="line"> &#123;(x, y) =&gt; x - y&#125;,</span><br><span class="line"> <span class="type">Seconds</span>(<span class="number">30</span>),</span><br><span class="line"> <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><p> //加上新进入窗口的批次中的元素 //移除离开窗口的老批次中的元素 //窗口时长// 滑动步长</p><p>countByWindow()和countByValueAndWindow()作为对数据进行计数操作的简写。countByWindow()返回一个表示每个窗口中元素个数的DStream，而countByValueAndWindow()返回的DStream则包含窗口中每个值的个数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map&#123;entry =&gt; entry.getIpAddress()&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ipAddressRequestCount = ipDStream.countByValueAndWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>)) </span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> requestCount = accessLogsDStream.countByWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><h1 id="DStream输出"><a href="#DStream输出" class="headerlink" title="DStream输出"></a>DStream输出</h1><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与RDD中的惰性求值类似，如果一个DStream及其派生出的DStream都没有被执行输出操作，那么这些DStream就都不会被求值。如果StreamingContext中没有设定输出操作，整个context就都不会启动。</p><p>输出操作如下：</p><p><font color="blue">print()</font>：在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫print()。</p><p><font color="blue">saveAsTextFiles(prefix, [suffix])</font>：以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”。</p><p><font color="blue">saveAsObjectFiles(prefix, [suffix])</font>：以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles . 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python中目前不可用。</p><p><font color="blue">saveAsHadoopFiles(prefix, [suffix])</font>：将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”。Python API 中目前不可用。</p><p><font color="blue">foreachRDD(func)</font>：这是最通用的输出操作，即将函数 func 用于产生于 stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">DriverManager</span>, <span class="type">PreparedStatement</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DStream_Output</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">//将数据保存到mysql数据库中</span></span><br><span class="line">    socketDS.foreachRDD(</span><br><span class="line">      rdd =&gt;&#123;</span><br><span class="line">        rdd.foreach(</span><br><span class="line">          data =&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = data.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> id: <span class="type">Int</span> = datas(<span class="number">0</span>).toInt</span><br><span class="line">            <span class="keyword">val</span> name: <span class="type">String</span> = datas(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">val</span> age: <span class="type">Int</span> = datas(<span class="number">0</span>).toInt</span><br><span class="line"></span><br><span class="line">            <span class="comment">//TODO 加载数据库驱动</span></span><br><span class="line">            <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">            <span class="comment">// TODO 建立链接和操作对象</span></span><br><span class="line">            <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(</span><br><span class="line">              <span class="string">"jdbc:mysql://3306/studenttest"</span>,</span><br><span class="line">              <span class="string">"root"</span>,</span><br><span class="line">              <span class="string">"000000"</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">val</span> sql = <span class="string">"insert into student(id,name,age) values (?,?,?)"</span></span><br><span class="line">            <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = conn.prepareStatement(sql)</span><br><span class="line">            statement.setInt(<span class="number">1</span>,id)</span><br><span class="line">            statement.setString(<span class="number">2</span>,name)</span><br><span class="line">            statement.setInt(<span class="number">3</span>,age)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// TODO 操作数据</span></span><br><span class="line">            statement.executeUpdate()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// TODO 关闭连接</span></span><br><span class="line">            statement.close()</span><br><span class="line">            conn.close()</span><br><span class="line"></span><br><span class="line">          &#125;)</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong><font color="red">注：但是在真实场景中数据传播速度快，传递量大，所以上述代码并不适合用在实际操作中。</font></strong></p><p><strong><font color="red">原因：数据来一次建立一次链接，数据库链接创建太多，显然是不合理的。在数据量巨大的情况下用连接池也是不合理的，处理不过来。</font></strong></p><p>通用的输出操作foreachRDD()，它用来对DStream中的RDD运行任意计算。这和transform() 有些类似，都可以让我们访问任意RDD。在foreachRDD()中，可以重用我们在Spark中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如MySQL的外部数据库中。 </p><p>注意：</p><p>1) 连接不能写在driver层面（序列化）</p><p>2) 如果写在foreach则每个RDD中的每一条数据都创建，得不偿失；</p><p>3) 增加foreachPartition，在分区创建（获取）。</p><p>rdd.foreachPartition()：以分区为单位进行遍历，不需要返回</p><p>rdd.foreachPartitions()：以分区为单位进行转换，需要返回</p><h1 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h1><p>流式任务需要7*24小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。</p><p>使用外部文件系统来控制内部程序关闭。</p><p>MonitorStop</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.net.<span class="type">URI</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FileSystem</span>, <span class="type">Path</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">StreamingContext</span>, <span class="type">StreamingContextState</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MonitorStop</span>(<span class="params">ssc: <span class="type">StreamingContext</span></span>) <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fs: <span class="type">FileSystem</span> = <span class="type">FileSystem</span>.get(<span class="keyword">new</span> <span class="type">URI</span>(<span class="string">"hdfs://linux1:9000"</span>), <span class="keyword">new</span> <span class="type">Configuration</span>(), <span class="string">"red"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//优雅关闭判断条件</span></span><br><span class="line"><span class="comment">//stop方法不能放在driver的主线程中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//一般标志不在Driver端，在第三方软件中 eg:redis/zk/mysql/hdfs</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span></span><br><span class="line"></span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">catch</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">InterruptedException</span> =&gt;</span><br><span class="line"></span><br><span class="line">     e.printStackTrace()</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//获取当前线程状态</span></span><br><span class="line">   <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = ssc.getState</span><br><span class="line"> </span><br><span class="line"><span class="comment">// TODO 设置标记，让当前关闭线程可以访问，可以动态改变状态</span></span><br><span class="line">   <span class="keyword">val</span> bool: <span class="type">Boolean</span> = fs.exists(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">"hdfs://linux1:9000/stopSpark"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//为了防止有新线程了之后SparkStreaming直接关闭，所以应该加一个判断条件，</span></span><br><span class="line"><span class="comment">// 并且循环判断，避免判断一次之后不再判断</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (bool) &#123;</span><br><span class="line"><span class="comment">//判断当前状态，如果当前不是激活状态的话根本不用关闭</span></span><br><span class="line">    <span class="keyword">if</span> (state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span>) &#123;</span><br><span class="line"></span><br><span class="line">     ssc.stop(stopSparkContext = <span class="literal">true</span>, stopGracefully = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">     <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SparkTest</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">createSSC</span></span>(): _root_.org.apache.spark.streaming.<span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> update: (<span class="type">Seq</span>[<span class="type">Int</span>], <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; <span class="type">Some</span>[<span class="type">Int</span>] = (values: <span class="type">Seq</span>[<span class="type">Int</span>], status: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="comment">//当前批次内容的计算</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> sum: <span class="type">Int</span> = values.sum</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="comment">//取出状态信息中上一次状态</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> lastStatu: <span class="type">Int</span> = status.getOrElse(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="type">Some</span>(sum + lastStatu)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[4]"</span>).setAppName(<span class="string">"SparkTest"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//****设置优雅的关闭*****</span></span><br><span class="line"></span><br><span class="line">  sparkConf.set(<span class="string">"spark.streaming.stopGracefullyOnShutdown"</span>, <span class="string">"true"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> line: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> word: <span class="type">DStream</span>[<span class="type">String</span>] = line.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordAndOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = word.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordAndCount: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.updateStateByKey(update)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  wordAndCount.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck"</span>, () =&gt; createSSC())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">MonitorStop</span>(ssc)).start()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>优雅关闭是要判断当前有没有数据没有处理完，如果有，先把当前数据处理完再关闭。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="大数据" scheme="http://www.red0819.top/categories/bigdata/"/>
    
    
    <category term="SparkStreaming" scheme="http://www.red0819.top/tags/SparkStreaming/"/>
    
  </entry>
  
  <entry>
    <title>Redis和Jedis</title>
    <link href="http://www.red0819.top/2020/02/23/Redis-Jedis/"/>
    <id>http://www.red0819.top/2020/02/23/Redis-Jedis/</id>
    <published>2020-02-23T12:33:25.000Z</published>
    <updated>2020-09-14T21:07:17.006Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><table><thead><tr><th></th><th>MySQL</th><th>Redis</th></tr></thead><tbody><tr><td>连接</td><td>Connection</td><td>Jedis</td></tr><tr><td>连接池</td><td>C3P0等等</td><td>JedisPool</td></tr><tr><td>操作完成</td><td>关闭连接</td><td>关闭连接</td></tr></tbody></table><h2 id="Redis准备"><a href="#Redis准备" class="headerlink" title="Redis准备"></a>Redis准备</h2><h3 id="①Redis配置文件中bind配置项含义"><a href="#①Redis配置文件中bind配置项含义" class="headerlink" title="①Redis配置文件中bind配置项含义"></a>①Redis配置文件中bind配置项含义</h3><p>bind后面跟的ip地址是客户端访问Redis时使用的IP地址。</p><table><thead><tr><th>bind值</th><th>访问方式</th></tr></thead><tbody><tr><td>127.0.0.1</td><td>./redis-cli -h 127.0.0.1</td></tr><tr><td>192.168.200.100</td><td>./redis-cli -h 192.168.200.100</td></tr></tbody></table><h3 id="②查看Linux系统本机IP"><a href="#②查看Linux系统本机IP" class="headerlink" title="②查看Linux系统本机IP"></a>②查看Linux系统本机IP</h3><p>远程客户端访问Linux服务器时不能使用127.0.0.1，要使用网络上的实际IP。可以用ifconfig命令查看。</p><h3 id="③将Redis配置文件中的bind配置项设置为本机IP。"><a href="#③将Redis配置文件中的bind配置项设置为本机IP。" class="headerlink" title="③将Redis配置文件中的bind配置项设置为本机IP。"></a>③将Redis配置文件中的bind配置项设置为本机IP。</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bind [你的实际IP]</span><br><span class="line">bind 192.168.200.100</span><br></pre></td></tr></table></figure><h2 id="Jedis"><a href="#Jedis" class="headerlink" title="Jedis"></a>Jedis</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//指定Redis服务器的IP地址和端口号</span></span><br><span class="line">Jedis jedis = <span class="keyword">new</span> Jedis(<span class="string">"192.168.200.100"</span>, <span class="number">6379</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//执行ping命令</span></span><br><span class="line">String ping = jedis.ping();</span><br><span class="line"></span><br><span class="line">System.out.println(ping);</span><br><span class="line"></span><br><span class="line"><span class="comment">//关闭连接</span></span><br><span class="line">jedis.close();</span><br></pre></td></tr></table></figure><h2 id="JedisPool"><a href="#JedisPool" class="headerlink" title="JedisPool"></a>JedisPool</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//声明Linux服务器IP地址</span></span><br><span class="line">String host = <span class="string">"192.168.200.100"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//声明Redis端口号</span></span><br><span class="line"><span class="keyword">int</span> port = Protocol.DEFAULT_PORT;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建连接池对象</span></span><br><span class="line">JedisPool jedisPool = <span class="keyword">new</span> JedisPool(host, port);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取Jedis对象连接Redis</span></span><br><span class="line">Jedis jedis = jedisPool.getResource();</span><br><span class="line"></span><br><span class="line"><span class="comment">//执行具体操作</span></span><br><span class="line">String ping = jedis.ping();</span><br><span class="line"></span><br><span class="line">System.out.println(ping);</span><br><span class="line"></span><br><span class="line"><span class="comment">//关闭连接</span></span><br><span class="line">jedisPool.close();</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="大数据" scheme="http://www.red0819.top/categories/bigdata/"/>
    
    
    <category term="Redis" scheme="http://www.red0819.top/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis持久化机制</title>
    <link href="http://www.red0819.top/2020/02/15/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"/>
    <id>http://www.red0819.top/2020/02/15/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/</id>
    <published>2020-02-15T05:22:38.000Z</published>
    <updated>2020-09-14T21:07:02.487Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Redis持久化机制"><a href="#Redis持久化机制" class="headerlink" title="Redis持久化机制"></a>Redis持久化机制</h1><p><a href="https://redis.io/topics/persistence#snapshotting" target="_blank" rel="noopener">官网描述</a></p><p>Redis工作时数据都存储在内存中，万一服务器断电，则所有数据都会丢失。针对这种情况，Redis采用持久化机制来增强数据安全性。</p><h2 id="1-RDB"><a href="#1-RDB" class="headerlink" title="1.RDB"></a>1.RDB</h2><h3 id="①机制描述"><a href="#①机制描述" class="headerlink" title="①机制描述"></a>①机制描述</h3><p>每隔一定的时间把内存中的数据作为一个快照保存到硬盘上的文件中。Redis默认开启RDB机制。</p><h3 id="②触发时机"><a href="#②触发时机" class="headerlink" title="②触发时机"></a>②触发时机</h3><h4 id="1-基于默认配置"><a href="#1-基于默认配置" class="headerlink" title="[1]基于默认配置"></a>[1]基于默认配置</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">save</span> <span class="string">900 1</span></span><br><span class="line"><span class="attr">save</span> <span class="string">300 10</span></span><br><span class="line"><span class="attr">save</span> <span class="string">60 10000</span></span><br></pre></td></tr></table></figure><p>含义</p><table><thead><tr><th>配置</th><th>含义</th></tr></thead><tbody><tr><td>save 900 1</td><td>900秒内至少有一次修改则触发保存操作</td></tr><tr><td>save 300 10</td><td>300秒内至少有10次修改则触发保存操作</td></tr><tr><td>save 60 10000</td><td>60秒内至少有1万次修改则触发保存操作</td></tr></tbody></table><h4 id="2-使用保存命令"><a href="#2-使用保存命令" class="headerlink" title="[2]使用保存命令"></a>[2]使用保存命令</h4><p>save或bgsave</p><h4 id="3-使用flushall命令"><a href="#3-使用flushall命令" class="headerlink" title="[3]使用flushall命令"></a>[3]使用flushall命令</h4><p>这个命令也会产生dump.rdb文件，但里面是空的，没有意义</p><h4 id="4-服务器关闭"><a href="#4-服务器关闭" class="headerlink" title="[4]服务器关闭"></a>[4]服务器关闭</h4><p>如果执行SHUTDOWN命令让Redis正常退出，那么此前Redis就会执行一次持久化保存。</p><h3 id="③相关配置"><a href="#③相关配置" class="headerlink" title="③相关配置"></a>③相关配置</h3><table><thead><tr><th>配置项</th><th>取值</th><th>作用</th></tr></thead><tbody><tr><td>save</td><td>“”</td><td>禁用RDB机制</td></tr><tr><td>dbfilename</td><td>文件名，例如：dump.rdb</td><td>设置RDB机制下，数据存储文件的文件名</td></tr><tr><td>dir</td><td>Redis工作目录路径</td><td>指定存放持久化文件的目录的路径。注意：这里指定的必须是目录不能是文件名</td></tr></tbody></table><h3 id="④思考"><a href="#④思考" class="headerlink" title="④思考"></a>④思考</h3><p>RDB机制能够保证数据的绝对安全吗？</p><h2 id="2-AOF"><a href="#2-AOF" class="headerlink" title="2.AOF"></a>2.AOF</h2><h3 id="①机制描述-1"><a href="#①机制描述-1" class="headerlink" title="①机制描述"></a>①机制描述</h3><p>根据配置文件中指定的策略，把生成数据的命令保存到硬盘上的文件中。一个AOF文件的内容可以参照下面的例子：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">*2</span></span><br><span class="line"><span class="attr">$6</span></span><br><span class="line"><span class="attr">SELECT</span></span><br><span class="line"><span class="attr">$1</span></span><br><span class="line"><span class="attr">0</span></span><br><span class="line"><span class="attr">*3</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">set</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">num</span></span><br><span class="line"><span class="attr">$2</span></span><br><span class="line"><span class="attr">10</span></span><br><span class="line"><span class="attr">*2</span></span><br><span class="line"><span class="attr">$4</span></span><br><span class="line"><span class="attr">incr</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">num</span></span><br><span class="line"><span class="attr">*2</span></span><br><span class="line"><span class="attr">$4</span></span><br><span class="line"><span class="attr">incr</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">num</span></span><br><span class="line"><span class="attr">*2</span></span><br><span class="line"><span class="attr">$4</span></span><br><span class="line"><span class="attr">incr</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">num</span></span><br></pre></td></tr></table></figure><p>生成上面文件内容的Redis命令是：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set num 10</span><br><span class="line">incr num</span><br><span class="line">incr num</span><br><span class="line">incr num</span><br></pre></td></tr></table></figure><h3 id="②AOF基本配置"><a href="#②AOF基本配置" class="headerlink" title="②AOF基本配置"></a>②AOF基本配置</h3><table><thead><tr><th>配置项</th><th>取值</th><th>作用</th></tr></thead><tbody><tr><td>appendonly</td><td>yes</td><td>启用AOF持久化机制</td></tr><tr><td></td><td>no</td><td>禁用AOF持久化机制[默认值]</td></tr><tr><td>appendfilename</td><td>“文件名”</td><td>AOF持久化文件名</td></tr><tr><td>dir</td><td>Redis工作目录路径</td><td>指定存放持久化文件的目录的路径。注意：这里指定的必须是目录不能是文件名</td></tr><tr><td>appendfsync</td><td>always</td><td>每一次数据修改后都将执行文件写入操作，缓慢但是最安全。</td></tr><tr><td></td><td>everysec</td><td>每秒执行一次写入操作。折中。</td></tr><tr><td></td><td>no</td><td>由操作系统在适当的时候执行写入操作，最快。</td></tr></tbody></table><h3 id="③AOF重写"><a href="#③AOF重写" class="headerlink" title="③AOF重写"></a>③AOF重写</h3><p>对比下面两组命令：</p><table><thead><tr><th>AOF重写前</th><th>AOF重写后</th></tr></thead><tbody><tr><td>set count 1<br>incr count<br>incr count<br>incr count</td><td>set count 4</td></tr></tbody></table><p>两组命令执行后对于count来说最终的值是一致的，但是进行AOF重写后省略了中间过程，可以让AOF文件体积更小。而Redis会根据AOF文件的体积来决定是否进行AOF重写。参考的配置项如下：</p><table><thead><tr><th>配置项</th><th>含义</th></tr></thead><tbody><tr><td>auto-aof-rewrite-percentage 100</td><td>文件体积增大100%时执行AOF重写</td></tr><tr><td>auto-aof-rewrite-min-size 64mb</td><td>文件体积增长到64mb时执行AOF重写</td></tr></tbody></table><p>实际工作中不要进行频繁的AOF重写，因为CPU资源和硬盘资源二者之间肯定是CPU资源更加宝贵，所以不应该过多耗费CPU性能去节省硬盘空间。</p><h2 id="3-持久化文件损坏修复"><a href="#3-持久化文件损坏修复" class="headerlink" title="3.持久化文件损坏修复"></a>3.持久化文件损坏修复</h2><p>Redis服务器启动时如果读取了损坏的持久化文件会导致启动失败，此时为了让Redis服务器能够正常启动，需要对损坏的持久化文件进行修复。这里以AOF文件为例介绍修复操作的步骤。</p><ul><li><p>第一步：备份要修复的appendonly.aof文件</p></li><li><p>第二步：执行修复程序</p><p>/usr/local/redis/bin/redis-check-aof –fix /usr/local/redis/appendonly.aof</p></li><li><p>第三步：重启Redis</p></li></ul><p>注意：所谓修复持久化文件仅仅是把损坏的部分去掉，而没法把受损的数据找回。</p><h2 id="4-扩展阅读：两种持久化机制的取舍"><a href="#4-扩展阅读：两种持久化机制的取舍" class="headerlink" title="4.扩展阅读：两种持久化机制的取舍"></a>4.扩展阅读：两种持久化机制的取舍</h2><h3 id="①RDB"><a href="#①RDB" class="headerlink" title="①RDB"></a>①RDB</h3><h4 id="1-优势"><a href="#1-优势" class="headerlink" title="[1]优势"></a>[1]优势</h4><p>适合大规模的数据恢复，速度较快</p><h4 id="2-劣势"><a href="#2-劣势" class="headerlink" title="[2]劣势"></a>[2]劣势</h4><p>会丢失最后一次快照后的所有修改，不能绝对保证数据的高度一致性和完整性。Fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑，但上述成立有条件，Linux也有优化手段</p><h3 id="②AOF"><a href="#②AOF" class="headerlink" title="②AOF"></a>②AOF</h3><h4 id="1-优势-1"><a href="#1-优势-1" class="headerlink" title="[1]优势"></a>[1]优势</h4><p>选择appendfsync always方式运行时理论上能够做到数据完整一致，但此时性能又不好。文件内容具备一定可读性，能够用来分析Redis工作情况。</p><h4 id="2-劣势-1"><a href="#2-劣势-1" class="headerlink" title="[2]劣势"></a>[2]劣势</h4><p>持久化相同的数据，文件体积比RDB大，恢复速度比RDB慢。效率在同步写入时低于RDB，不同步写入时与RDB相同。</p><h3 id="③RDB和AOF并存"><a href="#③RDB和AOF并存" class="headerlink" title="③RDB和AOF并存"></a>③RDB和AOF并存</h3><p>Redis重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整</p><p>RDB的数据不实时，同时使用两者时服务器重启也只会找AOF文件。那要不要只使用AOF呢？作者建议不要，因为RDB更适合用于备份数据库(AOF在不断变化不好备份)、快速重启，而且不会有AOF可能潜在的bug，留着作为一个万一的手段。</p><h3 id="④使用建议"><a href="#④使用建议" class="headerlink" title="④使用建议"></a>④使用建议</h3><p>如果Redis仅仅作为缓存可以不使用任何持久化方式。</p><p>其他应用方式综合考虑性能和完整性、一致性要求。</p><p>RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够了，只保留save 900 1这条规则。如果Enalbe AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自己的AOF文件就可以了。代价一是带来了持续的IO，二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上。默认超过原大小100%大小时重写可以改到适当的数值。如果不开启AOF，仅靠Master-Slave Replication 实现高可用性能也不错。能省掉一大笔IO也减少了rewrite时带来的系统波动。代价是如果Master/Slave同时倒掉，会丢失十几分钟的数据，启动脚本也要比较两个Master/Slave中的RDB文件，载入较新的那个。新浪微博就选用了这种架构。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="大数据" scheme="http://www.red0819.top/categories/bigdata/"/>
    
    
    <category term="Redis" scheme="http://www.red0819.top/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis基础</title>
    <link href="http://www.red0819.top/2020/02/05/Redis%E5%9F%BA%E7%A1%80/"/>
    <id>http://www.red0819.top/2020/02/05/Redis%E5%9F%BA%E7%A1%80/</id>
    <published>2020-02-05T15:02:25.000Z</published>
    <updated>2020-09-14T21:07:22.197Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h1><p>NoSQL是Not only SQL的缩写，大意为“不只是SQL”，说明这项技术是<b><font color="red">传统关系型数据库的补充</font></b>而非替代。在整个NoSQL技术栈中<b><font color="blue">MemCache</font></b>、<b><font color="blue">Redis</font></b>、<b><font color="blue">MongoDB</font></b>被称为NoSQL三剑客。</p><p>对比如下：</p><table><thead><tr><th></th><th>关系型数据库</th><th>NoSQL数据库</th></tr></thead><tbody><tr><td>数据存储位置</td><td>硬盘</td><td>内存</td></tr><tr><td>数据结构</td><td>高度组织化结构化数据</td><td>没有预定义的模式</td></tr><tr><td>数据操作方式</td><td>SQL</td><td>所有数据都是键值对，没有声明性查询语言</td></tr><tr><td>事务控制</td><td>严格的基础事务ACID原则</td><td>基于乐观锁的松散事务控制</td></tr></tbody></table><p>NoSQL数据库的最大优势体现为：<strong>高性能、高可用性和可伸缩性</strong>。</p><h1 id="Redis简介"><a href="#Redis简介" class="headerlink" title="Redis简介"></a>Redis简介</h1><p>Redis英文官网介绍：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs and geospatial indexes with radius queries. Redis has built-in replication, Lua scripting, LRU eviction, transactions and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.</span><br></pre></td></tr></table></figure><p>Redis中文官网介绍：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。</span><br></pre></td></tr></table></figure><p>Redis命令参考文档网址：<a href="http://redisdoc.com" target="_blank" rel="noopener">http://redisdoc.com</a></p><p>Redis的典型应用场景：</p><blockquote><p>1、缓存</p><p>使用Redis可以建立性能非常出色的缓存服务器，查询请求先在Redis中查找所需要的数据，如果能够查询到（命中）则直接返回，大大减轻关系型数据库的压力。</p><p>2、数据临时存储位置</p><p>使用token（令牌）作为用户登录系统时的身份标识，这个token就可以在Redis中临时存储。</p><p>3、分布式环境下解决Session不一致问题时的Session库</p><p>Spring提供了一种技术解决分布式环境下Session不一致问题，叫SpringSession。而Redis就可以为SpringSession提供一个数据存储空间。</p><p>4、流式数据去重</p><p>在Redis中有一种数据类型是set，和Java中的Set集合很像，不允许存储重复数据。借助这个特性我们可以在Redis中使用set类型存储流式数据达到去重的目的。</p></blockquote><h1 id="Redis安装"><a href="#Redis安装" class="headerlink" title="Redis安装"></a>Redis安装</h1><h2 id="上传并解压"><a href="#上传并解压" class="headerlink" title="上传并解压"></a>上传并解压</h2><p>redis-4.0.2.tar.gz</p><h2 id="安装C语言编译环境"><a href="#安装C语言编译环境" class="headerlink" title="安装C语言编译环境"></a>安装C语言编译环境</h2><p>[建议先拍快照]<br></p><p>yum install -y gcc-c++</p><h2 id="修改安装位置"><a href="#修改安装位置" class="headerlink" title="修改安装位置"></a>修改安装位置</h2><p>vim redis解压目录/src/Makefile</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PREFIX?=/usr/local/redis</span><br></pre></td></tr></table></figure><p>就Redis自身而言是不需要修改的，这里修改的目的是让Redis的运行程序不要和其他文件混杂在一起。</p><h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><p>编译：进入Redis解压目录执行make命令<br></p><p>[建议先拍快照]<br></p><p>安装：make install</p><h2 id="启动Redis服务器"><a href="#启动Redis服务器" class="headerlink" title="启动Redis服务器"></a>启动Redis服务器</h2><h3 id="①默认启动"><a href="#①默认启动" class="headerlink" title="①默认启动"></a>①默认启动</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@rich ~]# /usr/local/redis/bin/redis-server</span><br><span class="line">7239:C 07 Oct 18:59:12.144 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">7239:C 07 Oct 18:59:12.144 # Redis version=4.0.2, bits=64, commit=00000000, modified=0, pid=7239, just started</span><br><span class="line">7239:C 07 Oct 18:59:12.144 # Warning: no config file specified, using the default config. In order to specify a config file use /usr/local/redis/bin/redis-server /path/to/redis.conf</span><br><span class="line">7239:M 07 Oct 18:59:12.145 * Increased maximum number of open files to 10032 (it was originally set to 1024).</span><br><span class="line">                _._                                                  </span><br><span class="line">           _.-``__ ''-._                                             </span><br><span class="line">      _.-``    `.  `_.  ''-._           Redis 4.0.2 (00000000/0) 64 bit</span><br><span class="line">  .-`` .-```.  ```\/    _.,_ ''-._                                   </span><br><span class="line"> (    '      ,       .-`  | `,    )     Running in standalone mode</span><br><span class="line"> |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379</span><br><span class="line"> |    `-._   `._    /     _.-'    |     PID: 7239</span><br><span class="line">  `-._    `-._  `-./  _.-'    _.-'                                   </span><br><span class="line"> |`-._`-._    `-.__.-'    _.-'_.-'|                                  </span><br><span class="line"> |    `-._`-._        _.-'_.-'    |           http://redis.io        </span><br><span class="line">  `-._    `-._`-.__.-'_.-'    _.-'                                   </span><br><span class="line"> |`-._`-._    `-.__.-'    _.-'_.-'|                                  </span><br><span class="line"> |    `-._`-._        _.-'_.-'    |                                  </span><br><span class="line">  `-._    `-._`-.__.-'_.-'    _.-'                                   </span><br><span class="line">      `-._    `-.__.-'    _.-'                                       </span><br><span class="line">          `-._        _.-'                                           </span><br><span class="line">              `-.__.-'                                               </span><br><span class="line"></span><br><span class="line">7239:M 07 Oct 18:59:12.148 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.</span><br><span class="line">7239:M 07 Oct 18:59:12.148 # Server initialized</span><br><span class="line">7239:M 07 Oct 18:59:12.148 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.</span><br><span class="line">7239:M 07 Oct 18:59:12.148 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.</span><br><span class="line">7239:M 07 Oct 18:59:12.148 * Ready to accept connections</span><br></pre></td></tr></table></figure><p>停止Redis服务器</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/redis/bin/redis-cli shutdown</span><br></pre></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">7239:M 07 Oct 19:00:53.208 # User requested shutdown...</span><br><span class="line">7239:M 07 Oct 19:00:53.208 * Saving the final RDB snapshot before exiting.</span><br><span class="line">7239:M 07 Oct 19:00:53.214 * DB saved on disk</span><br><span class="line">7239:M 07 Oct 19:00:53.214 # Redis is now ready to exit, bye bye...</span><br></pre></td></tr></table></figure><h3 id="②定制配置项启动"><a href="#②定制配置项启动" class="headerlink" title="②定制配置项启动"></a>②定制配置项启动</h3><h4 id="1-准备配置文件"><a href="#1-准备配置文件" class="headerlink" title="[1]准备配置文件"></a>[1]准备配置文件</h4><p>cp /opt/redis-4.0.2/redis.conf /usr/local/redis/</p><h4 id="2-修改配置项"><a href="#2-修改配置项" class="headerlink" title="[2]修改配置项"></a>[2]修改配置项</h4><table><thead><tr><th>配置项名称</th><th>作用</th><th>取值</th></tr></thead><tbody><tr><td>daemonize</td><td>控制是否以守护进程形式运行Redis服务器</td><td>yes</td></tr><tr><td>logfile</td><td>指定日志文件位置</td><td>“/var/logs/redis.log”</td></tr><tr><td>dir</td><td>Redis工作目录</td><td>/usr/local/redis</td></tr></tbody></table><p>注意：/var/logs目录需要我们提前创建好</p><h4 id="3-让Redis根据指定的配置文件启动"><a href="#3-让Redis根据指定的配置文件启动" class="headerlink" title="[3]让Redis根据指定的配置文件启动"></a>[3]让Redis根据指定的配置文件启动</h4><p>格式</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server文件路径 redis.conf文件路径</span><br></pre></td></tr></table></figure><p>举例</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/redis/bin/redis-server /usr/local/redis/redis.conf</span><br></pre></td></tr></table></figure><h2 id="客户端登录"><a href="#客户端登录" class="headerlink" title="客户端登录"></a>客户端登录</h2><p>/usr/local/redis/bin/redis-cli</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; ping</span><br><span class="line">PONG</span><br><span class="line">127.0.0.1:6379&gt; exit</span><br></pre></td></tr></table></figure><h1 id="Redis五种常用数据结构"><a href="#Redis五种常用数据结构" class="headerlink" title="Redis五种常用数据结构"></a>Redis五种常用数据结构</h1><h2 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h2><table>    <tr>        <td rowspan="6">KEY</td>        <td>VALUE</td>    </tr>    <tr>        <td>string</td>    </tr>    <tr>        <td>list</td>    </tr>    <tr>        <td>set</td>    </tr>    <tr>        <td>hash</td>    </tr>    <tr>        <td>zset</td>    </tr></table><p>Redis中的数据，总体上是键值对，不同数据类型指的是键值对中值的类型。</p><h2 id="string类型"><a href="#string类型" class="headerlink" title="string类型"></a>string类型</h2><p>Redis中最基本的类型，它是key对应的一个单一值。二进制安全，不必担心由于编码等问题导致二进制数据变化。所以redis的string可以包含任何数据，比如jpg图片或者序列化的对象。Redis中一个字符串值的最大容量是512M。</p><h2 id="list类型"><a href="#list类型" class="headerlink" title="list类型"></a>list类型</h2><p>Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。说明它的底层是基于链表实现的，所以它操作时头尾效率高，中间效率低。</p><h2 id="set类型"><a href="#set类型" class="headerlink" title="set类型"></a>set类型</h2><p>Redis的set是string类型的无序集合。它是基于哈希表实现的。</p><h2 id="hash类型"><a href="#hash类型" class="headerlink" title="hash类型"></a>hash类型</h2><p>本身就是一个键值对集合。可以当做Java中的Map&lt;String,String&gt;对待。</p><h2 id="zset类型"><a href="#zset类型" class="headerlink" title="zset类型"></a>zset类型</h2><p>Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。</p><h1 id="Redis命令行操作"><a href="#Redis命令行操作" class="headerlink" title="Redis命令行操作"></a>Redis命令行操作</h1><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="①切换数据库"><a href="#①切换数据库" class="headerlink" title="①切换数据库"></a>①切换数据库</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Redis默认有16个数据库。</span><br><span class="line">115 # Set the number of databases. The default database is DB 0, you can select</span><br><span class="line">116 # a different one on a per-connection basis using SELECT <span class="tag">&lt;<span class="name">dbid</span>&gt;</span> where</span><br><span class="line">117 # dbid is a number between 0 and 'databases'-1</span><br><span class="line">118 databases 16</span><br><span class="line">使用select进行切换，数据库索引从0开始</span><br><span class="line">127.0.0.1:6379&gt; select 2</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379[2]&gt; select 0</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure><h3 id="②查看数据库长度"><a href="#②查看数据库长度" class="headerlink" title="②查看数据库长度"></a>②查看数据库长度</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; dbsize</span><br><span class="line">(integer) 3</span><br></pre></td></tr></table></figure><h2 id="KEY操作"><a href="#KEY操作" class="headerlink" title="KEY操作"></a>KEY操作</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">●KEYS PATTERN</span><br><span class="line">●TYPE KEY</span><br><span class="line">返回KEY对应的值的类型</span><br><span class="line">●MOVE KEY DB</span><br><span class="line">把一组键值对数据移动到另一个数据库中</span><br><span class="line">●DEL KEY [KEY ...]</span><br><span class="line">根据KEY进行删除，至少要指定一个KEY</span><br><span class="line">●EXISTS KEY</span><br><span class="line">检查指定的KEY是否存在。指定一个KEY时，存在返回1，不存在返回0。可以指定多个，返回存在的KEY的数量。</span><br><span class="line">●RANDOMKEY</span><br><span class="line">在现有的KEY中随机返回一个</span><br><span class="line">●RENAME KEY NEWKEY</span><br><span class="line">重命名一个KEY，NEWKEY不管是否是已经存在的都会执行，如果NEWKEY已经存在则会被覆盖。</span><br><span class="line">●RENAMENX KEY NEWKEY</span><br><span class="line">只有在NEWKEY不存在时能够执行成功，否则失败</span><br><span class="line">●TTL KEY</span><br><span class="line">以秒为单位查看KEY还能存在多长时间</span><br><span class="line">●EXPIRE KEY SECONDS</span><br><span class="line">给一个KEY设置在SECONDS秒后过期，过期会被Redis移除。</span><br><span class="line">●PERSIST KEY</span><br><span class="line">移除过期时间，变成永久key</span><br></pre></td></tr></table></figure><h2 id="string操作"><a href="#string操作" class="headerlink" title="string操作"></a>string操作</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">●SET KEY VALUE [EX SECONDS] [PX MILLISECONDS] [NX|XX]</span><br><span class="line">给KEY设置一个string类型的值。</span><br><span class="line">EX参数用于设置存活的秒数。</span><br><span class="line">PX参数用于设置存活的毫秒数。</span><br><span class="line">NX参数表示当前命令中指定的KEY不存在才行。</span><br><span class="line">XX参数表示当前命令中指定的KEY存在才行。</span><br><span class="line">●GET KEY</span><br><span class="line">根据key得到值，只能用于string类型。</span><br><span class="line">●APPEND KEY VALUE</span><br><span class="line">把指定的value追加到KEY对应的原来的值后面，返回值是追加后字符串长度</span><br><span class="line">●STRLEN KEY</span><br><span class="line">直接返回字符串长度</span><br><span class="line">●INCR KEY</span><br><span class="line">自增1</span><br><span class="line">●DECR KEY</span><br><span class="line">自减1</span><br><span class="line">●INCRBY KEY INCREMENT</span><br><span class="line">原值+INCREMENT</span><br><span class="line">●DECRBY KEY DECREMENT</span><br><span class="line">原值-DECREMENT</span><br><span class="line">●GETRANGE KEY START END</span><br><span class="line">从字符串中取指定的一段</span><br><span class="line">●SETRANGE KEY OFFSET VALUE</span><br><span class="line">从offset开始使用VALUE进行替换</span><br><span class="line">●SETEX KEY SECONDS VALUE</span><br><span class="line">设置KEY,VALUE时指定存在秒数</span><br><span class="line">●SETNX KEY VALUE</span><br><span class="line">新建字符串类型的键值对</span><br><span class="line">●MSET KEY VALUE [KEY VALUE ...]</span><br><span class="line">一次性设置一组多个键值对</span><br><span class="line">●MGET KEY [KEY ...]</span><br><span class="line">一次性指定多个KEY，返回它们对应的值，没有值的KEY返回值是(nil)</span><br><span class="line">●MSETNX KEY VALUE [KEY VALUE ...]</span><br><span class="line">一次性新建多个值</span><br><span class="line">●GETSET KEY VALUE</span><br><span class="line">设置新值，同时能够将旧值返回</span><br></pre></td></tr></table></figure><h2 id="list操作"><a href="#list操作" class="headerlink" title="list操作"></a>list操作</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">●LPUSH key value [value ...]</span><br><span class="line">●RPUSH key value [value ...]</span><br><span class="line">●LRANGE key start stop</span><br><span class="line">根据list集合的索引打印元素数据</span><br><span class="line">正着数：0,1,2,3,...</span><br><span class="line">倒着数：-1,-2,-3,...</span><br><span class="line">●LLEN key</span><br><span class="line">●LPOP key</span><br><span class="line">从左边弹出一个元素。</span><br><span class="line">弹出=返回+删除。</span><br><span class="line">●RPOP key</span><br><span class="line">从右边弹出一个元素。</span><br><span class="line">●RPOPLPUSH source destination</span><br><span class="line">从source中RPOP一个元素，LPUSH到destination中</span><br><span class="line">●LINDEX key index</span><br><span class="line">根据索引从集合中取值</span><br><span class="line">●LINSERT key BEFORE|AFTER pivot value</span><br><span class="line">在pivot指定的值前面或后面插入value</span><br><span class="line">●LPUSHX key value</span><br><span class="line">只能针对存在的list执行LPUSH</span><br><span class="line">●LREM key count value</span><br><span class="line">根据count指定的数量从key对应的list中删除value</span><br><span class="line">●LSET key index value</span><br><span class="line">把指定索引位置的元素替换为另一个值</span><br><span class="line">●LTRIM key start stop</span><br><span class="line">仅保留指定区间的数据，两边的数据被删除</span><br></pre></td></tr></table></figure><h2 id="set操作"><a href="#set操作" class="headerlink" title="set操作"></a>set操作</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">●SADD key member [member ...]</span><br><span class="line">●SMEMBERS key</span><br><span class="line">●SCARD key</span><br><span class="line">返回集合中元素的数量</span><br><span class="line">●SISMEMBER key member</span><br><span class="line">检查当前指定member是否是集合中的元素</span><br><span class="line">●SREM key member [member ...]</span><br><span class="line">从集合中删除元素</span><br><span class="line">●SINTER key [key ...]</span><br><span class="line">将指定的集合进行“交集”操作</span><br><span class="line">集合A：a,b,c</span><br><span class="line">集合B：b,c,d</span><br><span class="line">交集：b,c</span><br><span class="line">●SINTERSTORE destination key [key ...]</span><br><span class="line">取交集后存入destination</span><br><span class="line">●SDIFF key [key ...]</span><br><span class="line">将指定的集合执行“差集”操作</span><br><span class="line">集合A：a,b,c</span><br><span class="line">集合B：b,c,d</span><br><span class="line">A对B执行diff：a</span><br><span class="line">相当于：A-交集部分</span><br><span class="line">●SDIFFSTORE destination key [key ...]</span><br><span class="line">●SUNION key [key ...]</span><br><span class="line">将指定的集合执行“并集”操作</span><br><span class="line">集合A：a,b,c</span><br><span class="line">集合B：b,c,d</span><br><span class="line">并集：a,b,c,d</span><br><span class="line">●SUNIONSTORE destination key [key ...]</span><br><span class="line">●SMOVE source destination member</span><br><span class="line">把member从source移动到destination</span><br><span class="line">●SPOP key [count]</span><br><span class="line">从集合中随机弹出count个数量的元素，count不指定就弹出1个</span><br><span class="line">●SRANDMEMBER key [count]</span><br><span class="line">从集合中随机返回count个数量的元素，count不指定就返回1个</span><br><span class="line">●SSCAN key cursor [MATCH pattern] [COUNT count]</span><br><span class="line">基于游标的遍历</span><br></pre></td></tr></table></figure><h2 id="hash操作"><a href="#hash操作" class="headerlink" title="hash操作"></a>hash操作</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">●HSET key field value</span><br><span class="line">●HGETALL key</span><br><span class="line">●HGET key field</span><br><span class="line">●HLEN key</span><br><span class="line">●HKEYS key</span><br><span class="line">●HVALS key</span><br><span class="line">●HEXISTS key field</span><br><span class="line">●HDEL key field [field ...]</span><br><span class="line">●HINCRBY key field increment</span><br><span class="line">●HMGET key field [field ...]</span><br><span class="line">●HMSET key field value [field value ...]</span><br><span class="line">●HSETNX key field value</span><br></pre></td></tr></table></figure><h2 id="zset操作"><a href="#zset操作" class="headerlink" title="zset操作"></a>zset操作</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">●ZADD key [NX|XX] [CH] [INCR] score member [score member ...]</span><br><span class="line">●ZRANGE key start stop [WITHSCORES]</span><br><span class="line">●ZCARD key</span><br><span class="line">●ZSCORE key member</span><br><span class="line">●ZINCRBY key increment member</span><br><span class="line">●ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]</span><br><span class="line">在分数的指定区间内返回数据</span><br><span class="line">●ZRANK key member</span><br><span class="line">先对分数进行升序排序，返回member的排名</span><br><span class="line">●ZREM key member [member ...]</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="大数据" scheme="http://www.red0819.top/categories/bigdata/"/>
    
    
    <category term="Redis" scheme="http://www.red0819.top/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Hive入门</title>
    <link href="http://www.red0819.top/2018/12/22/Hive%E5%85%A5%E9%97%A8/"/>
    <id>http://www.red0819.top/2018/12/22/Hive%E5%85%A5%E9%97%A8/</id>
    <published>2018-12-22T12:59:01.000Z</published>
    <updated>2020-09-14T21:05:09.965Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="HIVE入门"><a href="#HIVE入门" class="headerlink" title="HIVE入门"></a>HIVE入门</h1><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计。</p><p>Hive是基于Hadoop的一个<strong>数据仓库工具</strong>，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p><p><strong>本质是：将HQL转化成MapReduce程序</strong></p><p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps1.png" alt="wps1"></p><p>1）Hive处理的数据存储在HDFS</p><p>2）Hive分析数据底层的实现是MapReduce</p><p>3）执行程序运行在Yarn上</p><h2 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>1) 操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</p><p>2) 避免了去写MapReduce，减少开发人员的学习成本。</p><p>3) Hive的<strong>执行延迟比较高</strong>，因此Hive常用于数据分析，对实时性要求不高的场合。</p><p>4) Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</p><p>5) Hive<strong>支持用户自定义函数</strong>，用户可以根据自己的需求来实现自己的函数。</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1．Hive的HQL表达能力有限</p><p>（1）迭代式算法无法表达</p><p>（2）数据挖掘方面不擅长</p><p>2．Hive的效率比较低</p><p>（1）Hive自动生成的MapReduce作业，通常情况下不够智能化</p><p>（2）Hive调优比较困难，粒度较粗</p><h2 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h2><p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps2.png" alt="wps2"></p><p>1．用户接口：Client</p><p>CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</p><p>2．元数据：Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p><p><strong>默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</strong></p><p>3．Hadoop</p><p>使用HDFS进行存储，使用MapReduce进行计算。</p><p>4．驱动器：Driver</p><p>（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p><p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps3.png" alt="wps3"></p><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p><h2 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h2><p>由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><h3 id="数据存储位置"><a href="#数据存储位置" class="headerlink" title="数据存储位置"></a>数据存储位置</h3><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p><h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><p>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，<strong>Hive中不建议对数据的改写</strong>，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。<u>Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。</u>由于 MapReduce 的引入，<u>Hive 可以并行访问数据</u>，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。</p><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。</p><p>数据库通常有自己的执行引擎。</p><h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><p>1）Hive执行延迟高</p><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。</p><p>另一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。</p><p>2）数据库的执行延迟较低（相对而言）</p><p>数据规模较小时提现。</p><p>当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle在理论上的扩展能力也只有100台左右。</p><h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p><h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><h2 id="Hive安装地址"><a href="#Hive安装地址" class="headerlink" title="Hive安装地址"></a>Hive安装地址</h2><p>1．Hive官网地址</p><p><a href="http://hive.apache.org/" target="_blank" rel="noopener">http://hive.apache.org/</a></p><p>2．文档查看地址</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p><p>3．下载地址</p><p><a href="http://archive.apache.org/dist/hive/" target="_blank" rel="noopener">http://archive.apache.org/dist/hive/</a></p><p>4．github地址</p><p><a href="https://github.com/apache/hive" target="_blank" rel="noopener">https://github.com/apache/hive</a></p><h2 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h2><p>1．Hive安装及配置</p><p>（1）把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p><p>（2）解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><p>（3）修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 module]$ mv apache-hive-1.2.1-bin&#x2F; hive</span><br></pre></td></tr></table></figure><p>（4）修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><p>​    （5）配置hive-env.sh文件</p><p>​    （a）配置HADOOP_HOME路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure><p>​    （b）配置HIVE_CONF_DIR路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure><p>2．Hadoop集群配置</p><p>（1）必须启动hdfs和yarn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hadoop-2.7.2]$ sbin&#x2F;start-dfs.sh</span><br><span class="line"></span><br><span class="line">[red@hadoop103 hadoop-2.7.2]$ sbin&#x2F;start-yarn.sh</span><br></pre></td></tr></table></figure><p>（2）在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hadoop-2.7.2]$ bin&#x2F;hadoop fs -mkdir &#x2F;tmp</span><br><span class="line"></span><br><span class="line">[red@hadoop102 hadoop-2.7.2]$ bin&#x2F;hadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse</span><br><span class="line"></span><br><span class="line">[red@hadoop102 hadoop-2.7.2]$ bin&#x2F;hadoop fs -chmod g+w &#x2F;tmp</span><br><span class="line"></span><br><span class="line">[red@hadoop102 hadoop-2.7.2]$ bin&#x2F;hadoop fs -chmod g+w &#x2F;user&#x2F;hive&#x2F;warehouse</span><br></pre></td></tr></table></figure><p>3．Hive基本操作</p><p>（1）启动hive</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive</span><br></pre></td></tr></table></figure><p>（2）查看数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>（3）打开默认数据库</p><p>hive&gt; use default;</p><p>（4）显示default数据库中的表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（5）创建一张表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure><p>（6）显示数据库中有几张表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（7）查看表的结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure><p>（8）向表中插入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into student values(1000,&quot;ss&quot;);</span><br></pre></td></tr></table></figure><p>（9）查询表中数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure><p>（10）退出hive</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure><h2 id="将本地文件导入Hive案例"><a href="#将本地文件导入Hive案例" class="headerlink" title="将本地文件导入Hive案例"></a>将本地文件导入Hive案例</h2><p>需求</p><p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。</p><p>1．数据准备</p><p>在/opt/module/datas这个目录下准备数据</p><p>（1）在/opt/module/目录下创建datas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 module]$ mkdir datas</span><br></pre></td></tr></table></figure><p>（2）在/opt/module/datas/目录下创建student.txt文件并添加数据（tab键间隔）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 datas]$ touch student.txt</span><br><span class="line"></span><br><span class="line">[red@hadoop102 datas]$ vi student.txt</span><br></pre></td></tr></table></figure><blockquote><p>1001    zhangshan</p><p>1002    lishi</p><p>1003    zhaoliu</p></blockquote><p>2．Hive实际操作</p><p>（1）启动hive</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive</span><br></pre></td></tr></table></figure><p>（2）显示数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>（3）使用default数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure><p>（4）显示default数据库中的表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure><p>（5）删除已创建的student表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop table student;</span><br></pre></td></tr></table></figure><p>（6）创建student表, 并声明文件分隔符’\t’</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlhive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;;</span><br></pre></td></tr></table></figure><p>（7）加载/opt/module/datas/student.txt 文件到student数据库表中。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath '/opt/module/datas/student.txt' into table student;</span><br></pre></td></tr></table></figure><p>（8）Hive查询结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">1001zhangshan</span><br><span class="line"></span><br><span class="line">1002lishi</span><br><span class="line"></span><br><span class="line">1003zhaoliu</span><br><span class="line"></span><br><span class="line">Time taken: 0.266 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure><p>3．遇到的问题</p><p>再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> java.lang.RuntimeException: java.lang.RuntimeException:</span><br><span class="line"></span><br><span class="line"> Unable to instantiate</span><br><span class="line"></span><br><span class="line"> org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:<span class="number">522</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:<span class="number">677</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:<span class="number">621</span>)</span><br><span class="line"></span><br><span class="line">   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line"></span><br><span class="line">   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">57</span>)</span><br><span class="line"></span><br><span class="line">   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line"></span><br><span class="line">   at java.lang.reflect.Method.invoke(Method.java:<span class="number">606</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.util.RunJar.run(RunJar.java:<span class="number">221</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.util.RunJar.main(RunJar.java:<span class="number">136</span>)</span><br><span class="line"></span><br><span class="line">Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:<span class="number">1523</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:<span class="number">86</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:<span class="number">132</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:<span class="number">104</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:<span class="number">3005</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:<span class="number">3024</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:<span class="number">503</span>)</span><br><span class="line"></span><br><span class="line">... <span class="number">8</span> more</span><br></pre></td></tr></table></figure><p>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;</p><h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><p>（1）检查当前系统是否安装过Mysql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]$ rpm -qa|grep mysql</span><br><span class="line"></span><br><span class="line"> mysql-libs-5.1.73-7.el6.x86_64 &#x2F;&#x2F;如果存在通过如下命令卸载</span><br><span class="line"></span><br><span class="line">[root@hadoop102 ~]$ rpm -e --nodeps  mysql-libs &#x2F;&#x2F;用此命令卸载Mysql</span><br></pre></td></tr></table></figure><p>（2）将MySQL安装包拷贝到/opt/software目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> [root@hadoop102 software]# ll</span><br><span class="line"></span><br><span class="line">总用量 528384</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root 541061120 11月 29 17:56 mysql-5.7.28-1.el6.x86_64.rpm-bundle.tar</span><br></pre></td></tr></table></figure><p>（3）解压MySQL安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -xf mysql-5.7.28-1.el6.x86_64.rpm-bundle.tar</span><br></pre></td></tr></table></figure><p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps4.jpg" alt="wps4"></p><p>（4）在安装目录下执行rpm安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]$ rpm -ivh mysql-community-common-5.7.28-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@hadoop102 software]$ rpm -ivh mysql-community-libs-5.7.28-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@hadoop102 software]$ rpm -ivh mysql-community-client-5.7.28-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@hadoop102 software]$ rpm -ivh mysql-community-server-5.7.28-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><p>​     注意:按照顺序依次执行</p><p>（5）修改/etc/my.cnf文件,在[mysqld]节点下添加如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line"> explicit_defaults_for_timestamp&#x3D;true  &#x2F;&#x2F;显示指定默认值为timestamp类型的字段</span><br></pre></td></tr></table></figure><p>（6）删除/etc/my.cnf文件中datadir指向的目录下的所有内容:</p><p> 查看datadir的值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql</span><br></pre></td></tr></table></figure><p> 删除/var/lib/mysql目录下的所有内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql]# pwd</span><br><span class="line"></span><br><span class="line">&#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line"></span><br><span class="line">[root@hadoop102 mysql]# rm -rf *   &#x2F;&#x2F;注意执行命令的位置</span><br></pre></td></tr></table></figure><p>（7）初始化数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]$ mysqld --initialize --user&#x3D;mysql</span><br></pre></td></tr></table></figure><p>（8）查看临时生成的root用户的密码 </p><p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps5.jpg" alt="wps5"></p><p>（9）启动MySQL服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]$ service mysqld start</span><br></pre></td></tr></table></figure><p>（10）登录MySQL数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]$ mysql -uroot -p</span><br><span class="line"></span><br><span class="line">Enter password:  输入临时生成的密码</span><br></pre></td></tr></table></figure><p> <img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps6.jpg" alt="wps6"></p><p> 登录成功.</p><p>（11）修改root用户的密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; set password &#x3D; password(&quot;新密码&quot;)</span><br></pre></td></tr></table></figure><p>（12）修改root用户支持任意IP连接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use mysql ;</span><br><span class="line"></span><br><span class="line">mysql&gt; update user set host&#x3D; ‘%’ where  user &#x3D; ‘root’;</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges ;</span><br></pre></td></tr></table></figure><h2 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h2><h3 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h3><p>1．拷贝mysql-connector-java-5.1.37-bin.jar到/opt/module/hive/lib/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# cp mysql-connector-java-5.1.37-bin.jar &#x2F;opt&#x2F;module&#x2F;hive&#x2F;lib&#x2F;</span><br></pre></td></tr></table></figure><h3 id="配置Metastore到MySql"><a href="#配置Metastore到MySql" class="headerlink" title="配置Metastore到MySql"></a>配置Metastore到MySql</h3><p>1．在/opt/module/hive/conf目录下创建一个hive-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 conf]$ touch hive-site.xml</span><br><span class="line"></span><br><span class="line">[red@hadoop102 conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure><p>2．根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​     jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true</span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​ <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3．配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群）</p><h3 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h3><p>1．先启动MySQL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 mysql-libs]$ mysql -uroot -p000000</span><br></pre></td></tr></table></figure><p>查看有几个数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| Database      |</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| information_schema |</span><br><span class="line"></span><br><span class="line">| mysql       |</span><br><span class="line"></span><br><span class="line">| performance_schema |</span><br><span class="line"></span><br><span class="line">| test        |</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure><p>2．再次打开多个窗口，分别启动hive</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive</span><br></pre></td></tr></table></figure><p>3．启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| Database      |</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| information_schema |</span><br><span class="line"></span><br><span class="line">| metastore      |</span><br><span class="line"></span><br><span class="line">| mysql       |</span><br><span class="line"></span><br><span class="line">| performance_schema |</span><br><span class="line"></span><br><span class="line">| test        |</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure><h2 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h2><h3 id="启动hiveserver2服务"><a href="#启动hiveserver2服务" class="headerlink" title="启动hiveserver2服务"></a>启动hiveserver2服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hiveserver2</span><br></pre></td></tr></table></figure><h3 id="启动beeline"><a href="#启动beeline" class="headerlink" title="启动beeline"></a>启动beeline</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;beeline</span><br><span class="line"></span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"></span><br><span class="line">beeline&gt;</span><br></pre></td></tr></table></figure><h3 id="连接hiveserver2"><a href="#连接hiveserver2" class="headerlink" title="连接hiveserver2"></a>连接hiveserver2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;hadoop102:10000（回车）</span><br><span class="line"></span><br><span class="line">Connecting to jdbc:hive2:&#x2F;&#x2F;hadoop102:10000</span><br><span class="line"></span><br><span class="line">Enter username for jdbc:hive2:&#x2F;&#x2F;hadoop102:10000: red（回车）</span><br><span class="line"></span><br><span class="line">Enter password for jdbc:hive2:&#x2F;&#x2F;hadoop102:10000: （直接回车）</span><br><span class="line"></span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line"></span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line"></span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;hadoop102:10000&gt; show databases;</span><br><span class="line"></span><br><span class="line">+----------------+--+</span><br><span class="line"></span><br><span class="line">| database_name  |</span><br><span class="line"></span><br><span class="line">+----------------+--+</span><br><span class="line"></span><br><span class="line">| default     |</span><br><span class="line"></span><br><span class="line">| hive_db2    |</span><br><span class="line"></span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure><h2 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive -helpusage: hive -d,--define &lt;key&#x3D;value&gt;      Variable subsitution to apply to hive                   commands. e.g. -d A&#x3D;B or --define A&#x3D;B  --database &lt;databasename&gt;   Specify the database to use -e &lt;quoted-query-string&gt;     SQL from command line -f &lt;filename&gt;           SQL from files -H,--help             Print help information  --hiveconf &lt;property&#x3D;value&gt;  Use value for given property  --hivevar &lt;key&#x3D;value&gt;     Variable subsitution to apply to hive                  commands. e.g. --hivevar A&#x3D;B -i &lt;filename&gt;           Initialization SQL file -S,--silent            Silent mode in interactive shell -v,--verbose           Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure><p>1．“-e”不进入hive的交互窗口执行sql语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure><p>2．“-f”执行脚本中sql语句</p><p>​    （1）在/opt/module/datas目录下创建hivef.sql文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure><p>​        文件中写入正确的sql语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select *from student;</span><br></pre></td></tr></table></figure><p>​    （2）执行文件中的sql语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive -f &#x2F;opt&#x2F;module&#x2F;datas&#x2F;hivef.sql</span><br></pre></td></tr></table></figure><p>（3）执行文件中的sql语句并将结果写入文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive -f &#x2F;opt&#x2F;module&#x2F;datas&#x2F;hivef.sql  &gt; &#x2F;opt&#x2F;module&#x2F;datas&#x2F;hive_result.txt</span><br></pre></td></tr></table></figure><h2 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h2><p>1．退出hive窗口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;exit;</span><br><span class="line"></span><br><span class="line">hive(default)&gt;quit;</span><br></pre></td></tr></table></figure><p>在新版的hive中没区别了，在以前的版本是有的：</p><p>exit:先隐性提交数据，再退出；</p><p>quit:不提交数据，退出；</p><p>2．在hive cli命令窗口中如何查看hdfs文件系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;dfs -ls &#x2F;;</span><br></pre></td></tr></table></figure><p>3．在hive cli命令窗口中如何查看本地文件系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;! ls &#x2F;opt&#x2F;module&#x2F;datas;</span><br></pre></td></tr></table></figure><p>4．查看在hive中输入的所有历史命令</p><p>​    （1）进入到当前用户的根目录/root或/home/red</p><p>​    （2）查看. hivehistory文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure><h2 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h2><h3 id="Hive数据仓库位置配置"><a href="#Hive数据仓库位置配置" class="headerlink" title="Hive数据仓库位置配置"></a>Hive数据仓库位置配置</h3><p>​    1）Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。</p><p>​    2）在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。</p><p>​    3）修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><p>bin/hdfs dfs -chmod g+w /user/hive/warehouse</p><h3 id="查询后信息显示配置"><a href="#查询后信息显示配置" class="headerlink" title="查询后信息显示配置"></a>查询后信息显示配置</h3><p>1）在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>​    2）重新启动hive，对比配置前后差异。</p><p>（1）配置前，如图6-2所示</p><p> <img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps7.jpg" alt="wps7"></p><p>图6-2 配置前</p><p>（2）配置后，如图6-3所示</p><p> <img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps8.jpg" alt="wps8"></p><p>图6-3 配置后</p><h3 id="Hive运行日志信息配置"><a href="#Hive运行日志信息配置" class="headerlink" title="Hive运行日志信息配置"></a>Hive运行日志信息配置</h3><p>1．Hive的log默认存放在/tmp/red/hive.log目录下（当前用户名下）</p><p>2．修改hive的log存放日志到/opt/module/hive/logs</p><p>​    （1）修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为</p><blockquote><p>hive-log4j.properties</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop102 conf]$ pwd</span><br><span class="line"></span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf</span><br><span class="line"></span><br><span class="line">[red@hadoop102 conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure><p>​    （2）在hive-log4j.properties文件中修改log存放位置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.log.dir&#x3D;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs</span><br></pre></td></tr></table></figure><h3 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h3><p>1．查看当前所有的配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set;</span><br></pre></td></tr></table></figure><p>2．参数的配置三种方式</p><p>​    （1）配置文件方式</p><p>默认配置文件：hive-default.xml </p><p>用户自定义配置文件：hive-site.xml</p><p>​    注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p><p>（2）命令行参数方式</p><p>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[red@hadoop103 hive]$ bin&#x2F;hive -hiveconf mapred.reduce.tasks&#x3D;10;</span><br></pre></td></tr></table></figure><p>注意：仅对本次hive启动有效</p><p>查看参数设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>（3）参数声明方式</p><p>可以在HQL中使用SET关键字设定参数</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks&#x3D;100;</span><br></pre></td></tr></table></figure><p>注意：仅对本次hive启动有效。</p><p>查看参数设置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p><h1 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><p>表6-1</p><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE  FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><p>表6-2</p><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()</td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><p>案例实操</p><p>1） 假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p><blockquote><p>{  “name”: “songsong”,  “friends”: [“bingbing” , “lili”] ,    </p><p>//列表Array,   “children”: {            </p><p>//键值Map,    “xiao song”: 18 ,    “xiaoxiao song”: 19 </p><p>}  “address”: {            </p><p>//结构Struct,   </p><p>“street”: “hui long guan” ,    “city”: “beijing”   }}</p></blockquote><p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。 </p><p>创建本地测试文件test.txt</p><blockquote><p>songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</p></blockquote><p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p><p>3）Hive上创建测试表test</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table test(name string,friends array&lt;string&gt;,children map&lt;string, int&gt;,address struct&lt;street:string, city:string&gt;)row format delimited fields terminated by &#39;,&#39;collection items terminated by &#39;_&#39;map keys terminated by &#39;:&#39;lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><p>字段解释：</p><p>row format delimited fields terminated by ‘,’  – 列分隔符</p><p>collection items terminated by ‘_’  –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p>map keys terminated by ‘:’                – MAP中的key与value的分隔符</p><p>lines terminated by ‘\n’;                    – 行分隔符</p><p>4）导入文本数据到测试表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘&#x2F;opt&#x2F;module&#x2F;datas&#x2F;test.txt’into table test</span><br></pre></td></tr></table></figure><p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from testwhere name&#x3D;&quot;songsong&quot;;</span><br><span class="line">OK_c0   _c1   citylili   18    beijingTime taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><h2 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h2><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><p>1．隐式类型转换规则如下</p><p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</p><p>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</p><p>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。</p><p>（4）BOOLEAN类型不可以转换为任何其它的类型。</p><p>2．可以使用CAST操作显示进行数据类型转换</p><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="大数据" scheme="http://www.red0819.top/categories/bigdata/"/>
    
    
    <category term="Hive" scheme="http://www.red0819.top/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>testfile</title>
    <link href="http://www.red0819.top/2017/09/03/testfile/"/>
    <id>http://www.red0819.top/2017/09/03/testfile/</id>
    <published>2017-09-03T07:09:47.000Z</published>
    <updated>2020-07-03T08:05:30.515Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>public class First{</p><p>​    public static void main(String[] args){</p><p>​        System.out.println(“Hello Hexo”);</p><p>​    }</p><p>}</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="First_Test" scheme="http://www.red0819.top/tags/First-Test/"/>
    
  </entry>
  
</feed>
