<!DOCTYPE html>
<html lang="zh-CN">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="Snow Monster" />



<meta name="description" content="[TOC] 概述Spark Streaming是微批次处理方式，批处理间隔是Spark Streaming是的核心概念和关键参数。 Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行 Spark Streaming是什么Spark流使得构建可扩展的容错流应用程序变得更加容易。 Spark Streaming用于流式数据的处理。S">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkStreaming">
<meta property="og:url" content="http://www.red0819.top/2020/03/11/SparkStreaming/index.html">
<meta property="og:site_name" content="Snow MonsterBlog">
<meta property="og:description" content="[TOC] 概述Spark Streaming是微批次处理方式，批处理间隔是Spark Streaming是的核心概念和关键参数。 Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行 Spark Streaming是什么Spark流使得构建可扩展的容错流应用程序变得更加容易。 Spark Streaming用于流式数据的处理。S">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming-logo.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming%E5%9B%BE.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming%E5%9B%BE%E8%A7%A3.png">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming5.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming6.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming7.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming8.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/spark1.5%E4%B9%8B%E5%90%8E%E6%9E%B6%E6%9E%84.png">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming9.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming3.png">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming10.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming11.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming12.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming13.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming14.jpg">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/image-20200914120823766.png">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/image-20200914120922340.png">
<meta property="og:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming15.jpg">
<meta property="article:published_time" content="2020-03-11T08:45:00.000Z">
<meta property="article:modified_time" content="2020-09-14T09:52:21.837Z">
<meta property="article:author" content="Snow Monster">
<meta property="article:tag" content="SparkStreaming">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.red0819.top/2020/03/11/SparkStreaming/SparkStreaming-logo.jpg">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Snow MonsterBlog" type="application/atom+xml">



    <link rel="shortcut icon" href="/img/logo.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">




<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>SparkStreaming | Snow MonsterBlog</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 4.2.1"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/yanmoai.jpg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">Snow Monster</a></h1>
        </hgroup>

        
        <p class="header-subtitle">我らが征くは星の大海</p>
        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="/jqh0819@163.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa CSDN" href="https://mp.csdn.net/console/article" target="_blank" rel="noopener" title="CSDN"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/First-Test/" rel="tag">First_Test</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL%E5%9F%BA%E7%A1%80/" rel="tag">SQL基础</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SparkStreaming/" rel="tag">SparkStreaming</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/" rel="tag">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B0%9D%E8%AF%95/" rel="tag">尝试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/" target="_blank" rel="noopener">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/" target="_blank" rel="noopener">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">欲知后事如何，且听我下回分说</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">Snow Monster</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/yanmoai.jpg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">Snow Monster</a></h1>
            </hgroup>
            
            <p class="header-subtitle">我らが征くは星の大海</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="/jqh0819@163.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa CSDN" target="_blank" href="https://mp.csdn.net/console/article" title="CSDN"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-SparkStreaming" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/03/11/SparkStreaming/" class="article-date">
      <time datetime="2020-03-11T08:45:00.000Z" itemprop="datePublished">2020-03-11</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      SparkStreaming
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/bigdata/">大数据</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SparkStreaming/" rel="tag">SparkStreaming</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p><img src="/2020/03/11/SparkStreaming/SparkStreaming-logo.jpg" alt="SparkStreaming-logo"></p>
<p>[TOC]</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Spark Streaming是微批次处理方式，批处理间隔是Spark Streaming是的核心概念和关键参数。</p>
<p>Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行</p>
<h2 id="Spark-Streaming是什么"><a href="#Spark-Streaming是什么" class="headerlink" title="Spark Streaming是什么"></a>Spark Streaming是什么</h2><p>Spark流使得构建可扩展的容错流应用程序变得更加容易。</p>
<p>Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming%E5%9B%BE.jpg" alt="SparkStreaming图"></p>
<p>和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。<strong>在内部，每个时间区间收到的数据都作为 RDD 存在，而DStream是由这些RDD所组成的序列(因此得名“离散化”)。</strong></p>
<p>离散流反义词就是连续流。</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming%E5%9B%BE%E8%A7%A3.png" alt="SparkStreaming图解"></p>
<h2 id="Spark-Streaming的特点"><a href="#Spark-Streaming的特点" class="headerlink" title="Spark Streaming的特点"></a>Spark Streaming的特点</h2><p><strong>易用</strong></p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming5.jpg" alt="SparkStreaming5"></p>
<p><strong>容错</strong></p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming6.jpg" alt="SparkStreaming6"></p>
<p> <strong>易整合到Spark体系</strong><img src="/2020/03/11/SparkStreaming/SparkStreaming7.jpg" alt="SparkStreaming7"></p>
<h2 id="Spark-Streaming架构"><a href="#Spark-Streaming架构" class="headerlink" title="Spark Streaming架构"></a>Spark Streaming架构</h2><p>最基本的架构：底层就是spark-core。数据采集和封装之后传给Driver，Driver拿到相应的RDD，再形成一个一个Task然后传给Executor执行。。</p>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p> 整体架构图</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming8.jpg" alt="SparkStreaming8"></p>
<p>​                                        spark1.5之前</p>
<p><img src="/2020/03/11/SparkStreaming/spark1.5%E4%B9%8B%E5%90%8E%E6%9E%B6%E6%9E%84.png" alt="spark1.5之后架构"></p>
<p>​                                       spark1.5之后</p>
<p>SparkStreaming架构图</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming9.jpg" alt="SparkStreaming9"></p>
<h3 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h3><p>背压(back pressure)机制主要用于解决流处理系统中，业务流量在短时间内剧增，造成巨大的流量毛刺，数据流入速度远高于数据处理速度，对流处理系统构成巨大的负载压力的问题。</p>
<p>如果不能处理流量毛刺或者持续的数据过高速率输入，可能导致Executor端出现OOM的情况或者任务崩溃。</p>
<h4 id="Spark-1-5以前版本"><a href="#Spark-1-5以前版本" class="headerlink" title="Spark 1.5以前版本"></a>Spark 1.5以前版本</h4><p>用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现（限制每个receiver没每秒最大可以接收的数据量）。此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。</p>
<p>direct-approach方式接收数据，可以配置 “spark.streaming.kafka.maxRatePerPartition”参数来限制每个kafka分区最多读取的数据量。</p>
<p>缺点：</p>
<p>​          1、实现需要进行压测，来设置最大值。参数的设置必须合理，如果集群处理能力高于配置的速率，则会造成资源的浪费。</p>
<p>​          2、参数需要手动设置，设置过后必须重启streaming服务。</p>
<h4 id="Spark-1-5以后版本"><a href="#Spark-1-5以后版本" class="headerlink" title="Spark 1.5以后版本"></a>Spark 1.5以后版本</h4><p>为了更好的协调数据接收速率与资源处理能力，1.5版本开始Spark Streaming可以动态控制数据接收速率来适配集群数据处理能力（能够根据当前数据量以及集群状态来预估下个批次最优速率）。背压机制（即Spark Streaming Backpressure）: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。</p>
<p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。</p>
<p><strong><u>以下背压机制spark1.5之后流程以及配置均摘抄自：<a href="https://blog.csdn.net/may_fly/article/details/103922862" target="_blank" rel="noopener">https://blog.csdn.net/may_fly/article/details/103922862</a></u></strong></p>
<p>新版具体流程如下：<img src="/2020/03/11/SparkStreaming/SparkStreaming3.png" alt="SparkStreaming3"></p>
<p>新版的背压机制主要通过<code>RateController</code>组件来实现。<code>RateController</code>继承了接口<code>StreamingListener</code>并实现了<code>onBatchCompleted</code>方法。</p>
<p>结合direct-approach方式的源码来理解</p>
<ol>
<li>首先创建一个kafka流。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>,<span class="type">String</span>,<span class="type">StringDecoder</span>,<span class="type">StringDecoder</span>,(<span class="type">String</span>,<span class="type">String</span>)](streamingContext, kafkaParams, getOffsets(topics,kc,kafkaParams),messageHandler)</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>

<ol>
<li>createDirectStream方法创建并返回一个DirectKafkaInputDStream对象</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create an input stream that directly pulls messages from Kafka Brokers</span></span><br><span class="line"><span class="comment">   * without using any receiver. This stream can guarantee that each message</span></span><br><span class="line"><span class="comment">   * from Kafka is included in transformations exactly once (see points below).</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Points to note:</span></span><br><span class="line"><span class="comment">   *  - No receivers: This stream does not use any receiver. It directly queries Kafka</span></span><br><span class="line"><span class="comment">   *  - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked</span></span><br><span class="line"><span class="comment">   *    by the stream itself. For interoperability with Kafka monitoring tools that depend on</span></span><br><span class="line"><span class="comment">   *    Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.</span></span><br><span class="line"><span class="comment">   *    You can access the offsets used in each batch from the generated RDDs (see</span></span><br><span class="line"><span class="comment">   *    [[org.apache.spark.streaming.kafka.HasOffsetRanges]]).</span></span><br><span class="line"><span class="comment">   *  - Failure Recovery: To recover from driver failures, you have to enable checkpointing</span></span><br><span class="line"><span class="comment">   *    in the `StreamingContext`. The information on consumed offset can be</span></span><br><span class="line"><span class="comment">   *    recovered from the checkpoint. See the programming guide for details (constraints, etc.).</span></span><br><span class="line"><span class="comment">   *  - End-to-end semantics: This stream ensures that every records is effectively received and</span></span><br><span class="line"><span class="comment">   *    transformed exactly once, but gives no guarantees on whether the transformed data are</span></span><br><span class="line"><span class="comment">   *    outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure</span></span><br><span class="line"><span class="comment">   *    that the output operation is idempotent, or use transactions to output records atomically.</span></span><br><span class="line"><span class="comment">   *    See the programming guide for more details.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param ssc StreamingContext object</span></span><br><span class="line"><span class="comment">   * @param kafkaParams Kafka &lt;a href="http://kafka.apache.org/documentation.html#configuration"&gt;</span></span><br><span class="line"><span class="comment">   *    configuration parameters&lt;/a&gt;. Requires "metadata.broker.list" or "bootstrap.servers"</span></span><br><span class="line"><span class="comment">   *    to be set with Kafka broker(s) (NOT zookeeper servers) specified in</span></span><br><span class="line"><span class="comment">   *    host1:port1,host2:port2 form.</span></span><br><span class="line"><span class="comment">   * @param fromOffsets Per-topic/partition Kafka offsets defining the (inclusive)</span></span><br><span class="line"><span class="comment">   *    starting point of the stream</span></span><br><span class="line"><span class="comment">   * @param messageHandler Function for translating each message and metadata into the desired type</span></span><br><span class="line"><span class="comment">   * @tparam K type of Kafka message key</span></span><br><span class="line"><span class="comment">   * @tparam V type of Kafka message value</span></span><br><span class="line"><span class="comment">   * @tparam KD type of Kafka message key decoder</span></span><br><span class="line"><span class="comment">   * @tparam VD type of Kafka message value decoder</span></span><br><span class="line"><span class="comment">   * @tparam R type returned by messageHandler</span></span><br><span class="line"><span class="comment">   * @return DStream of R</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDirectStream</span></span>[</span><br><span class="line">    <span class="type">K</span>: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">V</span>: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">KD</span> &lt;: <span class="type">Decoder</span>[<span class="type">K</span>]: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">VD</span> &lt;: <span class="type">Decoder</span>[<span class="type">V</span>]: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">R</span>: <span class="type">ClassTag</span>] (</span><br><span class="line">      ssc: <span class="type">StreamingContext</span>,</span><br><span class="line">      kafkaParams: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">      fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>],</span><br><span class="line">      messageHandler: <span class="type">MessageAndMetadata</span>[<span class="type">K</span>, <span class="type">V</span>] =&gt; <span class="type">R</span></span><br><span class="line">  ): <span class="type">InputDStream</span>[<span class="type">R</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanedHandler = ssc.sc.clean(messageHandler)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DirectKafkaInputDStream</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">KD</span>, <span class="type">VD</span>, <span class="type">R</span>](</span><br><span class="line">      ssc, kafkaParams, fromOffsets, cleanedHandler)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152</span></span><br></pre></td></tr></table></figure>

<ol>
<li>DirectKafkaInputDStream类继承了抽象类InputDStream，并重载了rateController方法。创建了DirectKafkaRateController类，并传入了一个速率估计类。如果设置RateController.isBackPressureEnabled为true也就是开启背压则开始计算下一次的最优速率</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Asynchronously maintains &amp; sends new rate limits to the receiver through the receiver tracker.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span>[streaming] <span class="keyword">val</span> rateController: <span class="type">Option</span>[<span class="type">RateController</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">RateController</span>.isBackPressureEnabled(ssc.conf)) &#123;</span><br><span class="line">      <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">DirectKafkaRateController</span>(id,</span><br><span class="line">        <span class="type">RateEstimator</span>.create(ssc.conf, context.graph.batchDuration)))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">1234567891011</span></span><br></pre></td></tr></table></figure>

<ol>
<li>DirectKafkaRateController内部实现了一个私有类来计算速率，publish方法使用lambda表达式调用了RateController中唯一一个公有的方法onBatchCompleted获取计算结果</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A RateController to retrieve the rate from RateEstimator.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[streaming] <span class="class"><span class="keyword">class</span> <span class="title">DirectKafkaRateController</span>(<span class="params">id: <span class="type">Int</span>, estimator: <span class="type">RateEstimator</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">RateController</span>(<span class="params">id, estimator</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">publish</span></span>(rate: <span class="type">Long</span>): <span class="type">Unit</span> = ()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">1234567</span></span><br></pre></td></tr></table></figure>

<ol>
<li>onBatchCompleted获取三个时间一个数据量：处理结束时间，处理时间，等待时间，当前处理数据量，并调用computeAndPublish方法计算下次最优的数据量</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBatchCompleted</span></span>(batchCompleted: <span class="type">StreamingListenerBatchCompleted</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> elements = batchCompleted.batchInfo.streamIdToInputInfo</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">      processingEnd &lt;- batchCompleted.batchInfo.processingEndTime</span><br><span class="line">      workDelay &lt;- batchCompleted.batchInfo.processingDelay</span><br><span class="line">      waitDelay &lt;- batchCompleted.batchInfo.schedulingDelay</span><br><span class="line">      elems &lt;- elements.get(streamUID).map(_.numRecords)</span><br><span class="line">    &#125; computeAndPublish(processingEnd, elems, workDelay, waitDelay)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">12345678910</span></span><br></pre></td></tr></table></figure>

<ol>
<li>computeAndPublish调用rateEstimator.compute方法计算速率</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Compute the new rate limit and publish it asynchronously.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">computeAndPublish</span></span>(time: <span class="type">Long</span>, elems: <span class="type">Long</span>, workDelay: <span class="type">Long</span>, waitDelay: <span class="type">Long</span>): <span class="type">Unit</span> =</span><br><span class="line">    <span class="type">Future</span>[<span class="type">Unit</span>] &#123;</span><br><span class="line">      <span class="keyword">val</span> newRate = rateEstimator.compute(time, elems, workDelay, waitDelay)</span><br><span class="line">      newRate.foreach &#123; s =&gt;</span><br><span class="line">        rateLimit.set(s.toLong)</span><br><span class="line">        publish(getLatestRate())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Computes the number of records the stream attached to this `RateEstimator`</span></span><br><span class="line"><span class="comment">   * should ingest per second, given an update on the size and completion</span></span><br><span class="line"><span class="comment">   * times of the latest batch.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param time The timestamp of the current batch interval that just finished</span></span><br><span class="line"><span class="comment">   * @param elements The number of records that were processed in this batch</span></span><br><span class="line"><span class="comment">   * @param processingDelay The time in ms that took for the job to complete</span></span><br><span class="line"><span class="comment">   * @param schedulingDelay The time in ms that the job spent in the scheduling queue</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>,</span><br><span class="line">      elements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>,</span><br><span class="line">      schedulingDelay: <span class="type">Long</span>): <span class="type">Option</span>[<span class="type">Double</span>]</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627</span></span><br></pre></td></tr></table></figure>

<ol>
<li>compute方法的具体实现，需要来看3中<code>RateEstimator.create(ssc.conf, context.graph.batchDuration)))</code>传入的RateEstimator类。由源码可知，默认调用pid速率估计器，是 <code>RateEstimator</code>的唯一实现 ，具体计算逻辑要看pid速率估计器的compute方法</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RateEstimator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new `RateEstimator` based on the value of</span></span><br><span class="line"><span class="comment">   * `spark.streaming.backpressure.rateEstimator`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * The only known and acceptable estimator right now is `pid`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return An instance of RateEstimator</span></span><br><span class="line"><span class="comment">   * @throws IllegalArgumentException if the configured RateEstimator is not `pid`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(conf: <span class="type">SparkConf</span>, batchInterval: <span class="type">Duration</span>): <span class="type">RateEstimator</span> =</span><br><span class="line">    conf.get(<span class="string">"spark.streaming.backpressure.rateEstimator"</span>, <span class="string">"pid"</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"pid"</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> proportional = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.proportional"</span>, <span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">val</span> integral = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.integral"</span>, <span class="number">0.2</span>)</span><br><span class="line">        <span class="keyword">val</span> derived = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.derived"</span>, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">val</span> minRate = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.minRate"</span>, <span class="number">100</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PIDRateEstimator</span>(batchInterval.milliseconds, proportional, integral, derived, minRate)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> estimator =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Unknown rate estimator: <span class="subst">$estimator</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="number">123456789101112131415161718192021222324</span></span><br></pre></td></tr></table></figure>

<ol>
<li>pid速率估计器的compute方法如下。具体流程不再细述，有时间举个例子推一下</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      numElements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      schedulingDelay: <span class="type">Long</span> <span class="comment">// in milliseconds</span></span><br><span class="line">    ): <span class="type">Option</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    logTrace(<span class="string">s"\ntime = <span class="subst">$time</span>, # records = <span class="subst">$numElements</span>, "</span> +</span><br><span class="line">      <span class="string">s"processing time = <span class="subst">$processingDelay</span>, scheduling delay = <span class="subst">$schedulingDelay</span>"</span>)</span><br><span class="line">    <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (time &gt; latestTime &amp;&amp; numElements &gt; <span class="number">0</span> &amp;&amp; processingDelay &gt; <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in seconds, should be close to batchDuration</span></span><br><span class="line">        <span class="keyword">val</span> delaySinceUpdate = (time - latestTime).toDouble / <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> processingRate = numElements.toDouble / processingDelay * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// In our system `error` is the difference between the desired rate and the measured rate</span></span><br><span class="line">        <span class="comment">// based on the latest batch information. We consider the desired rate to be latest rate,</span></span><br><span class="line">        <span class="comment">// which is what this estimator calculated for the previous batch.</span></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> error = latestRate - processingRate</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The error integral, based on schedulingDelay as an indicator for accumulated errors.</span></span><br><span class="line">        <span class="comment">// A scheduling delay s corresponds to s * processingRate overflowing elements. Those</span></span><br><span class="line">        <span class="comment">// are elements that couldn't be processed in previous batches, leading to this delay.</span></span><br><span class="line">        <span class="comment">// In the following, we assume the processingRate didn't change too much.</span></span><br><span class="line">        <span class="comment">// From the number of overflowing elements we can calculate the rate at which they would be</span></span><br><span class="line">        <span class="comment">// processed by dividing it by the batch interval. This rate is our "historical" error,</span></span><br><span class="line">        <span class="comment">// or integral part, since if we subtracted this rate from the previous "calculated rate",</span></span><br><span class="line">        <span class="comment">// there wouldn't have been any overflowing elements, and the scheduling delay would have</span></span><br><span class="line">        <span class="comment">// been zero.</span></span><br><span class="line">        <span class="comment">// (in elements/second)</span></span><br><span class="line">        <span class="keyword">val</span> historicalError = schedulingDelay.toDouble * processingRate / batchIntervalMillis</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/(second ^ 2)</span></span><br><span class="line">        <span class="keyword">val</span> dError = (error - latestError) / delaySinceUpdate</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> newRate = (latestRate - proportional * error -</span><br><span class="line">                                    integral * historicalError -</span><br><span class="line">                                    derivative * dError).max(minRate)</span><br><span class="line">        logTrace(<span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">            | latestRate = $latestRate, error = $error</span></span><br><span class="line"><span class="string">            | latestError = $latestError, historicalError = $historicalError</span></span><br><span class="line"><span class="string">            | delaySinceUpdate = $delaySinceUpdate, dError = $dError</span></span><br><span class="line"><span class="string">            "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        latestTime = time</span><br><span class="line">        <span class="keyword">if</span> (firstRun) &#123;</span><br><span class="line">          latestRate = processingRate</span><br><span class="line">          latestError = <span class="number">0</span>D</span><br><span class="line">          firstRun = <span class="literal">false</span></span><br><span class="line">          logTrace(<span class="string">"First run, rate estimation skipped"</span>)</span><br><span class="line">          <span class="type">None</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          latestRate = newRate</span><br><span class="line">          latestError = error</span><br><span class="line">          logTrace(<span class="string">s"New rate = <span class="subst">$newRate</span>"</span>)</span><br><span class="line">          <span class="type">Some</span>(newRate)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logTrace(<span class="string">"Rate estimation skipped"</span>)</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566</span></span><br></pre></td></tr></table></figure>

<ol>
<li>虽然通过这个公式计算出了一个速率，但最终的速率并不一定是计算出的结果。由代码可知，如果设置了参数spark.streaming.kafka.maxRatePerPartition，则每个分区所取数据最大量为计算出的结果以及设置参数的最小值，否则直接使用计算出的值</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span>[streaming] <span class="function"><span class="keyword">def</span> <span class="title">maxMessagesPerPartition</span></span>(</span><br><span class="line">      offsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>]): <span class="type">Option</span>[<span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> estimatedRateLimit = rateController.map(_.getLatestRate())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate a per-partition rate limit based on current lag</span></span><br><span class="line">    <span class="keyword">val</span> effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ &gt; <span class="number">0</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(rate) =&gt;</span><br><span class="line">        <span class="keyword">val</span> lagPerPartition = offsets.map &#123; <span class="keyword">case</span> (tp, offset) =&gt;</span><br><span class="line">          tp -&gt; <span class="type">Math</span>.max(offset - currentOffsets(tp), <span class="number">0</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> totalLag = lagPerPartition.values.sum</span><br><span class="line"></span><br><span class="line">        lagPerPartition.map &#123; <span class="keyword">case</span> (tp, lag) =&gt;</span><br><span class="line">          <span class="keyword">val</span> backpressureRate = <span class="type">Math</span>.round(lag / totalLag.toFloat * rate)</span><br><span class="line">          tp -&gt; (<span class="keyword">if</span> (maxRateLimitPerPartition &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">Math</span>.min(backpressureRate, maxRateLimitPerPartition)&#125; <span class="keyword">else</span> backpressureRate)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; offsets.map &#123; <span class="keyword">case</span> (tp, offset) =&gt; tp -&gt; maxRateLimitPerPartition &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> maxRateLimitPerPartition: <span class="type">Long</span> = context.sparkContext.getConf.getLong(</span><br><span class="line">      <span class="string">"spark.streaming.kafka.maxRatePerPartition"</span>, <span class="number">0</span>)</span><br><span class="line"><span class="number">12345678910111213141516171819202122</span></span><br></pre></td></tr></table></figure>

<p>一些相关的参数：</p>
<ol>
<li>开启背压机制：设置<strong>spark.streaming.backpressure.enabled</strong> 为true，默认为false</li>
<li>启用反压机制时每个接收器接收第一批数据的初始最大速率。默认值没有设置 <strong>spark.streaming.backpressure.initialRate</strong></li>
<li>速率估算器类，默认值为 pid ，目前 Spark 只支持这个，大家可以根据自己的需要实现 <strong>spark.streaming.backpressure.rateEstimator</strong></li>
<li>用于响应错误的权重（最后批次和当前批次之间的更改）。默认值为1，只能设置成非负值。<em>weight for response to “error” (change between last batch and this batch)</em> <strong>spark.streaming.backpressure.pid.proportional</strong></li>
<li>错误积累的响应权重，具有抑制作用（有效阻尼）。默认值为 0.2 ，只能设置成非负值。<em>weight for the response to the accumulation of error. This has a dampening effect.</em> <strong>spark.streaming.backpressure.pid.integral</strong></li>
<li>对错误趋势的响应权重。 这可能会引起 batch size 的波动，可以帮助快速增加/减少容量。默认值为0，只能设置成非负值。<em>weight for the response to the trend in error. This can cause arbitrary/noise-induced fluctuations in batch size, but can also help react quickly to increased/reduced capacity.</em> <strong>spark.streaming.backpressure.pid.derived</strong></li>
<li>可以估算的最低费率是多少。默认值为 100，只能设置成非负值。 <strong>spark.streaming.backpressure.pid.minRate</strong></li>
</ol>
<p>参考：</p>
<p><a href="https://blog.csdn.net/wangpei1949/article/details/90727805" target="_blank" rel="noopener">https://blog.csdn.net/wangpei1949/article/details/90727805</a></p>
<p><a href="https://blog.csdn.net/zengxiaosen/article/details/72822869" target="_blank" rel="noopener">https://blog.csdn.net/zengxiaosen/article/details/72822869</a></p>
<p><a href="https://www.cnblogs.com/barrenlake/p/5349949.html" target="_blank" rel="noopener">https://www.cnblogs.com/barrenlake/p/5349949.html</a></p>
<p><a href="https://www.iteblog.com/archives/2323.html?from=related" target="_blank" rel="noopener">https://www.iteblog.com/archives/2323.html?from=related</a></p>
<h1 id="Dstream入门"><a href="#Dstream入门" class="headerlink" title="Dstream入门"></a>Dstream入门</h1><h2 id="WordCount案例实操"><a href="#WordCount案例实操" class="headerlink" title="WordCount案例实操"></a>WordCount案例实操</h2><p>需求：使用netcat工具向9999端口不断的发送数据，通过SparkStreaming读取端口数据并统计不同单词出现的次数</p>
<p>1) 添加依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2) 编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Wordcount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Todo 1.配置对象</span></span><br><span class="line">    <span class="comment">//初始化Spark配置信息</span></span><br><span class="line">    <span class="comment">//Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行（local至少要两个节点）</span></span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Todo 2.环境对象</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    class StreamingContext private[streaming]加了包权限（私有） 所以主构不能用</span></span><br><span class="line"><span class="comment">    def this(sparkContext: SparkContext, batchDuration: Duration) 辅助构建方法可用（Spark配置信息，批处理持续时间）</span></span><br><span class="line"><span class="comment">    case class Duration (private val millis: Long)样例类 直接用 但是不方便 要自己算毫秒</span></span><br><span class="line"><span class="comment">    new StreamingContext(sparkconf , Duration(1000 * 3))</span></span><br><span class="line"><span class="comment">    所以直接使用伴生对象Seconds()</span></span><br><span class="line"><span class="comment">     object Seconds &#123;</span></span><br><span class="line"><span class="comment">        def apply(seconds: Long): Duration = new Duration(seconds * 1000)</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 初始化SparkStreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">3</span>)) <span class="comment">// 创建对象的第二个参数表示数据的采集周期</span></span><br><span class="line">    <span class="comment">//Todo 3.数据处理</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 从数据源采集数据</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 //内存和磁盘有两个序列化副本（socketStream默认的存储级别）</span></span><br><span class="line"><span class="comment">    ReceiverInputDStream[String]  Receiver:接收器 Input:输入  DStream:离散化流   */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span> )</span><br><span class="line">    <span class="comment">//TODO 将采集数据进行WordCount的处理</span></span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = socketDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在控制台上打印结果</span></span><br><span class="line"><span class="comment">/*    可以看出底层还是RDD</span></span><br><span class="line"><span class="comment">def print(num: Int): Unit = ssc.withScope &#123;</span></span><br><span class="line"><span class="comment">      def foreachFunc: (RDD[T], Time) =&gt; Unit = &#123;</span></span><br><span class="line"><span class="comment">        (rdd: RDD[T], time: Time) =&gt; &#123;</span></span><br><span class="line"><span class="comment">          val firstNum = rdd.take(num + 1)</span></span><br><span class="line"><span class="comment">          // scalastyle:off println</span></span><br><span class="line"><span class="comment">          println("-------------------------------------------")</span></span><br><span class="line"><span class="comment">          println(s"Time: $time")</span></span><br><span class="line"><span class="comment">          println("-------------------------------------------")</span></span><br><span class="line"><span class="comment">          firstNum.take(num).foreach(println)</span></span><br><span class="line"><span class="comment">          if (firstNum.length &gt; num) println("...")</span></span><br><span class="line"><span class="comment">          println()</span></span><br><span class="line"><span class="comment">          // scalastyle:on println</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Todo 4.开启连接环境</span></span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">    和spark、scala不同的是：最后并不关闭连接环境（除非程序升级或者出现故障的时候，因为数据采集是要7*24）</span></span><br><span class="line"><span class="comment">    并且不能让driver程序结束，需要让driver程序等待,等待数据处理的停止或异常时，才会继续执行</span></span><br><span class="line"><span class="comment">      def awaitTermination() &#123;</span></span><br><span class="line"><span class="comment">        waiter.waitForStopOrError()  //等待停止或者异常，如果没有停止和异常现象 程序不结束</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    ssc.start();</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3) 启动程序并通过netcat发送数据：</p>
<blockquote>
<p>nc -lk 9999</p>
<p>hello red</p>
<p>hello world</p>
</blockquote>
<h2 id="WordCount解析"><a href="#WordCount解析" class="headerlink" title="WordCount解析"></a>WordCount解析</h2><p>Discretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，<strong>DStream是一系列连续的RDD来表示</strong>。<u>每个RDD含有一段时间间隔内的数据（段时间内所有数据在一个RDD里）。</u><img src="/2020/03/11/SparkStreaming/SparkStreaming10.jpg" alt="SparkStreaming10"></p>
<p>对数据的操作也是按照RDD为单位来进行的</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming11.jpg" alt="SparkStreaming11"></p>
<p>计算过程由Spark Engine来完成</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming12.jpg" alt="SparkStreaming12"></p>
<h1 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h1><h2 id="RDD队列"><a href="#RDD队列" class="headerlink" title="RDD队列"></a>RDD队列</h2><h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue()"></a>Queue()</h3><h4 id="用法及说明"><a href="#用法及说明" class="headerlink" title="用法及说明"></a>用法及说明</h4><p><font color="red">测试过程</font>中，可以通过使用ssc.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。</p>
<h4 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h4><p>需求：循环创建几个RDD，将RDD放入队列。通过SparkStream创建Dstream，计算WordCount</p>
<p>1) 编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDStream</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.初始化Spark配置信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDDStream"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.初始化SparkStreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.创建RDD队列</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.创建QueueInputDStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.处理队列中的RDD数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.打印结果</span></span><br><span class="line"></span><br><span class="line">  reducedStream.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.启动任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//8.循环创建并向RDD队列中放入RDD</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line"></span><br><span class="line">   rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">   <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2) 结果展示</p>
<blockquote>
<p>-——————————————</p>
<p>Time: 1539075280000 ms</p>
<p>-——————————————</p>
<p>(4,60)</p>
<p>(0,60)</p>
<p>(6,60)</p>
<p>(8,60)</p>
<p>(2,60)</p>
<p>(1,60)</p>
<p>(3,60)</p>
<p>(7,60)</p>
<p>(9,60)</p>
<p>(5,60)</p>
<p>-——————————————</p>
<p>Time: 1539075284000 ms</p>
<p>-——————————————</p>
<p>(4,60)</p>
<p>(0,60)</p>
<p>(6,60)</p>
<p>(8,60)</p>
<p>(2,60)</p>
<p>(1,60)</p>
<p>(3,60)</p>
<p>(7,60)</p>
<p>(9,60)</p>
<p>(5,60)</p>
<p>-——————————————</p>
<p>Time: 1539075288000 ms</p>
<p>-——————————————</p>
<p>(4,30)</p>
<p>(0,30)</p>
<p>(6,30)</p>
<p>(8,30)</p>
<p>(2,30)</p>
<p>(1,30)</p>
<p>(3,30)</p>
<p>(7,30)</p>
<p>(9,30)</p>
<p>(5,30)</p>
<p>-——————————————</p>
<p>Time: 1539075292000 ms</p>
<p>-——————————————</p>
</blockquote>
<h3 id="file"><a href="#file" class="headerlink" title="file()"></a>file()</h3><h4 id="用法及说明-1"><a href="#用法及说明-1" class="headerlink" title="用法及说明"></a>用法及说明</h4><p><font color="red">测试过程</font>中，可以通过使用ssc.textFileStream(“in”)来创建DStream，监控文件夹的变化。</p>
<p>从文件夹中读取新的文件数据（拽过去的文件可能读不到），功能不稳定 ，所以不推荐使用</p>
<p>flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume</p>
<h4 id="案例实操-1"><a href="#案例实操-1" class="headerlink" title="案例实操"></a>案例实操</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming03_DStream_File</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 数据处理</span></span><br><span class="line">    <span class="comment">// 从文件夹中读取新的文件数据，功能不稳定 ，所以不推荐使用</span></span><br><span class="line">    <span class="comment">// flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume</span></span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">DStream</span>[<span class="type">String</span>] = ssc.textFileStream(<span class="string">"in"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map( (_, <span class="number">1</span>) )</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭连接环境</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>






<h2 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h2><h3 id="用法及说明-2"><a href="#用法及说明-2" class="headerlink" title="用法及说明"></a>用法及说明</h3><p>需要继承Receiver，并实现onStart、onStop方法来自定义数据源采集。</p>
<h3 id="案例实操-2"><a href="#案例实操-2" class="headerlink" title="案例实操"></a>案例实操</h3><p>需求：自定义数据源，实现监控某个端口号，获取该端口号内容。</p>
<p>1) 自定义数据源</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  自定义数据采集器</span></span><br><span class="line"><span class="comment">    自定义数据采集器</span></span><br><span class="line"><span class="comment">    模仿spark自带的socket采集器</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  abstract class Receiver[T](val storageLevel: StorageLevel) extends Serializable</span></span><br><span class="line"><span class="comment">  StorageLevel :存储级别 MEMORY_ONLY DISK_ONLY MEMORY_AND_DISK</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  步骤： 1. 继承Receiver ,设定泛型（采集数据的类型）, 传递参数</span></span><br><span class="line"><span class="comment">         2. 重写方法</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyRecevier</span>(<span class="params">host : <span class="type">String</span> , port : <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>)</span>&#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   socketTextStream:  socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)</span></span><br><span class="line"><span class="comment">    socketStream：  new SocketInputDStream[T](this, hostname, port, converter, storageLevel)</span></span><br><span class="line"><span class="comment">    SocketInputDStream:  new SocketReceiver(host, port, bytesToObjects, storageLevel)</span></span><br><span class="line"><span class="comment">    SocketReceiver：extends Receiver[T](storageLevel) with Logging</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> socket: <span class="type">Socket</span> = _</span><br><span class="line">    <span class="comment">// SocketReceiver：socket.getInputStream()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">      <span class="comment">//我们需要字符串，所以将字节流转换为缓冲字符流</span></span><br><span class="line">      <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">InputStreamReader</span>(</span><br><span class="line">          <span class="comment">//获取从网络中传递来的数据（字节流）</span></span><br><span class="line">          socket.getInputStream,</span><br><span class="line">          <span class="string">"UTF-8"</span></span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      <span class="keyword">var</span> s:<span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">     <span class="comment">/*</span></span><br><span class="line"><span class="comment">      这个“s = reader.readLine())!= null”语句是错误的，因为在网络编程中获取的数据是没有null的概念</span></span><br><span class="line"><span class="comment">      文件读取时，如果读到结束的时候，获取的结果为null（文件读取这样是对的）</span></span><br><span class="line"><span class="comment">      但是在网络中我可以现在传递一些数据  过一段就再传一次，所以null是无法判断的</span></span><br><span class="line"><span class="comment">      网络编程中，需要明确告知服务器，客户端不再传数据，需要发送特殊的指令</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      <span class="keyword">while</span>(( s = reader.readLine())!= <span class="literal">null</span>)&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需要发送特殊的指令</span></span><br><span class="line">        <span class="keyword">if</span>(s != <span class="string">"-END-"</span>)&#123;</span><br><span class="line">          <span class="comment">//采集到数据后，进行封装(存储)</span></span><br><span class="line">              store(s)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          <span class="comment">// stop</span></span><br><span class="line">          <span class="comment">// close</span></span><br><span class="line">          <span class="comment">// 重启</span></span><br><span class="line">          <span class="comment">//restart("")</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 启动采集器</span></span><br><span class="line">    <span class="comment">// 采集 &amp; 封装</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      socket = <span class="keyword">new</span> <span class="type">Socket</span>(host , port)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"Socket Receiver"</span>) &#123;</span><br><span class="line">        setDaemon(<span class="literal">true</span>)<span class="comment">//守护线程</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">          receive() &#125;</span><br><span class="line">      &#125;.start()<span class="comment">//start()会回调run()方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (socket != <span class="literal">null</span>)&#123;</span><br><span class="line">        socket.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>2) 使用自定义的数据源采集数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamig_DIY</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span>  <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"DIY采集器"</span>)</span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// TODO 数据处理</span></span><br><span class="line">    <span class="comment">// 自定义数据采集器</span></span><br><span class="line">    <span class="keyword">val</span> myDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.receiverStream(<span class="keyword">new</span> <span class="type">MyRecevier</span>(<span class="string">"localhost"</span> , <span class="number">9999</span>))</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = myDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line">    <span class="comment">// TODO 开启连接环境</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h2 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h2><h3 id="版本选型"><a href="#版本选型" class="headerlink" title="版本选型"></a>版本选型</h3><p><strong>ReceiverAPI</strong>：需要一个专门的Executor去接收数据，然后发送给其他的Executor做计算。存在的问题，接收数据的Executor和计算的Executor速度会有所不同，特别在接收数据的Executor速度大于计算的Executor速度，会导致计算数据的节点内存溢出。</p>
<p><strong>DirectAPI</strong>：是由计算的Executor来主动消费Kafka的数据，速度由自身控制。</p>
<p> <img src="/2020/03/11/SparkStreaming/SparkStreaming13.jpg" alt="SparkStreaming13"></p>
<h3 id="Kafka-0-8-Receiver模式"><a href="#Kafka-0-8-Receiver模式" class="headerlink" title="Kafka 0-8 Receiver模式"></a>Kafka 0-8 Receiver模式</h3><p>1） 需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Kafka</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span>  <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"DIY采集器"</span>)</span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// TODO 数据处理 - 读取Kafka数据创建DStream(基于Receive方式)</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    def createStream(</span></span><br><span class="line"><span class="comment">                      ssc : org.apache.spark.streaming.StreamingContext,</span></span><br><span class="line"><span class="comment">                      zkQuorum : scala.Predef.String, //zookeeper</span></span><br><span class="line"><span class="comment">                      groupId : scala.Predef.String, //消费者组</span></span><br><span class="line"><span class="comment">                      topics : scala.Predef.Map[scala.Predef.String, scala.Int],//分区数</span></span><br><span class="line"><span class="comment">                      storageLevel : org.apache.spark.storage.StorageLevel = &#123;  compiled code  &#125;</span></span><br><span class="line"><span class="comment">                    ) : org.apache.spark.streaming.dstream.ReceiverInputDStream[scala.Tuple2[scala.Predef.String, scala.Predef.String]] = &#123;  compiled code  &#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaDS: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="string">"linux1:2181,linux2:2181,linux3:2181"</span>,</span><br><span class="line">      <span class="string">"red0819"</span>,</span><br><span class="line">      <span class="type">Map</span>(<span class="string">"red0819"</span> -&gt; <span class="number">3</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// Kafka消息传递的时候以k-v对</span></span><br><span class="line">    <span class="comment">// k - 传值的时候提供的，默认为null,主要用于分区</span></span><br><span class="line">    <span class="comment">// v - message</span></span><br><span class="line">    kafkaDS.map((_._2)).print()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Kafka-0-8-Direct模式"><a href="#Kafka-0-8-Direct模式" class="headerlink" title="Kafka 0-8 Direct模式"></a>Kafka 0-8 Direct模式</h3><p>1）需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<h4 id="自动维护offset"><a href="#自动维护offset" class="headerlink" title="自动维护offset"></a>自动维护offset</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIAuto02</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> getSSC1: () =&gt; <span class="type">StreamingContext</span> = () =&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">getSSC</span></span>: <span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//设置CK</span></span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck2"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.定义Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.读取Kafka数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc,</span><br><span class="line"></span><br><span class="line">   kafkaPara,</span><br><span class="line"></span><br><span class="line">   <span class="type">Set</span>(<span class="string">"red"</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.计算WordCount</span></span><br><span class="line"></span><br><span class="line">  kafkaDStream.map(_._2)</span><br><span class="line"></span><br><span class="line">   .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">   .print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.返回数据</span></span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取SSC</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck2"</span>, () =&gt; getSSC)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="手动维护offset"><a href="#手动维护offset" class="headerlink" title="手动维护offset"></a>手动维护offset</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.common.<span class="type">TopicAndPartition</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.message.<span class="type">MessageAndMetadata</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.&#123;<span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">OffsetRange</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIHandler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.获取上一次启动最后保留的Offset=&gt;getOffset(MySQL)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>] = <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>](<span class="type">TopicAndPartition</span>(<span class="string">"red"</span>, <span class="number">0</span>) -&gt; <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.读取Kafka数据创建DStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">String</span>] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>, <span class="type">String</span>](ssc,</span><br><span class="line"></span><br><span class="line">   kafkaPara,</span><br><span class="line"></span><br><span class="line">   fromOffsets,</span><br><span class="line"></span><br><span class="line">   (m: <span class="type">MessageAndMetadata</span>[<span class="type">String</span>, <span class="type">String</span>]) =&gt; m.message())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.创建一个数组用于存放当前消费数据的offset信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> offsetRanges = <span class="type">Array</span>.empty[<span class="type">OffsetRange</span>]</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.获取当前消费数据的offset信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToCountDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = kafkaDStream.transform &#123; rdd =&gt;</span><br><span class="line"></span><br><span class="line">   offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">   rdd</span><br><span class="line"></span><br><span class="line">  &#125;.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//8.打印Offset信息</span></span><br><span class="line"></span><br><span class="line">  wordToCountDStream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span>:<span class="subst">$&#123;o.partition&#125;</span>:<span class="subst">$&#123;o.fromOffset&#125;</span>:<span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//9.开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Kafka-0-10-Direct模式"><a href="#Kafka-0-10-Direct模式" class="headerlink" title="Kafka 0-10 Direct模式"></a>Kafka 0-10 Direct模式</h3><p>1）需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPI</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.定义Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.读取Kafka数据创建DStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line"></span><br><span class="line">   <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"red"</span>), kafkaPara))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.将每条消息的KV取出</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> valueDStream: <span class="type">DStream</span>[<span class="type">String</span>] = kafkaDStream.map(record =&gt; record.value())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.计算WordCount</span></span><br><span class="line"></span><br><span class="line">  valueDStream.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">   .print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="消费Kafka数据模式总结"><a href="#消费Kafka数据模式总结" class="headerlink" title="消费Kafka数据模式总结"></a>消费Kafka数据模式总结</h3><h4 id="0-8"><a href="#0-8" class="headerlink" title="0-8"></a>0-8</h4><h5 id="ReceiverAPI"><a href="#ReceiverAPI" class="headerlink" title="ReceiverAPI"></a>ReceiverAPI</h5><p>1) 专门的Executor读取数据，速度不统一</p>
<p>  数据丢失：预写日志开启</p>
<p>2) 跨机器传输数据</p>
<p>3) Executor读取数据通过多个线程的方式，想要增加并行度，则需要多个流union</p>
<p>4) offset存储在zookeeper中</p>
<h5 id="DirectAPI"><a href="#DirectAPI" class="headerlink" title="DirectAPI"></a>DirectAPI</h5><p>1) Executor读取数据并计算</p>
<p>2) 增加Executor个数来增加消费的并行度</p>
<p>3) offset存储</p>
<p>a. CheckPoint(getActiveOrCreate方式创建StreamingContext)</p>
<p>从checkpoint中读取数据偏移量（不推荐使用）</p>
<blockquote>
<p>理由：</p>
<p>​        checkpoint还保存了计算逻辑，不适合扩展功能<br>​                checkpoint会延续计算，但是可能会压垮内存<br>​                checkpoint一般的存储路径为HDFS，所以会导致小文件过多</p>
</blockquote>
<p>b. 手动维护(有事务的存储系统)</p>
<p>4) 获取offset必须在第一个调用的算子中：</p>
<p>offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</p>
<h4 id="0-10-DirectAPI"><a href="#0-10-DirectAPI" class="headerlink" title="0-10 DirectAPI"></a>0-10 DirectAPI</h4><p>1) Executor读取数据并计算</p>
<p>2) 增加Executor个数来增加消费的并行度</p>
<p>3) offset存储</p>
<p>a. __consumer_offsets系统主题中</p>
<p>b. 手动维护(有事务的存储系统)</p>
<h1 id="DStream转换"><a href="#DStream转换" class="headerlink" title="DStream转换"></a>DStream转换</h1><p>DStream上的操作与RDD的类似，分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。</p>
<h2 id="无状态转化操作"><a href="#无状态转化操作" class="headerlink" title="无状态转化操作"></a>无状态转化操作</h2><p>无状态转化操作就是把简单的RDD转化操作应用到每个批次上，也就是转化DStream中的每一个RDD。部分无状态转化操作列在了下表中。注意，针对键值对的DStream转化操作(比如 reduceByKey())要添加import StreamingContext._才能在Scala中使用。</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming14.jpg" alt="SparkStreaming14"></p>
<p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个DStream在内部是由许多RDD（批次）组成，且无状态转化操作是分别应用到每个RDD上的。</p>
<p>例如：reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。</p>
<h3 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h3><p>Transform允许DStream上执行任意的RDD-to-RDD函数。即使这些函数并没有在DStream的API中暴露出来，通过该函数可以方便的扩展Spark API。该函数每一批次调度一次。其实也就是对DStream中的RDD应用转换。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DStream_WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"queue"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf, <span class="type">Seconds</span>(<span class="number">5</span>));</span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resultDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDS.transform(</span><br><span class="line">      rdd =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> flatRDD: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = flatRDD.map((_, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> reduceRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_ + _)</span><br><span class="line">        reduceRDD</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line">    resultDS.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>两个流之间的join需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的RDD进行join，与两个RDD的join效果相同。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JoinTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"JoinTest"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.从端口获取数据创建流</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lineDStream1: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lineDStream2: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux2"</span>, <span class="number">8888</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.将两个流转换为KV类型</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToOneDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lineDStream1.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToADStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lineDStream2.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="string">"a"</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.流的JOIN</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> joinDStream: <span class="type">DStream</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">String</span>))] = wordToOneDStream.join(wordToADStream)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.打印</span></span><br><span class="line"></span><br><span class="line">  joinDStream.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.启动任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="有状态转化操作"><a href="#有状态转化操作" class="headerlink" title="有状态转化操作"></a>有状态转化操作</h2><h3 id="UpdateStateByKey"><a href="#UpdateStateByKey" class="headerlink" title="UpdateStateByKey"></a>UpdateStateByKey</h3><p>UpdateStateByKey原语用于记录历史记录，有时，我们需要在DStream中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey()为我们提供了对一个状态变量的访问，用于键值对形式的DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。</p>
<p>updateStateByKey() 的结果会是一个新的DStream，其内部的RDD 序列是由每个时间区间对应的(键，状态)对组成的。</p>
<p>updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，需要做下面两步：</p>
<ol>
<li><p>定义状态，状态可以是一个任意的数据类型。</p>
</li>
<li><p>定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。</p>
</li>
</ol>
<p>使用updateStateByKey需要对检查点目录进行配置，会使用检查点来保存状态。</p>
<p>更新版的wordcount</p>
<p>1) 编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DSream_State</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = socketDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 使用有状态操作保存数据 updateStateByKey</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"scp"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordToOneDS.updateStateByKey[<span class="type">Long</span>](</span><br><span class="line">      <span class="comment">// TODO 第一个参数表示相同key的value数据集合</span></span><br><span class="line">      <span class="comment">// TODO 第二个参数表示相同key的缓冲区的数据</span></span><br><span class="line">      (seq: <span class="type">Seq</span>[<span class="type">Int</span>], opt: <span class="type">Option</span>[<span class="type">Long</span>]) =&gt; &#123;</span><br><span class="line">        <span class="comment">// TODO 返回值表示更新后的缓冲区的值</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> newValue = opt.getOrElse(<span class="number">0</span>L) + seq.sum</span><br><span class="line">        <span class="type">Option</span>(newValue)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2) 启动程序并向9999端口发送数据</p>
<blockquote>
<p>nc -lk 9999</p>
<p>Hello World</p>
<p>Hello Scala</p>
</blockquote>
<p>3) 结果展示</p>
<blockquote>
<p>-——————————————</p>
<p>Time: 1504685175000 ms</p>
<p>-——————————————</p>
<p>-——————————————</p>
<p>Time: 1504685181000 ms</p>
<p>-——————————————</p>
<p>(shi,1)</p>
<p>(shui,1)</p>
<p>(ni,1)</p>
<p>-——————————————</p>
<p>Time: 1504685187000 ms</p>
<p>-——————————————</p>
<p>(shi,1)</p>
<p>(ma,1)</p>
<p>(hao,1)</p>
<p>(shui,1)</p>
</blockquote>
<h3 id="WindowOperations"><a href="#WindowOperations" class="headerlink" title="WindowOperations"></a>WindowOperations</h3><p>Window Operations可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。</p>
<p>窗口时长：计算内容的时间范围；</p>
<p>滑动步长：隔多久触发一次计算。</p>
<p><font color="red"><strong>注意：这两者都必须为采集周期大小的整数倍。</strong></font></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WorldCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Split each line into words</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">// Count each word in each batch</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b),<span class="type">Seconds</span>(<span class="number">12</span>), <span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line"></span><br><span class="line">  wordCounts.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.start()       <span class="comment">// Start the computation</span></span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h4><p><img src="/2020/03/11/SparkStreaming/image-20200914120823766.png" alt="image-20200914120823766"></p>
<h4 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h4><p><img src="/2020/03/11/SparkStreaming/image-20200914120922340.png" alt="image-20200914120922340"></p>
<p>关于Window的操作还有如下方法：</p>
<p>（1）window(windowLength, slideInterval): 基于对源DStream窗化的批次进行计算返回一个新的Dstream；</p>
<p>（2）countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//countByWindow</span></span><br><span class="line">ssc.checkpoint(<span class="string">"scp"</span>)</span><br><span class="line"><span class="keyword">val</span> countDS: <span class="type">DStream</span>[<span class="type">Long</span>] = socketDS.countByWindow(<span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">print(countDS)</span><br></pre></td></tr></table></figure>

<p>（3）reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；</p>
<p>（4）reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的DStream上调用此函数，会返回一个新(K,V)对的DStream，此处通过对滑动窗口中批次数据使用reduce函数来整合每个key的value值。(func 两两聚合)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//reduceByKeyAndWindow</span></span><br><span class="line"><span class="keyword">val</span> wordToOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDS.flatMap(_.split(<span class="string">" "</span>)).map((_ , <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> reduceByKeyAndWindowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKeyAndWindow(</span><br><span class="line">  (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x + y, <span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">3</span>)</span><br><span class="line">)</span><br><span class="line">reduceByKeyAndWindowDS.print()</span><br></pre></td></tr></table></figure>

<p>（5）reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的reduce值都是通过用前一个窗的reduce值来递增计算。通过reduce进入到滑动窗口数据并”反向reduce”离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys的“加”“减”计数。<strong>通过前边介绍可以想到，这个函数只适用于”可逆的reduce函数”，也就是这些reduce函数有相应的”反reduce”函数(以参数invFunc形式传入)。</strong>如前述函数，reduce任务的数量通过可选参数来配置。</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming15.jpg" alt="SparkStreaming15"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map(logEntry =&gt; (logEntry.getIpAddress(), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> ipCountDStream = ipDStream.reduceByKeyAndWindow(</span><br><span class="line"> &#123;(x, y) =&gt; x + y&#125;,</span><br><span class="line"> &#123;(x, y) =&gt; x - y&#125;,</span><br><span class="line"> <span class="type">Seconds</span>(<span class="number">30</span>),</span><br><span class="line"> <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>


<p> //加上新进入窗口的批次中的元素 //移除离开窗口的老批次中的元素 //窗口时长// 滑动步长</p>
<p>countByWindow()和countByValueAndWindow()作为对数据进行计数操作的简写。countByWindow()返回一个表示每个窗口中元素个数的DStream，而countByValueAndWindow()返回的DStream则包含窗口中每个值的个数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map&#123;entry =&gt; entry.getIpAddress()&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ipAddressRequestCount = ipDStream.countByValueAndWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>)) </span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> requestCount = accessLogsDStream.countByWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>



<h1 id="DStream输出"><a href="#DStream输出" class="headerlink" title="DStream输出"></a>DStream输出</h1><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与RDD中的惰性求值类似，如果一个DStream及其派生出的DStream都没有被执行输出操作，那么这些DStream就都不会被求值。如果StreamingContext中没有设定输出操作，整个context就都不会启动。</p>
<p>输出操作如下：</p>
<p><font color="blue">print()</font>：在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫print()。</p>
<p><font color="blue">saveAsTextFiles(prefix, [suffix])</font>：以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”。</p>
<p><font color="blue">saveAsObjectFiles(prefix, [suffix])</font>：以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles . 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python中目前不可用。</p>
<p><font color="blue">saveAsHadoopFiles(prefix, [suffix])</font>：将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”。Python API 中目前不可用。</p>
<p><font color="blue">foreachRDD(func)</font>：这是最通用的输出操作，即将函数 func 用于产生于 stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">DriverManager</span>, <span class="type">PreparedStatement</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DStream_Output</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">//将数据保存到mysql数据库中</span></span><br><span class="line">    socketDS.foreachRDD(</span><br><span class="line">      rdd =&gt;&#123;</span><br><span class="line">        rdd.foreach(</span><br><span class="line">          data =&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = data.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> id: <span class="type">Int</span> = datas(<span class="number">0</span>).toInt</span><br><span class="line">            <span class="keyword">val</span> name: <span class="type">String</span> = datas(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">val</span> age: <span class="type">Int</span> = datas(<span class="number">0</span>).toInt</span><br><span class="line"></span><br><span class="line">            <span class="comment">//TODO 加载数据库驱动</span></span><br><span class="line">            <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">            <span class="comment">// TODO 建立链接和操作对象</span></span><br><span class="line">            <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(</span><br><span class="line">              <span class="string">"jdbc:mysql://3306/studenttest"</span>,</span><br><span class="line">              <span class="string">"root"</span>,</span><br><span class="line">              <span class="string">"000000"</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">val</span> sql = <span class="string">"insert into student(id,name,age) values (?,?,?)"</span></span><br><span class="line">            <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = conn.prepareStatement(sql)</span><br><span class="line">            statement.setInt(<span class="number">1</span>,id)</span><br><span class="line">            statement.setString(<span class="number">2</span>,name)</span><br><span class="line">            statement.setInt(<span class="number">3</span>,age)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// TODO 操作数据</span></span><br><span class="line">            statement.executeUpdate()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// TODO 关闭连接</span></span><br><span class="line">            statement.close()</span><br><span class="line">            conn.close()</span><br><span class="line"></span><br><span class="line">          &#125;)</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong><font color="red">注：但是在真实场景中数据传播速度快，传递量大，所以上述代码并不适合用在实际操作中。</font></strong></p>
<p><strong><font color="red">原因：数据来一次建立一次链接，数据库链接创建太多，显然是不合理的。在数据量巨大的情况下用连接池也是不合理的，处理不过来。</font></strong></p>
<p>通用的输出操作foreachRDD()，它用来对DStream中的RDD运行任意计算。这和transform() 有些类似，都可以让我们访问任意RDD。在foreachRDD()中，可以重用我们在Spark中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如MySQL的外部数据库中。 </p>
<p>注意：</p>
<p>1) 连接不能写在driver层面（序列化）</p>
<p>2) 如果写在foreach则每个RDD中的每一条数据都创建，得不偿失；</p>
<p>3) 增加foreachPartition，在分区创建（获取）。</p>
<p>rdd.foreachPartition()：以分区为单位进行遍历，不需要返回</p>
<p>rdd.foreachPartitions()：以分区为单位进行转换，需要返回</p>
<h1 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h1><p>流式任务需要7*24小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。</p>
<p>使用外部文件系统来控制内部程序关闭。</p>
<p>MonitorStop</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.net.<span class="type">URI</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FileSystem</span>, <span class="type">Path</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">StreamingContext</span>, <span class="type">StreamingContextState</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MonitorStop</span>(<span class="params">ssc: <span class="type">StreamingContext</span></span>) <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fs: <span class="type">FileSystem</span> = <span class="type">FileSystem</span>.get(<span class="keyword">new</span> <span class="type">URI</span>(<span class="string">"hdfs://linux1:9000"</span>), <span class="keyword">new</span> <span class="type">Configuration</span>(), <span class="string">"red"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//优雅关闭判断条件</span></span><br><span class="line"><span class="comment">//stop方法不能放在driver的主线程中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//一般标志不在Driver端，在第三方软件中 eg:redis/zk/mysql/hdfs</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span></span><br><span class="line"></span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">catch</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">InterruptedException</span> =&gt;</span><br><span class="line"></span><br><span class="line">     e.printStackTrace()</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//获取当前线程状态</span></span><br><span class="line">   <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = ssc.getState</span><br><span class="line"> </span><br><span class="line"><span class="comment">// TODO 设置标记，让当前关闭线程可以访问，可以动态改变状态</span></span><br><span class="line">   <span class="keyword">val</span> bool: <span class="type">Boolean</span> = fs.exists(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">"hdfs://linux1:9000/stopSpark"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//为了防止有新线程了之后SparkStreaming直接关闭，所以应该加一个判断条件，</span></span><br><span class="line"><span class="comment">// 并且循环判断，避免判断一次之后不再判断</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (bool) &#123;</span><br><span class="line"><span class="comment">//判断当前状态，如果当前不是激活状态的话根本不用关闭</span></span><br><span class="line">    <span class="keyword">if</span> (state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span>) &#123;</span><br><span class="line"></span><br><span class="line">     ssc.stop(stopSparkContext = <span class="literal">true</span>, stopGracefully = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">     <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>SparkTest</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">createSSC</span></span>(): _root_.org.apache.spark.streaming.<span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> update: (<span class="type">Seq</span>[<span class="type">Int</span>], <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; <span class="type">Some</span>[<span class="type">Int</span>] = (values: <span class="type">Seq</span>[<span class="type">Int</span>], status: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="comment">//当前批次内容的计算</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> sum: <span class="type">Int</span> = values.sum</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="comment">//取出状态信息中上一次状态</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> lastStatu: <span class="type">Int</span> = status.getOrElse(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   <span class="type">Some</span>(sum + lastStatu)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[4]"</span>).setAppName(<span class="string">"SparkTest"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//****设置优雅的关闭*****</span></span><br><span class="line"></span><br><span class="line">  sparkConf.set(<span class="string">"spark.streaming.stopGracefullyOnShutdown"</span>, <span class="string">"true"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> line: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> word: <span class="type">DStream</span>[<span class="type">String</span>] = line.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordAndOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = word.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordAndCount: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.updateStateByKey(update)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  wordAndCount.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck"</span>, () =&gt; createSSC())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">MonitorStop</span>(ssc)).start()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>优雅关闭是要判断当前有没有数据没有处理完，如果有，先把当前数据处理完再关闭。</p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2020/03/11/SparkStreaming/">SparkStreaming</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">Snow Monster</a></p>
        <p><span>发布时间:</span>2020-03-11, 16:45:00</p>
        <p><span>最后更新:</span>2020-09-14, 17:52:21</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2020/03/11/SparkStreaming/" title="SparkStreaming">http://www.red0819.top/2020/03/11/SparkStreaming/</a>
            <span class="copy-path" data-clipboard-text="原文: http://www.red0819.top/2020/03/11/SparkStreaming/　　作者: Snow Monster" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2020/08/26/Sockecket%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/">
                    Socket入门介绍
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2020/02/23/Redis-Jedis/">
                    Redis和Jedis
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#概述"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming是什么"><span class="toc-number">1.1.</span> <span class="toc-text">Spark Streaming是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming的特点"><span class="toc-number">1.2.</span> <span class="toc-text">Spark Streaming的特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming架构"><span class="toc-number">1.3.</span> <span class="toc-text">Spark Streaming架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#架构图"><span class="toc-number">1.3.1.</span> <span class="toc-text">架构图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#背压机制"><span class="toc-number">1.3.2.</span> <span class="toc-text">背压机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-1-5以前版本"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">Spark 1.5以前版本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-1-5以后版本"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">Spark 1.5以后版本</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dstream入门"><span class="toc-number">2.</span> <span class="toc-text">Dstream入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#WordCount案例实操"><span class="toc-number">2.1.</span> <span class="toc-text">WordCount案例实操</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WordCount解析"><span class="toc-number">2.2.</span> <span class="toc-text">WordCount解析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DStream创建"><span class="toc-number">3.</span> <span class="toc-text">DStream创建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD队列"><span class="toc-number">3.1.</span> <span class="toc-text">RDD队列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Queue"><span class="toc-number">3.1.1.</span> <span class="toc-text">Queue()</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#用法及说明"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">用法及说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#案例实操"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#file"><span class="toc-number">3.1.2.</span> <span class="toc-text">file()</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#用法及说明-1"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">用法及说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#案例实操-1"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自定义数据源"><span class="toc-number">3.2.</span> <span class="toc-text">自定义数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#用法及说明-2"><span class="toc-number">3.2.1.</span> <span class="toc-text">用法及说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#案例实操-2"><span class="toc-number">3.2.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka数据源"><span class="toc-number">3.3.</span> <span class="toc-text">Kafka数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#版本选型"><span class="toc-number">3.3.1.</span> <span class="toc-text">版本选型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-0-8-Receiver模式"><span class="toc-number">3.3.2.</span> <span class="toc-text">Kafka 0-8 Receiver模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-0-8-Direct模式"><span class="toc-number">3.3.3.</span> <span class="toc-text">Kafka 0-8 Direct模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#自动维护offset"><span class="toc-number">3.3.3.1.</span> <span class="toc-text">自动维护offset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#手动维护offset"><span class="toc-number">3.3.3.2.</span> <span class="toc-text">手动维护offset</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-0-10-Direct模式"><span class="toc-number">3.3.4.</span> <span class="toc-text">Kafka 0-10 Direct模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消费Kafka数据模式总结"><span class="toc-number">3.3.5.</span> <span class="toc-text">消费Kafka数据模式总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0-8"><span class="toc-number">3.3.5.1.</span> <span class="toc-text">0-8</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#ReceiverAPI"><span class="toc-number">3.3.5.1.1.</span> <span class="toc-text">ReceiverAPI</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#DirectAPI"><span class="toc-number">3.3.5.1.2.</span> <span class="toc-text">DirectAPI</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0-10-DirectAPI"><span class="toc-number">3.3.5.2.</span> <span class="toc-text">0-10 DirectAPI</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DStream转换"><span class="toc-number">4.</span> <span class="toc-text">DStream转换</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#无状态转化操作"><span class="toc-number">4.1.</span> <span class="toc-text">无状态转化操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transform"><span class="toc-number">4.1.1.</span> <span class="toc-text">Transform</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#join"><span class="toc-number">4.1.2.</span> <span class="toc-text">join</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#有状态转化操作"><span class="toc-number">4.2.</span> <span class="toc-text">有状态转化操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#UpdateStateByKey"><span class="toc-number">4.2.1.</span> <span class="toc-text">UpdateStateByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WindowOperations"><span class="toc-number">4.2.2.</span> <span class="toc-text">WindowOperations</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#窗口"><span class="toc-number">4.2.2.1.</span> <span class="toc-text">窗口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#滚动窗口"><span class="toc-number">4.2.2.2.</span> <span class="toc-text">滚动窗口</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DStream输出"><span class="toc-number">5.</span> <span class="toc-text">DStream输出</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#优雅关闭"><span class="toc-number">6.</span> <span class="toc-text">优雅关闭</span></a></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"SparkStreaming　| Snow MonsterBlog　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2020/08/26/Sockecket%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/" title="上一篇: Socket入门介绍">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2020/02/23/Redis-Jedis/" title="下一篇: Redis和Jedis">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/09/14/test1/">test1</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/12/%E4%B8%BB%E6%B5%81ETL%E6%B8%85%E6%B4%97%E5%B7%A5%E5%85%B7/">主流ETL清洗工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/12/OLAP%E5%92%8COLTP/">OLAP和OLTP</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/12/%E4%B8%BB%E6%B5%81OLAP%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93/">主流OLAP系统对比总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/09/%E5%9F%BA%E7%A1%80SQL%E9%A2%98%EF%BC%88%E6%9D%A5%E8%87%AA%E7%89%9B%E5%AE%A2%E7%BD%91%EF%BC%89/">基础SQL题（来自牛客网）</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/06/Oozie/">Oozie</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/06/my-test-blog/">my test blog</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/Sockecket%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/">Socket入门介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/11/SparkStreaming/">SparkStreaming</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/23/Redis-Jedis/">Redis和Jedis</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/15/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/">Redis持久化机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/05/Redis%E5%9F%BA%E7%A1%80/">Redis基础</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/Hive%E5%85%A5%E9%97%A8/">Hive入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/03/testfile/">testfile</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2017-2020 Snow Monster
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = /background/taizichangqin.jpg;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
             tags: ".article-tag a", 
             categories: ".article-category a, a.tag-list-link", 
             articleNav: "#article-nav a, #post-nav-button a", 
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
             menu: ".header-menu a", 
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>