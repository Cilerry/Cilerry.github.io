<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hive入门</title>
    <url>/bigdata/Hive%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="HIVE入门"><a href="#HIVE入门" class="headerlink" title="HIVE入门"></a>HIVE入门</h1><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计。</p>
<p>Hive是基于Hadoop的一个<strong>数据仓库工具</strong>，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p>
<p><strong>本质是：将HQL转化成MapReduce程序</strong></p>
<p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps1.png" alt="wps1"></p>
<p>1）Hive处理的数据存储在HDFS</p>
<p>2）Hive分析数据底层的实现是MapReduce</p>
<p>3）执行程序运行在Yarn上</p>
<h2 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>1) 操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</p>
<p>2) 避免了去写MapReduce，减少开发人员的学习成本。</p>
<p>3) Hive的<strong>执行延迟比较高</strong>，因此Hive常用于数据分析，对实时性要求不高的场合。</p>
<p>4) Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</p>
<p>5) Hive<strong>支持用户自定义函数</strong>，用户可以根据自己的需求来实现自己的函数。</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1．Hive的HQL表达能力有限</p>
<p>（1）迭代式算法无法表达</p>
<p>（2）数据挖掘方面不擅长</p>
<p>2．Hive的效率比较低</p>
<p>（1）Hive自动生成的MapReduce作业，通常情况下不够智能化</p>
<p>（2）Hive调优比较困难，粒度较粗</p>
<h2 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h2><p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps2.png" alt="wps2"></p>
<p>1．用户接口：Client</p>
<p>CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</p>
<p>2．元数据：Metastore</p>
<p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p>
<p><strong>默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</strong></p>
<p>3．Hadoop</p>
<p>使用HDFS进行存储，使用MapReduce进行计算。</p>
<p>4．驱动器：Driver</p>
<p>（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p>
<p>（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p>
<p>（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p>
<p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p>
<p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps3.png" alt="wps3"></p>
<p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p>
<h2 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h2><p>由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p>
<h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p>
<h3 id="数据存储位置"><a href="#数据存储位置" class="headerlink" title="数据存储位置"></a>数据存储位置</h3><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p>
<h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><p>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，<strong>Hive中不建议对数据的改写</strong>，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p>
<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。<u>Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。</u>由于 MapReduce 的引入，<u>Hive 可以并行访问数据</u>，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。</p>
<h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。</p>
<p>数据库通常有自己的执行引擎。</p>
<h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><p>1）Hive执行延迟高</p>
<p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。</p>
<p>另一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。</p>
<p>2）数据库的执行延迟较低（相对而言）</p>
<p>数据规模较小时提现。</p>
<p>当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p>
<h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle在理论上的扩展能力也只有100台左右。</p>
<h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p>
<h1 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h1><h2 id="Hive安装地址"><a href="#Hive安装地址" class="headerlink" title="Hive安装地址"></a>Hive安装地址</h2><p>1．Hive官网地址</p>
<p><a href="http://hive.apache.org/" target="_blank" rel="noopener">http://hive.apache.org/</a></p>
<p>2．文档查看地址</p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p>
<p>3．下载地址</p>
<p><a href="http://archive.apache.org/dist/hive/" target="_blank" rel="noopener">http://archive.apache.org/dist/hive/</a></p>
<p>4．github地址</p>
<p><a href="https://github.com/apache/hive" target="_blank" rel="noopener">https://github.com/apache/hive</a></p>
<h2 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h2><p>1．Hive安装及配置</p>
<p>（1）把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p>
<p>（2）解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure>

<p>（3）修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 module]$ mv apache-hive-1.2.1-bin&#x2F; hive</span><br></pre></td></tr></table></figure>

<p>（4）修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure>

<p>​    （5）配置hive-env.sh文件</p>
<p>​    （a）配置HADOOP_HOME路径</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure>

<p>​    （b）配置HIVE_CONF_DIR路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure>

<p>2．Hadoop集群配置</p>
<p>（1）必须启动hdfs和yarn</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hadoop-2.7.2]$ sbin&#x2F;start-dfs.sh</span><br><span class="line"></span><br><span class="line">[red@hadoop103 hadoop-2.7.2]$ sbin&#x2F;start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>（2）在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hadoop-2.7.2]$ bin&#x2F;hadoop fs -mkdir &#x2F;tmp</span><br><span class="line"></span><br><span class="line">[red@hadoop102 hadoop-2.7.2]$ bin&#x2F;hadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse</span><br><span class="line"></span><br><span class="line">[red@hadoop102 hadoop-2.7.2]$ bin&#x2F;hadoop fs -chmod g+w &#x2F;tmp</span><br><span class="line"></span><br><span class="line">[red@hadoop102 hadoop-2.7.2]$ bin&#x2F;hadoop fs -chmod g+w &#x2F;user&#x2F;hive&#x2F;warehouse</span><br></pre></td></tr></table></figure>

<p>3．Hive基本操作</p>
<p>（1）启动hive</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive</span><br></pre></td></tr></table></figure>

<p>（2）查看数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>

<p>（3）打开默认数据库</p>
<p>hive&gt; use default;</p>
<p>（4）显示default数据库中的表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure>

<p>（5）创建一张表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure>

<p>（6）显示数据库中有几张表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure>

<p>（7）查看表的结构</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure>

<p>（8）向表中插入数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; insert into student values(1000,&quot;ss&quot;);</span><br></pre></td></tr></table></figure>

<p>（9）查询表中数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure>

<p>（10）退出hive</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure>



<h2 id="将本地文件导入Hive案例"><a href="#将本地文件导入Hive案例" class="headerlink" title="将本地文件导入Hive案例"></a>将本地文件导入Hive案例</h2><p>需求</p>
<p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。</p>
<p>1．数据准备</p>
<p>在/opt/module/datas这个目录下准备数据</p>
<p>（1）在/opt/module/目录下创建datas</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 module]$ mkdir datas</span><br></pre></td></tr></table></figure>

<p>（2）在/opt/module/datas/目录下创建student.txt文件并添加数据（tab键间隔）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 datas]$ touch student.txt</span><br><span class="line"></span><br><span class="line">[red@hadoop102 datas]$ vi student.txt</span><br></pre></td></tr></table></figure>

<blockquote>
<p>1001    zhangshan</p>
<p>1002    lishi</p>
<p>1003    zhaoliu</p>
</blockquote>
<p>2．Hive实际操作</p>
<p>（1）启动hive</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive</span><br></pre></td></tr></table></figure>

<p>（2）显示数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>

<p>（3）使用default数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure>

<p>（4）显示default数据库中的表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure>

<p>（5）删除已创建的student表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; drop table student;</span><br></pre></td></tr></table></figure>

<p>（6）创建student表, 并声明文件分隔符’\t’</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqlhive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;;</span><br></pre></td></tr></table></figure>

<p>（7）加载/opt/module/datas/student.txt 文件到student数据库表中。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; load data local inpath '/opt/module/datas/student.txt' into table student;</span><br></pre></td></tr></table></figure>

<p>（8）Hive查询结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">1001	zhangshan</span><br><span class="line"></span><br><span class="line">1002	lishi</span><br><span class="line"></span><br><span class="line">1003	zhaoliu</span><br><span class="line"></span><br><span class="line">Time taken: 0.266 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure>

<p>3．遇到的问题</p>
<p>再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> java.lang.RuntimeException: java.lang.RuntimeException:</span><br><span class="line"></span><br><span class="line"> Unable to instantiate</span><br><span class="line"></span><br><span class="line"> org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:<span class="number">522</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:<span class="number">677</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:<span class="number">621</span>)</span><br><span class="line"></span><br><span class="line">   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line"></span><br><span class="line">   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">57</span>)</span><br><span class="line"></span><br><span class="line">   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line"></span><br><span class="line">   at java.lang.reflect.Method.invoke(Method.java:<span class="number">606</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.util.RunJar.run(RunJar.java:<span class="number">221</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.util.RunJar.main(RunJar.java:<span class="number">136</span>)</span><br><span class="line"></span><br><span class="line">Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:<span class="number">1523</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:<span class="number">86</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:<span class="number">132</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:<span class="number">104</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:<span class="number">3005</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:<span class="number">3024</span>)</span><br><span class="line"></span><br><span class="line">   at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:<span class="number">503</span>)</span><br><span class="line"></span><br><span class="line">... <span class="number">8</span> more</span><br></pre></td></tr></table></figure>

<p>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;</p>
<h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><p>（1）检查当前系统是否安装过Mysql</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]$ rpm -qa|grep mysql</span><br><span class="line"></span><br><span class="line"> mysql-libs-5.1.73-7.el6.x86_64 &#x2F;&#x2F;如果存在通过如下命令卸载</span><br><span class="line"></span><br><span class="line">[root@hadoop102 ~]$ rpm -e --nodeps  mysql-libs &#x2F;&#x2F;用此命令卸载Mysql</span><br></pre></td></tr></table></figure>

<p>（2）将MySQL安装包拷贝到/opt/software目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> [root@hadoop102 software]# ll</span><br><span class="line"></span><br><span class="line">总用量 528384</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root 541061120 11月 29 17:56 mysql-5.7.28-1.el6.x86_64.rpm-bundle.tar</span><br></pre></td></tr></table></figure>

<p>（3）解压MySQL安装包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -xf mysql-5.7.28-1.el6.x86_64.rpm-bundle.tar</span><br></pre></td></tr></table></figure>

<p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps4.jpg" alt="wps4"></p>
<p>（4）在安装目录下执行rpm安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 software]$ rpm -ivh mysql-community-common-5.7.28-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@hadoop102 software]$ rpm -ivh mysql-community-libs-5.7.28-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@hadoop102 software]$ rpm -ivh mysql-community-client-5.7.28-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@hadoop102 software]$ rpm -ivh mysql-community-server-5.7.28-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure>

<p>​     注意:按照顺序依次执行</p>
<p>（5）修改/etc/my.cnf文件,在[mysqld]节点下添加如下配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line"> explicit_defaults_for_timestamp&#x3D;true  &#x2F;&#x2F;显示指定默认值为timestamp类型的字段</span><br></pre></td></tr></table></figure>

<p>（6）删除/etc/my.cnf文件中datadir指向的目录下的所有内容:</p>
<p> 查看datadir的值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql</span><br></pre></td></tr></table></figure>

<p> 删除/var/lib/mysql目录下的所有内容:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 mysql]# pwd</span><br><span class="line"></span><br><span class="line">&#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line"></span><br><span class="line">[root@hadoop102 mysql]# rm -rf *   &#x2F;&#x2F;注意执行命令的位置</span><br></pre></td></tr></table></figure>

<p>（7）初始化数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 opt]$ mysqld --initialize --user&#x3D;mysql</span><br></pre></td></tr></table></figure>

<p>（8）查看临时生成的root用户的密码 </p>
<p><img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps5.jpg" alt="wps5"></p>
<p>（9）启动MySQL服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 opt]$ service mysqld start</span><br></pre></td></tr></table></figure>

<p>（10）登录MySQL数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 opt]$ mysql -uroot -p</span><br><span class="line"></span><br><span class="line">Enter password:  输入临时生成的密码</span><br></pre></td></tr></table></figure>

<p> <img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps6.jpg" alt="wps6"></p>
<p> 登录成功.</p>
<p>（11）修改root用户的密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; set password &#x3D; password(&quot;新密码&quot;)</span><br></pre></td></tr></table></figure>

<p>（12）修改root用户支持任意IP连接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; use mysql ;</span><br><span class="line"></span><br><span class="line">	mysql&gt; update user set host&#x3D; ‘%’ where  user &#x3D; ‘root’;</span><br><span class="line"></span><br><span class="line">	mysql&gt; flush privileges ;</span><br></pre></td></tr></table></figure>



<h2 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h2><h3 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h3><p>1．拷贝mysql-connector-java-5.1.37-bin.jar到/opt/module/hive/lib/</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 software]# cp mysql-connector-java-5.1.37-bin.jar &#x2F;opt&#x2F;module&#x2F;hive&#x2F;lib&#x2F;</span><br></pre></td></tr></table></figure>



<h3 id="配置Metastore到MySql"><a href="#配置Metastore到MySql" class="headerlink" title="配置Metastore到MySql"></a>配置Metastore到MySql</h3><p>1．在/opt/module/hive/conf目录下创建一个hive-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 conf]$ touch hive-site.xml</span><br><span class="line"></span><br><span class="line">[red@hadoop102 conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure>

<p>2．根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​     jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true</span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	 <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3．配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群）</p>
<h3 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h3><p>1．先启动MySQL</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 mysql-libs]$ mysql -uroot -p000000</span><br></pre></td></tr></table></figure>

<p>查看有几个数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| Database      |</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| information_schema |</span><br><span class="line"></span><br><span class="line">| mysql       |</span><br><span class="line"></span><br><span class="line">| performance_schema |</span><br><span class="line"></span><br><span class="line">| test        |</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>

<p>2．再次打开多个窗口，分别启动hive</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive</span><br></pre></td></tr></table></figure>

<p>3．启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">	mysql&gt; show databases;</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| Database      |</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| information_schema |</span><br><span class="line"></span><br><span class="line">| metastore      |</span><br><span class="line"></span><br><span class="line">| mysql       |</span><br><span class="line"></span><br><span class="line">| performance_schema |</span><br><span class="line"></span><br><span class="line">| test        |</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>

<h2 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h2><h3 id="启动hiveserver2服务"><a href="#启动hiveserver2服务" class="headerlink" title="启动hiveserver2服务"></a>启动hiveserver2服务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hiveserver2</span><br></pre></td></tr></table></figure>



<h3 id="启动beeline"><a href="#启动beeline" class="headerlink" title="启动beeline"></a>启动beeline</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;beeline</span><br><span class="line"></span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"></span><br><span class="line">beeline&gt;</span><br></pre></td></tr></table></figure>



<h3 id="连接hiveserver2"><a href="#连接hiveserver2" class="headerlink" title="连接hiveserver2"></a>连接hiveserver2</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;hadoop102:10000（回车）</span><br><span class="line"></span><br><span class="line">Connecting to jdbc:hive2:&#x2F;&#x2F;hadoop102:10000</span><br><span class="line"></span><br><span class="line">Enter username for jdbc:hive2:&#x2F;&#x2F;hadoop102:10000: red（回车）</span><br><span class="line"></span><br><span class="line">Enter password for jdbc:hive2:&#x2F;&#x2F;hadoop102:10000: （直接回车）</span><br><span class="line"></span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line"></span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line"></span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;hadoop102:10000&gt; show databases;</span><br><span class="line"></span><br><span class="line">+----------------+--+</span><br><span class="line"></span><br><span class="line">| database_name  |</span><br><span class="line"></span><br><span class="line">+----------------+--+</span><br><span class="line"></span><br><span class="line">| default     |</span><br><span class="line"></span><br><span class="line">| hive_db2    |</span><br><span class="line"></span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure>



<h2 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive -helpusage: hive -d,--define &lt;key&#x3D;value&gt;      Variable subsitution to apply to hive                   commands. e.g. -d A&#x3D;B or --define A&#x3D;B  --database &lt;databasename&gt;   Specify the database to use -e &lt;quoted-query-string&gt;     SQL from command line -f &lt;filename&gt;           SQL from files -H,--help             Print help information  --hiveconf &lt;property&#x3D;value&gt;  Use value for given property  --hivevar &lt;key&#x3D;value&gt;     Variable subsitution to apply to hive                  commands. e.g. --hivevar A&#x3D;B -i &lt;filename&gt;           Initialization SQL file -S,--silent            Silent mode in interactive shell -v,--verbose           Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure>

<p>1．“-e”不进入hive的交互窗口执行sql语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure>

<p>2．“-f”执行脚本中sql语句</p>
<p>​    （1）在/opt/module/datas目录下创建hivef.sql文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure>

<p>​        文件中写入正确的sql语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select *from student;</span><br></pre></td></tr></table></figure>

<p>​    （2）执行文件中的sql语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive -f &#x2F;opt&#x2F;module&#x2F;datas&#x2F;hivef.sql</span><br></pre></td></tr></table></figure>

<p>（3）执行文件中的sql语句并将结果写入文件中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hive]$ bin&#x2F;hive -f &#x2F;opt&#x2F;module&#x2F;datas&#x2F;hivef.sql  &gt; &#x2F;opt&#x2F;module&#x2F;datas&#x2F;hive_result.txt</span><br></pre></td></tr></table></figure>



<h2 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h2><p>1．退出hive窗口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;exit;</span><br><span class="line"></span><br><span class="line">hive(default)&gt;quit;</span><br></pre></td></tr></table></figure>

<p>在新版的hive中没区别了，在以前的版本是有的：</p>
<p>exit:先隐性提交数据，再退出；</p>
<p>quit:不提交数据，退出；</p>
<p>2．在hive cli命令窗口中如何查看hdfs文件系统</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;dfs -ls &#x2F;;</span><br></pre></td></tr></table></figure>

<p>3．在hive cli命令窗口中如何查看本地文件系统</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;! ls &#x2F;opt&#x2F;module&#x2F;datas;</span><br></pre></td></tr></table></figure>

<p>4．查看在hive中输入的所有历史命令</p>
<p>​    （1）进入到当前用户的根目录/root或/home/red</p>
<p>​    （2）查看. hivehistory文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure>



<h2 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h2><h3 id="Hive数据仓库位置配置"><a href="#Hive数据仓库位置配置" class="headerlink" title="Hive数据仓库位置配置"></a>Hive数据仓库位置配置</h3><p>​    1）Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。</p>
<p>​    2）在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。</p>
<p>​    3）修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>配置同组用户有执行权限</p>
<p>bin/hdfs dfs -chmod g+w /user/hive/warehouse</p>
<h3 id="查询后信息显示配置"><a href="#查询后信息显示配置" class="headerlink" title="查询后信息显示配置"></a>查询后信息显示配置</h3><p>1）在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​    2）重新启动hive，对比配置前后差异。</p>
<p>（1）配置前，如图6-2所示</p>
<p> <img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps7.jpg" alt="wps7"></p>
<p>图6-2 配置前</p>
<p>（2）配置后，如图6-3所示</p>
<p> <img src="/2018/12/22/Hive%E5%85%A5%E9%97%A8/wps8.jpg" alt="wps8"></p>
<p>图6-3 配置后</p>
<h3 id="Hive运行日志信息配置"><a href="#Hive运行日志信息配置" class="headerlink" title="Hive运行日志信息配置"></a>Hive运行日志信息配置</h3><p>1．Hive的log默认存放在/tmp/red/hive.log目录下（当前用户名下）</p>
<p>2．修改hive的log存放日志到/opt/module/hive/logs</p>
<p>​    （1）修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为</p>
<blockquote>
<p>hive-log4j.properties</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 conf]$ pwd</span><br><span class="line"></span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf</span><br><span class="line"></span><br><span class="line">[red@hadoop102 conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure>

<p>​    （2）在hive-log4j.properties文件中修改log存放位置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive.log.dir&#x3D;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs</span><br></pre></td></tr></table></figure>



<h3 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h3><p>1．查看当前所有的配置信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt;set;</span><br></pre></td></tr></table></figure>

<p>2．参数的配置三种方式</p>
<p>​    （1）配置文件方式</p>
<p>默认配置文件：hive-default.xml </p>
<p>用户自定义配置文件：hive-site.xml</p>
<p>​    注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p>
<p>（2）命令行参数方式</p>
<p>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop103 hive]$ bin&#x2F;hive -hiveconf mapred.reduce.tasks&#x3D;10;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效</p>
<p>查看参数设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>

<p>（3）参数声明方式</p>
<p>可以在HQL中使用SET关键字设定参数</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks&#x3D;100;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效。</p>
<p>查看参数设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>

<p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p>
<h1 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><p>表6-1</p>
<table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE  FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p>
<h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><p>表6-2</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()</td>
</tr>
</tbody></table>
<p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<p>案例实操</p>
<p>1） 假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p>
<blockquote>
<p>{  “name”: “songsong”,  “friends”: [“bingbing” , “lili”] ,    </p>
<p>//列表Array,   “children”: {            </p>
<p>//键值Map,    “xiao song”: 18 ,    “xiaoxiao song”: 19 </p>
<p>}  “address”: {            </p>
<p>//结构Struct,   </p>
<p>“street”: “hui long guan” ,    “city”: “beijing”   }}</p>
</blockquote>
<p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。 </p>
<p>创建本地测试文件test.txt</p>
<blockquote>
<p>songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</p>
</blockquote>
<p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p>
<p>3）Hive上创建测试表test</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table test(name string,friends array&lt;string&gt;,children map&lt;string, int&gt;,address struct&lt;street:string, city:string&gt;)row format delimited fields terminated by &#39;,&#39;collection items terminated by &#39;_&#39;map keys terminated by &#39;:&#39;lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure>

<p>字段解释：</p>
<p>row format delimited fields terminated by ‘,’  – 列分隔符</p>
<p>collection items terminated by ‘_’  –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p>
<p>map keys terminated by ‘:’                – MAP中的key与value的分隔符</p>
<p>lines terminated by ‘\n’;                    – 行分隔符</p>
<p>4）导入文本数据到测试表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘&#x2F;opt&#x2F;module&#x2F;datas&#x2F;test.txt’into table test</span><br></pre></td></tr></table></figure>

<p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from testwhere name&#x3D;&quot;songsong&quot;;</span><br><span class="line">OK_c0   _c1   citylili   18    beijingTime taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>



<h2 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h2><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p>
<p>1．隐式类型转换规则如下</p>
<p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</p>
<p>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</p>
<p>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。</p>
<p>（4）BOOLEAN类型不可以转换为任何其它的类型。</p>
<p>2．可以使用CAST操作显示进行数据类型转换</p>
<p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Socket入门介绍</title>
    <url>/computernetwork/Sockecket%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>下面进行MS-Windows、HP-Unix网络编程的实践过程中总结出来的一些经验，仅供大家参考。本文所谈到的Socket函数如果没有特别说明，都是指的Windows Socket API。</p>
<h1 id="Socket-API简介"><a href="#Socket-API简介" class="headerlink" title="Socket API简介"></a>Socket API简介</h1><p>Windows Socket是从UNIX Socket继承发展而来，最新的版本是2.2。进行Windows网络编程，需要在程序中包含WINSOCK2.H或MSWSOCK.H，同时需要添加引入库WS2_32. LIB或WSOCK32.LIB。</p>
<p>网络中用一个三元组可以在全局唯一标志一个进程： （协议，本地地址，本地端口号） ，这样一个三元组，叫做一个半相关（half-association），它指定连接的每半部分。 </p>
<h1 id="套接字类型"><a href="#套接字类型" class="headerlink" title="套接字类型"></a>套接字类型</h1><p>TCP/IP的socket提供下列三种类型套接字。 </p>
<h2 id="流式套接字（SOCK-STREAM）"><a href="#流式套接字（SOCK-STREAM）" class="headerlink" title="流式套接字（SOCK_STREAM）"></a>流式套接字（SOCK_STREAM）</h2><p>提供了一个面向连接、可靠的数据传输服务，数据无差错、无重复地发送，且按发送顺序接收。内设流量控制，避免数据流超限；数据被看作是字节流，无长度限制。文件传送协议（FTP）即使用流式套接字。 </p>
<h2 id="数据报式套接字（SOCK-DGRAM）"><a href="#数据报式套接字（SOCK-DGRAM）" class="headerlink" title="数据报式套接字（SOCK_DGRAM）"></a>数据报式套接字（SOCK_DGRAM）</h2><p>提供了一个无连接服务。数据包以独立包形式被发送，不提供无错保证，数据可能丢失或重复，并且接收顺序混乱。网络文件系统（NFS）使用数据报式套接字。 </p>
<h2 id="原始式套接字（SOCK-RAW）"><a href="#原始式套接字（SOCK-RAW）" class="headerlink" title="原始式套接字（SOCK_RAW）"></a>原始式套接字（SOCK_RAW）</h2><p>该接口允许对较低层协议，如IP、ICMP直接访问。常用于检验新的协议实现或访问现有服务中配置的新设备。 </p>
<h3 id="一、WSAStartup函数"><a href="#一、WSAStartup函数" class="headerlink" title="一、WSAStartup函数"></a>一、WSAStartup函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">WSAStartup</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">WORD wVersionRequested,         <span class="comment">//使用的Socket版本</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">LPWSADATA lpWSAData         <span class="comment">//返回请求的Socket的版本信息</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>

<p>使用Socket的程序在使用Socket之前必须调用WSAStartup函数。该函数的第一个参数指明程序请求使用的Socket版本，其中高位字节指明副版本、低位字节指明主版本；操作系统利用第二个参数返回请求的Socket的版本信息。当一个应用程序调用WSAStartup函数时，操作系统根据请求的Socket版本来搜索相应的Socket库，然后绑定找到的Socket库到该应用程序中。以后应用程序就可以调用所请求的Socket库中的其它Socket函数了。该函数执行成功后返回0。</p>
<p>例：假如一个程序要使用2.1版本的Socket,那么程序代码如下</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">wVersionRequested = MAKEWORD( <span class="number">2</span>, <span class="number">1</span> ); </span><br><span class="line"></span><br><span class="line">err = WSAStartup( wVersionRequested, &amp;wsaData );</span><br></pre></td></tr></table></figure>



<h3 id="二、WSACleanup函数"><a href="#二、WSACleanup函数" class="headerlink" title="二、WSACleanup函数"></a>二、WSACleanup函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">WSACleanup</span> <span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure>

<p>应用程序在完成对请求的Socket库的使用后，要调用WSACleanup函数来解除与Socket库的绑定并且释放Socket库所占用的系统资源。 </p>
<h3 id="三、socket函数"><a href="#三、socket函数" class="headerlink" title="三、socket函数"></a>三、socket函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">SOCKET <span class="title">socket</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> af,          <span class="comment">//指定应用程序使用的通信协议的协议族，对于TCP/IP协议族，该参数置AF_INET; UNIX系统支持的地址族有：AF_UNIX、AF_INET、AF_NS等，而DOS、WINDOWS中仅支持AF_INET，它是网际网区域。</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> type,         <span class="comment">//套接字类型</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> protocol        <span class="comment">//应用程序所使用的通信协议</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>

<p>应用程序调用socket函数来创建一个能够进行网络通信的套接字。第一个参数指定应用程序使用的通信协议的协议族，对于TCP/IP协议族，该参数置PF_INET;第二个参数指定要创建的套接字类型，流套接字类型为SOCK_STREAM、数据报套接字类型为SOCK_DGRAM；第三个参数指定应用程序所使用的通信协议。该函数如果调用成功就返回新创建的套接字的描述符，如果失败就返回INVALID_SOCKET。套接字描述符是一个整数类型的值。每个进程的进程空间里都有一个套接字描述符表，该表中存放着套接字描述符和套接字数据结构的对应关系。该表中有一个字段存放新创建的套接字的描述符，另一个字段存放套接字数据结构的地址，因此根据套接字描述符就可以找到其对应的套接字数据结构。每个进程在自己的进程空间里都有一个套接字描述符表但是套接字数据结构都是在操作系统的内核缓冲里。下面是一个创建流套接字的例子：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">protoent</span> *<span class="title">ppe</span>;</span> </span><br><span class="line"></span><br><span class="line">ppe=getprotobyname(<span class="string">"tcp"</span>); </span><br><span class="line"></span><br><span class="line">SOCKET ListenSocket=socket(PF_INET,SOCK_STREAM,ppe-&gt;p_proto);</span><br></pre></td></tr></table></figure>



<h3 id="四、closesocket函数"><a href="#四、closesocket函数" class="headerlink" title="四、closesocket函数"></a>四、closesocket函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">closesocket</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>

<p>closesocket函数用来关闭一个描述符为s的套接字。由于每个进程中都有一个套接字描述符表，表中的每个套接字描述符都对应了一个位于操作系统缓冲区中的套接字数据结构，因此有可能有几个套接字描述符指向同一个套接字数据结构。套接字数据结构中专门有一个字段存放该结构的被引用次数，即有多少个套接字描述符指向该结构。当调用closesocket函数时，操作系统先检查套接字数据结构中的该字段的值，如果为1，就表明只有一个套接字描述符指向它，因此操作系统就先把s在套接字描述符表中对应的那条表项清除，并且释放s对应的套接字数据结构；如果该字段大于1，那么操作系统仅仅清除s在套接字描述符表中的对应表项，并且把s对应的套接字数据结构的引用次数减1。 </p>
<p>closesocket函数如果执行成功就返回0，否则返回SOCKET_ERROR。 </p>
<h3 id="五、send函数"><a href="#五、send函数" class="headerlink" title="五、send函数"></a>五、send函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">send</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s,                   <span class="comment">//指定发送端套接字描述符</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">const</span> <span class="keyword">char</span> FAR *buf,         <span class="comment">//一个存放应用程序要发送数据的缓冲区</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> len,              <span class="comment">//实际要发送的数据的字节数</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> flags </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>

<p>不论是客户还是服务器应用程序都用send函数来向TCP连接的另一端发送数据。客户程序一般用send函数向服务器发送请求，而服务器则通常用send函数来向客户程序发送应答。该函数的第一个参数指定发送端套接字描述符；第二个参数指明一个存放应用程序要发送数据的缓冲区；第三个参数指明实际要发送的数据的字节数；第四个参数一般置0。这里只描述同步Socket的send函数的执行流程。当调用该函数时，send先比较待发送数据的长度len和套接字s的发送缓冲区的长度，如果len大于s的发送缓冲区的长度，该函数返回SOCKET_ERROR；如果len小于或者等于s的发送缓冲区的长度，那么send先检查协议是否正在发送s的发送缓冲中的数据，如果是就等待协议把数据发送完，如果协议还没有开始发送s的发送缓冲中的数据或者s的发送缓冲中没有数据，那么send就比较s的发送缓冲区的剩余空间和len，如果len大于剩余空间大小send就一直等待协议把s的发送缓冲中的数据发送完，如果len小于剩余空间大小send就仅仅把buf中的数据copy到剩余空间里（注意并不是send把s的发送缓冲中的数据传到连接的另一端的，而是协议传的，send仅仅是把buf中的数据copy到s的发送缓冲区的剩余空间里）。如果send函数copy数据成功，就返回实际copy的字节数，如果send在copy数据时出现错误，那么send就返回SOCKET_ERROR；如果send在等待协议传送数据时网络断开的话，那么send函数也返回SOCKET_ERROR。要注意send函数把buf中的数据成功copy到s的发送缓冲的剩余空间里后它就返回了，但是此时这些数据并不一定马上被传到连接的另一端。如果协议在后续的传送过程中出现网络错误的话，那么下一个Socket函数就会返回SOCKET_ERROR。（每一个除send外的Socket函数在执行的最开始总要先等待套接字的发送缓冲中的数据被协议传送完毕才能继续，如果在等待时出现网络错误，那么该Socket函数就返回SOCKET_ERROR）<br>注意：在Unix系统下，如果send在等待协议传送数据时网络断开的话，调用send的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。 </p>
<h3 id="六、recv函数"><a href="#六、recv函数" class="headerlink" title="六、recv函数"></a>六、recv函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">recv</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">char</span> FAR *buf, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> len, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> flags </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>

<p>不论是客户还是服务器应用程序都用recv函数从TCP连接的另一端接收数据。该函数的第一个参数指定接收端套接字描述符；第二个参数指明一个缓冲区，该缓冲区用来存放recv函数接收到的数据；第三个参数指明buf的长度；第四个参数一般置0。这里只描述同步Socket的recv函数的执行流程。当应用程序调用recv函数时，recv先等待s的发送缓冲中的数据被协议传送完毕，如果协议在传送s的发送缓冲中的数据时出现网络错误，那么recv函数返回SOCKET_ERROR，如果s的发送缓冲中没有数据或者数据被协议成功发送完毕后，recv先检查套接字s的接收缓冲区，如果s接收缓冲区中没有数据或者协议正在接收数据，那么recv就一直等待，只到协议把数据接收完毕。当协议把数据接收完毕，recv函数就把s的接收缓冲中的数据copy到buf中（注意协议接收到的数据可能大于buf的长度，所以在这种情况下要调用几次recv函数才能把s的接收缓冲中的数据copy完。recv函数仅仅是copy数据，真正的接收数据是协议来完成的），recv函数返回其实际copy的字节数。如果recv在copy时出错，那么它返回SOCKET_ERROR；如果recv函数在等待协议接收数据时网络中断了，那么它返回0。 </p>
<p>注意：在Unix系统下，如果recv函数在等待协议接收数据时网络断开了，那么调用recv的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。 </p>
<h3 id="七、bind函数"><a href="#七、bind函数" class="headerlink" title="七、bind函数"></a>七、bind函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bind</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">const</span> struct sockaddr FAR *name, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> namelen </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>

<p>当创建了一个Socket以后，套接字数据结构中有一个默认的IP地址和默认的端口号。一个服务程序必须调用bind函数来给其绑定一个IP地址和一个特定的端口号。客户程序一般不必调用bind函数来为其Socket绑定IP地址和断口号。该函数的第一个参数指定待绑定的Socket描述符；第二个参数指定一个sockaddr结构，该结构是这样定义的： </p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr</span> &#123;</span></span><br><span class="line"></span><br><span class="line">u_short sa_family;            <span class="comment">//地址族 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> sa_data[<span class="number">14</span>];             <span class="comment">// 14字节协议地址</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>sa_family指定地址族，对于TCP/IP协议族的套接字，给其置AF_INET。当对TCP/IP协议族的套接字进行绑定时，我们通常使用另一个地址结构：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> &#123;</span>          <span class="comment">// ("in" 代表 "Internet"。)</span></span><br><span class="line"></span><br><span class="line">short sin_family; </span><br><span class="line"></span><br><span class="line">u_short sin_port;         <span class="comment">// 16位端口号，网络字节顺序</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">in_addr</span> <span class="title">sin_addr</span>;</span>       <span class="comment">// 32位IP地址，网络字节顺序</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> sin_zero[<span class="number">8</span>];          <span class="comment">//保留</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>其中sin_family置AF_INET；sin_port指明端口号；sin_addr结构体中只有一个唯一的字段s_addr，表示IP地址，该字段是一个整数，一般用函数inet_addr（）把字符串形式的IP地址转换成unsigned long型的整数值后再置给s_addr。有的服务器是多宿主机，至少有两个网卡，那么运行在这样的服务器上的服务程序在为其Socket绑定IP地址时可以把htonl(INADDR_ANY)置给s_addr，这样做的好处是不论哪个网段上的客户程序都能与该服务程序通信；如果只给运行在多宿主机上的服务程序的Socket绑定一个固定的IP地址，那么就只有与该IP地址处于同一个网段上的客户程序才能与该服务程序通信。我们用0来填充sin_zero数组，目的是让sockaddr_in结构的大小与sockaddr结构的大小一致。下面是一个bind函数调用的例子： </p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">saddr</span>； </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">saddr</span>.<span class="title">sin_family</span> = <span class="title">AF_INET</span>;</span> </span><br><span class="line"></span><br><span class="line">saddr.sin_port = htons(<span class="number">8888</span>);     <span class="comment">// Host to Network Short</span></span><br><span class="line"></span><br><span class="line">saddr.sin_addr.s_addr = htonl(INADDR_ANY); <span class="comment">//使用htonl将IP地址转换为网络格式，INADDR_ANY自动填上它所运行的机器的 IP 地址</span></span><br><span class="line"></span><br><span class="line">bind(ListenSocket,(struct sockaddr *)&amp;saddr,<span class="keyword">sizeof</span>(saddr))；</span><br></pre></td></tr></table></figure>

<p>注意：不同的计算机存放多字节值的顺序不同，有的机器在起始地址存放低位字节（低价先存），有的存高位字节（高价先存）。为保证数据的正确性，在网络协议中须指定网络字节顺序。TCP/IP协议使用16位整数和32位整数的高价先存格式，它们均含在协议头文件中。</p>
<p>在调用 bind() 的时候，要小心的另一件事情是：不要采用小于 1024的端口号。所有小于1024的端口号都被系统保留！可以选择从1024 到65535的端口(如果它们没有被别的程序使用的话)。</p>
<h3 id="八、listen函数"><a href="#八、listen函数" class="headerlink" title="八、listen函数"></a>八、listen函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">listen</span><span class="params">( SOCKET s, <span class="keyword">int</span> backlog )</span></span>;</span><br></pre></td></tr></table></figure>

<p>服务程序可以调用listen函数使其流套接字s处于监听状态。处于监听状态的流套接字s将维护一个客户连接请求队列，该队列最多容纳backlog个客户连接请求。假如该函数执行成功，则返回0；如果执行失败，则返回SOCKET_ERROR。 </p>
<h3 id="九、accept函数"><a href="#九、accept函数" class="headerlink" title="九、accept函数"></a>九、accept函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">SOCKET <span class="title">accept</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">struct sockaddr FAR *addr,          <span class="comment">//返回新创建的套接字的地址结构</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> FAR *addrlen              <span class="comment">//新创建的套接字的地址结构的长度</span></span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>

<p>服务程序调用accept函数从处于监听状态的流套接字s的客户连接请求队列中取出排在最前的一个客户请求，并且创建一个新的套接字来与客户套接字创建连接通道，如果连接成功，就返回新创建的套接字的描述符，以后与客户套接字交换数据的是新创建的套接字；如果失败就返回INVALID_SOCKET。该函数的第一个参数指定处于监听状态的流套接字；操作系统利用第二个参数来返回新创建的套接字的地址结构；操作系统利用第三个参数来返回新创建的套接字的地址结构的长度。下面是一个调用accept的例子： </p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">ServerSocketAddr</span>;</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> addrlen; </span><br><span class="line"></span><br><span class="line">addrlen=<span class="keyword">sizeof</span>(ServerSocketAddr); </span><br><span class="line"></span><br><span class="line">ServerSocket=accept(ListenSocket,(struct sockaddr *)&amp;ServerSocketAddr,&amp;addrlen);</span><br></pre></td></tr></table></figure>



<h3 id="十、connect函数"><a href="#十、connect函数" class="headerlink" title="十、connect函数"></a>十、connect函数</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">connect</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">SOCKET s, </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">const</span> struct sockaddr FAR *name,</span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> namelen </span></span></span><br><span class="line"><span class="function"><span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>

<p>客户程序调用connect函数来使客户Socket s与监听于name所指定的计算机的特定端口上的服务Socket进行连接。如果连接成功，connect返回0；如果失败则返回SOCKET_ERROR。下面是一个例子：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">daddr</span>;</span> </span><br><span class="line"></span><br><span class="line"><span class="built_in">memset</span>((<span class="keyword">void</span> *)&amp;daddr,<span class="number">0</span>,<span class="keyword">sizeof</span>(daddr)); </span><br><span class="line"></span><br><span class="line">daddr.sin_family=AF_INET; </span><br><span class="line"></span><br><span class="line">daddr.sin_port=htons(<span class="number">8888</span>); </span><br><span class="line"></span><br><span class="line">daddr.sin_addr.s_addr=inet_addr(<span class="string">"133.197.22.4"</span>);  <span class="comment">//函数inet_addr(),将IP地址从 点数格式转换成无符号长整型， inet_addr()返回的地址已经是网络字节格式</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">connect</span>(ClientSocket,(struct sockaddr *)&amp;daddr,<span class="keyword">sizeof</span>(daddr));</span><br></pre></td></tr></table></figure>



<h1 id="实例基本步骤"><a href="#实例基本步骤" class="headerlink" title="实例基本步骤"></a>实例基本步骤</h1><p>设计一个基本的网络服务器有以下几个步骤：</p>
<p>1、初始化Windows Socket</p>
<p>2、创建一个监听的Socket</p>
<p>3、设置服务器地址信息，并将监听端口绑定到这个地址上</p>
<p>4、开始监听</p>
<p>5、接受客户端连接</p>
<p>6、和客户端通信</p>
<p>7、结束服务并清理Windows Socket和相关数据，或者返回第4步</p>
<h1 id="入门代码-C语言版"><a href="#入门代码-C语言版" class="headerlink" title="入门代码(C语言版)"></a>入门代码(C语言版)</h1><p>下面是简单的服务器和客户端源代码。（阻塞模式下的，供初学者理解）</p>
<h3 id="TCPServer"><a href="#TCPServer" class="headerlink" title="TCPServer"></a>TCPServer</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;winsock2.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">  WSADATA        wsaData;</span><br><span class="line"></span><br><span class="line">  SOCKET        ListeningSocket;</span><br><span class="line"></span><br><span class="line">  SOCKET        NewConnection;</span><br><span class="line"></span><br><span class="line">  SOCKADDR_IN      ServerAddr;</span><br><span class="line"></span><br><span class="line">  SOCKADDR_IN      ClientAddr;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>          Port = <span class="number">5150</span>;</span><br><span class="line"></span><br><span class="line">  WSAStartup(MAKEWORD(<span class="number">2</span>,<span class="number">2</span>), &amp;wsaData); <span class="comment">// 初始化Windows Socket 2.2</span></span><br><span class="line"></span><br><span class="line">  ListeningSocket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP); <span class="comment">// 创建一个新的Socket来响应客户端的连接请求</span></span><br><span class="line"></span><br><span class="line">  ServerAddr.sin_family = AF_INET; <span class="comment">// 填写服务器地址信息</span></span><br><span class="line"></span><br><span class="line">  ServerAddr.sin_port = htons(Port);   <span class="comment">// 端口为5150</span></span><br><span class="line"></span><br><span class="line">  ServerAddr.sin_addr.s_addr = htonl(INADDR_ANY); <span class="comment">// IP地址为INADDR_ANY，注意使用htonl将IP地址转换为网络格式</span></span><br><span class="line"></span><br><span class="line">  bind(ListeningSocket, (SOCKADDR *)&amp;ServerAddr, <span class="keyword">sizeof</span>(ServerAddr)); <span class="comment">// 绑定监听端口 listen(ListeningSocket, 5); // 开始监听，指定最大同时连接数为5 </span></span><br><span class="line"></span><br><span class="line">  NewConnection = accept(ListeningSocket, (SOCKADDR *) &amp;ClientAddr,&amp;ClientAddrLen)); <span class="comment">// 接受新的连接</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 新的连接建立后，就可以互相通信了，在这个简单的例子中，我们直接关闭连接，</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 并关闭监听Socket，然后退出应用程序</span></span><br><span class="line"></span><br><span class="line">  closesocket(NewConnection);</span><br><span class="line"></span><br><span class="line">  closesocket(ListeningSocket);</span><br><span class="line"></span><br><span class="line">  WSACleanup();<span class="comment">// 释放Windows Socket DLL的相关资源</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="TCPClient"><a href="#TCPClient" class="headerlink" title="TCPClient"></a>TCPClient</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;winsock2.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">  WSADATA        wsaData;</span><br><span class="line"></span><br><span class="line">  SOCKET        s;</span><br><span class="line"></span><br><span class="line">  SOCKADDR_IN      ServerAddr;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>          Port = <span class="number">5150</span>;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  WSAStartup(MAKEWORD(<span class="number">2</span>,<span class="number">2</span>), &amp;wsaData);  <span class="comment">//初始化Windows Socket 2.2</span></span><br><span class="line"></span><br><span class="line">  s = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);  <span class="comment">// 创建一个新的Socket来连接服务器</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 填写客户端地址信息</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 端口为5150</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 服务器IP地址为"136.149.3.29"，注意使用inet_addr将IP地址转换为网络格式</span></span><br><span class="line"></span><br><span class="line">  ServerAddr.sin_family = AF_INET;</span><br><span class="line"></span><br><span class="line">   ServerAddr.sin_port = htons(Port);   </span><br><span class="line"></span><br><span class="line">   ServerAddr.sin_addr.s_addr = inet_addr(<span class="string">"136.149.3.29"</span>);</span><br><span class="line"></span><br><span class="line">   <span class="built_in">connect</span>(s, (SOCKADDR *) &amp;ServerAddr, <span class="keyword">sizeof</span>(ServerAddr));    <span class="comment">// 向服务器发出连接请求</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 新的连接建立后，就可以互相通信了，在这个简单的例子中，我们直接关闭连接，</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 并关闭监听Socket，然后退出应用程序</span></span><br><span class="line"></span><br><span class="line">   closesocket(s);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 释放Windows Socket DLL的相关资源</span></span><br><span class="line"></span><br><span class="line">   WSACleanup();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title>testfile</title>
    <url>/uncategorized/testfile/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>public class First{</p>
<p>​    public static void main(String[] args){</p>
<p>​        System.out.println(“Hello Hexo”);</p>
<p>​    }</p>
<p>}</p>
]]></content>
      <tags>
        <tag>First_Test</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkStreaming</title>
    <url>/bigdata/SparkStreaming/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Spark Streaming是微批次处理方式，批处理间隔是Spark Streaming是的核心概念和关键参数。</p>
<p>Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行</p>
<h2 id="Spark-Streaming是什么"><a href="#Spark-Streaming是什么" class="headerlink" title="Spark Streaming是什么"></a>Spark Streaming是什么</h2><p>Spark流使得构建可扩展的容错流应用程序变得更加容易。</p>
<p>Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。</p>
<p><img src="/bigdata/SparkStreaming/SparkStreaming%E5%9B%BE.jpg" alt="SparkStreaming图"></p>
<p>和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。<strong>在内部，每个时间区间收到的数据都作为 RDD 存在，而DStream是由这些RDD所组成的序列(因此得名“离散化”)。</strong></p>
<p>离散流反义词就是连续流。</p>
<p><img src="/bigdata/SparkStreaming/SparkStreaming%E5%9B%BE%E8%A7%A3.png" alt="SparkStreaming图解"></p>
<h2 id="Spark-Streaming的特点"><a href="#Spark-Streaming的特点" class="headerlink" title="Spark Streaming的特点"></a>Spark Streaming的特点</h2><p><strong>易用</strong></p>
<p><img src="/bigdata/SparkStreaming/SparkStreaming5.jpg" alt="SparkStreaming5"></p>
<p><strong>容错</strong></p>
<p><img src="/bigdata/SparkStreaming/SparkStreaming6.jpg" alt="SparkStreaming6"></p>
<p> <strong>易整合到Spark体系</strong><img src="/bigdata/SparkStreaming/SparkStreaming7.jpg" alt="SparkStreaming7"></p>
<h2 id="Spark-Streaming架构"><a href="#Spark-Streaming架构" class="headerlink" title="Spark Streaming架构"></a>Spark Streaming架构</h2><p>最基本的架构：底层就是spark-core。数据采集和封装之后传给Driver，Driver拿到相应的RDD，再形成一个一个Task然后传给Executor执行。。</p>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p> 整体架构图</p>
<p><img src="/bigdata/SparkStreaming/SparkStreaming8.jpg" alt="SparkStreaming8"></p>
<p>​                                        spark1.5之前</p>
<p><img src="/bigdata/SparkStreaming/spark1.5%E4%B9%8B%E5%90%8E%E6%9E%B6%E6%9E%84.png" alt="spark1.5之后架构"></p>
<p>​                                       spark1.5之后</p>
<p>SparkStreaming架构图</p>
<p><img src="/bigdata/SparkStreaming/SparkStreaming9.jpg" alt="SparkStreaming9"></p>
<h3 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h3><p>背压(back pressure)机制主要用于解决流处理系统中，业务流量在短时间内剧增，造成巨大的流量毛刺，数据流入速度远高于数据处理速度，对流处理系统构成巨大的负载压力的问题。</p>
<p>如果不能处理流量毛刺或者持续的数据过高速率输入，可能导致Executor端出现OOM的情况或者任务崩溃。</p>
<h4 id="Spark-1-5以前版本"><a href="#Spark-1-5以前版本" class="headerlink" title="Spark 1.5以前版本"></a>Spark 1.5以前版本</h4><p>用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现（限制每个receiver没每秒最大可以接收的数据量）。此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。</p>
<p>direct-approach方式接收数据，可以配置 “spark.streaming.kafka.maxRatePerPartition”参数来限制每个kafka分区最多读取的数据量。</p>
<p>缺点：</p>
<p>​          1、实现需要进行压测，来设置最大值。参数的设置必须合理，如果集群处理能力高于配置的速率，则会造成资源的浪费。</p>
<p>​          2、参数需要手动设置，设置过后必须重启streaming服务。</p>
<h4 id="Spark-1-5以后版本"><a href="#Spark-1-5以后版本" class="headerlink" title="Spark 1.5以后版本"></a>Spark 1.5以后版本</h4><p>为了更好的协调数据接收速率与资源处理能力，1.5版本开始Spark Streaming可以动态控制数据接收速率来适配集群数据处理能力（能够根据当前数据量以及集群状态来预估下个批次最优速率）。背压机制（即Spark Streaming Backpressure）: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。</p>
<p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。</p>
<p><strong><u>以下背压机制spark1.5之后流程以及配置均摘抄自：<a href="https://blog.csdn.net/may_fly/article/details/103922862" target="_blank" rel="noopener">https://blog.csdn.net/may_fly/article/details/103922862</a></u></strong></p>
<p>新版具体流程如下：<img src="/bigdata/SparkStreaming/SparkStreaming3.png" alt="SparkStreaming3"></p>
<p>新版的背压机制主要通过<code>RateController</code>组件来实现。<code>RateController</code>继承了接口<code>StreamingListener</code>并实现了<code>onBatchCompleted</code>方法。</p>
<p>结合direct-approach方式的源码来理解</p>
<ol>
<li>首先创建一个kafka流。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>,<span class="type">String</span>,<span class="type">StringDecoder</span>,<span class="type">StringDecoder</span>,(<span class="type">String</span>,<span class="type">String</span>)](streamingContext, kafkaParams, getOffsets(topics,kc,kafkaParams),messageHandler)</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>

<ol>
<li>createDirectStream方法创建并返回一个DirectKafkaInputDStream对象</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create an input stream that directly pulls messages from Kafka Brokers</span></span><br><span class="line"><span class="comment">   * without using any receiver. This stream can guarantee that each message</span></span><br><span class="line"><span class="comment">   * from Kafka is included in transformations exactly once (see points below).</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Points to note:</span></span><br><span class="line"><span class="comment">   *  - No receivers: This stream does not use any receiver. It directly queries Kafka</span></span><br><span class="line"><span class="comment">   *  - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked</span></span><br><span class="line"><span class="comment">   *    by the stream itself. For interoperability with Kafka monitoring tools that depend on</span></span><br><span class="line"><span class="comment">   *    Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.</span></span><br><span class="line"><span class="comment">   *    You can access the offsets used in each batch from the generated RDDs (see</span></span><br><span class="line"><span class="comment">   *    [[org.apache.spark.streaming.kafka.HasOffsetRanges]]).</span></span><br><span class="line"><span class="comment">   *  - Failure Recovery: To recover from driver failures, you have to enable checkpointing</span></span><br><span class="line"><span class="comment">   *    in the `StreamingContext`. The information on consumed offset can be</span></span><br><span class="line"><span class="comment">   *    recovered from the checkpoint. See the programming guide for details (constraints, etc.).</span></span><br><span class="line"><span class="comment">   *  - End-to-end semantics: This stream ensures that every records is effectively received and</span></span><br><span class="line"><span class="comment">   *    transformed exactly once, but gives no guarantees on whether the transformed data are</span></span><br><span class="line"><span class="comment">   *    outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure</span></span><br><span class="line"><span class="comment">   *    that the output operation is idempotent, or use transactions to output records atomically.</span></span><br><span class="line"><span class="comment">   *    See the programming guide for more details.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param ssc StreamingContext object</span></span><br><span class="line"><span class="comment">   * @param kafkaParams Kafka &lt;a href="http://kafka.apache.org/documentation.html#configuration"&gt;</span></span><br><span class="line"><span class="comment">   *    configuration parameters&lt;/a&gt;. Requires "metadata.broker.list" or "bootstrap.servers"</span></span><br><span class="line"><span class="comment">   *    to be set with Kafka broker(s) (NOT zookeeper servers) specified in</span></span><br><span class="line"><span class="comment">   *    host1:port1,host2:port2 form.</span></span><br><span class="line"><span class="comment">   * @param fromOffsets Per-topic/partition Kafka offsets defining the (inclusive)</span></span><br><span class="line"><span class="comment">   *    starting point of the stream</span></span><br><span class="line"><span class="comment">   * @param messageHandler Function for translating each message and metadata into the desired type</span></span><br><span class="line"><span class="comment">   * @tparam K type of Kafka message key</span></span><br><span class="line"><span class="comment">   * @tparam V type of Kafka message value</span></span><br><span class="line"><span class="comment">   * @tparam KD type of Kafka message key decoder</span></span><br><span class="line"><span class="comment">   * @tparam VD type of Kafka message value decoder</span></span><br><span class="line"><span class="comment">   * @tparam R type returned by messageHandler</span></span><br><span class="line"><span class="comment">   * @return DStream of R</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDirectStream</span></span>[</span><br><span class="line">    <span class="type">K</span>: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">V</span>: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">KD</span> &lt;: <span class="type">Decoder</span>[<span class="type">K</span>]: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">VD</span> &lt;: <span class="type">Decoder</span>[<span class="type">V</span>]: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">R</span>: <span class="type">ClassTag</span>] (</span><br><span class="line">      ssc: <span class="type">StreamingContext</span>,</span><br><span class="line">      kafkaParams: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">      fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>],</span><br><span class="line">      messageHandler: <span class="type">MessageAndMetadata</span>[<span class="type">K</span>, <span class="type">V</span>] =&gt; <span class="type">R</span></span><br><span class="line">  ): <span class="type">InputDStream</span>[<span class="type">R</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanedHandler = ssc.sc.clean(messageHandler)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DirectKafkaInputDStream</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">KD</span>, <span class="type">VD</span>, <span class="type">R</span>](</span><br><span class="line">      ssc, kafkaParams, fromOffsets, cleanedHandler)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152</span></span><br></pre></td></tr></table></figure>

<ol>
<li>DirectKafkaInputDStream类继承了抽象类InputDStream，并重载了rateController方法。创建了DirectKafkaRateController类，并传入了一个速率估计类。如果设置RateController.isBackPressureEnabled为true也就是开启背压则开始计算下一次的最优速率</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Asynchronously maintains &amp; sends new rate limits to the receiver through the receiver tracker.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span>[streaming] <span class="keyword">val</span> rateController: <span class="type">Option</span>[<span class="type">RateController</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">RateController</span>.isBackPressureEnabled(ssc.conf)) &#123;</span><br><span class="line">      <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">DirectKafkaRateController</span>(id,</span><br><span class="line">        <span class="type">RateEstimator</span>.create(ssc.conf, context.graph.batchDuration)))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">1234567891011</span></span><br></pre></td></tr></table></figure>

<ol>
<li>DirectKafkaRateController内部实现了一个私有类来计算速率，publish方法使用lambda表达式调用了RateController中唯一一个公有的方法onBatchCompleted获取计算结果</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A RateController to retrieve the rate from RateEstimator.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[streaming] <span class="class"><span class="keyword">class</span> <span class="title">DirectKafkaRateController</span>(<span class="params">id: <span class="type">Int</span>, estimator: <span class="type">RateEstimator</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">RateController</span>(<span class="params">id, estimator</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">publish</span></span>(rate: <span class="type">Long</span>): <span class="type">Unit</span> = ()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">1234567</span></span><br></pre></td></tr></table></figure>

<ol>
<li>onBatchCompleted获取三个时间一个数据量：处理结束时间，处理时间，等待时间，当前处理数据量，并调用computeAndPublish方法计算下次最优的数据量</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBatchCompleted</span></span>(batchCompleted: <span class="type">StreamingListenerBatchCompleted</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> elements = batchCompleted.batchInfo.streamIdToInputInfo</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">      processingEnd &lt;- batchCompleted.batchInfo.processingEndTime</span><br><span class="line">      workDelay &lt;- batchCompleted.batchInfo.processingDelay</span><br><span class="line">      waitDelay &lt;- batchCompleted.batchInfo.schedulingDelay</span><br><span class="line">      elems &lt;- elements.get(streamUID).map(_.numRecords)</span><br><span class="line">    &#125; computeAndPublish(processingEnd, elems, workDelay, waitDelay)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">12345678910</span></span><br></pre></td></tr></table></figure>

<ol>
<li>computeAndPublish调用rateEstimator.compute方法计算速率</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Compute the new rate limit and publish it asynchronously.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">computeAndPublish</span></span>(time: <span class="type">Long</span>, elems: <span class="type">Long</span>, workDelay: <span class="type">Long</span>, waitDelay: <span class="type">Long</span>): <span class="type">Unit</span> =</span><br><span class="line">    <span class="type">Future</span>[<span class="type">Unit</span>] &#123;</span><br><span class="line">      <span class="keyword">val</span> newRate = rateEstimator.compute(time, elems, workDelay, waitDelay)</span><br><span class="line">      newRate.foreach &#123; s =&gt;</span><br><span class="line">        rateLimit.set(s.toLong)</span><br><span class="line">        publish(getLatestRate())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Computes the number of records the stream attached to this `RateEstimator`</span></span><br><span class="line"><span class="comment">   * should ingest per second, given an update on the size and completion</span></span><br><span class="line"><span class="comment">   * times of the latest batch.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param time The timestamp of the current batch interval that just finished</span></span><br><span class="line"><span class="comment">   * @param elements The number of records that were processed in this batch</span></span><br><span class="line"><span class="comment">   * @param processingDelay The time in ms that took for the job to complete</span></span><br><span class="line"><span class="comment">   * @param schedulingDelay The time in ms that the job spent in the scheduling queue</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>,</span><br><span class="line">      elements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>,</span><br><span class="line">      schedulingDelay: <span class="type">Long</span>): <span class="type">Option</span>[<span class="type">Double</span>]</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627</span></span><br></pre></td></tr></table></figure>

<ol>
<li>compute方法的具体实现，需要来看3中<code>RateEstimator.create(ssc.conf, context.graph.batchDuration)))</code>传入的RateEstimator类。由源码可知，默认调用pid速率估计器，是 <code>RateEstimator</code>的唯一实现 ，具体计算逻辑要看pid速率估计器的compute方法</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RateEstimator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new `RateEstimator` based on the value of</span></span><br><span class="line"><span class="comment">   * `spark.streaming.backpressure.rateEstimator`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * The only known and acceptable estimator right now is `pid`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return An instance of RateEstimator</span></span><br><span class="line"><span class="comment">   * @throws IllegalArgumentException if the configured RateEstimator is not `pid`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(conf: <span class="type">SparkConf</span>, batchInterval: <span class="type">Duration</span>): <span class="type">RateEstimator</span> =</span><br><span class="line">    conf.get(<span class="string">"spark.streaming.backpressure.rateEstimator"</span>, <span class="string">"pid"</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"pid"</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> proportional = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.proportional"</span>, <span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">val</span> integral = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.integral"</span>, <span class="number">0.2</span>)</span><br><span class="line">        <span class="keyword">val</span> derived = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.derived"</span>, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">val</span> minRate = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.minRate"</span>, <span class="number">100</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PIDRateEstimator</span>(batchInterval.milliseconds, proportional, integral, derived, minRate)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> estimator =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Unknown rate estimator: <span class="subst">$estimator</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="number">123456789101112131415161718192021222324</span></span><br></pre></td></tr></table></figure>

<ol>
<li>pid速率估计器的compute方法如下。具体流程不再细述，有时间举个例子推一下</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      numElements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      schedulingDelay: <span class="type">Long</span> <span class="comment">// in milliseconds</span></span><br><span class="line">    ): <span class="type">Option</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    logTrace(<span class="string">s"\ntime = <span class="subst">$time</span>, # records = <span class="subst">$numElements</span>, "</span> +</span><br><span class="line">      <span class="string">s"processing time = <span class="subst">$processingDelay</span>, scheduling delay = <span class="subst">$schedulingDelay</span>"</span>)</span><br><span class="line">    <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (time &gt; latestTime &amp;&amp; numElements &gt; <span class="number">0</span> &amp;&amp; processingDelay &gt; <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in seconds, should be close to batchDuration</span></span><br><span class="line">        <span class="keyword">val</span> delaySinceUpdate = (time - latestTime).toDouble / <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> processingRate = numElements.toDouble / processingDelay * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// In our system `error` is the difference between the desired rate and the measured rate</span></span><br><span class="line">        <span class="comment">// based on the latest batch information. We consider the desired rate to be latest rate,</span></span><br><span class="line">        <span class="comment">// which is what this estimator calculated for the previous batch.</span></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> error = latestRate - processingRate</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The error integral, based on schedulingDelay as an indicator for accumulated errors.</span></span><br><span class="line">        <span class="comment">// A scheduling delay s corresponds to s * processingRate overflowing elements. Those</span></span><br><span class="line">        <span class="comment">// are elements that couldn't be processed in previous batches, leading to this delay.</span></span><br><span class="line">        <span class="comment">// In the following, we assume the processingRate didn't change too much.</span></span><br><span class="line">        <span class="comment">// From the number of overflowing elements we can calculate the rate at which they would be</span></span><br><span class="line">        <span class="comment">// processed by dividing it by the batch interval. This rate is our "historical" error,</span></span><br><span class="line">        <span class="comment">// or integral part, since if we subtracted this rate from the previous "calculated rate",</span></span><br><span class="line">        <span class="comment">// there wouldn't have been any overflowing elements, and the scheduling delay would have</span></span><br><span class="line">        <span class="comment">// been zero.</span></span><br><span class="line">        <span class="comment">// (in elements/second)</span></span><br><span class="line">        <span class="keyword">val</span> historicalError = schedulingDelay.toDouble * processingRate / batchIntervalMillis</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/(second ^ 2)</span></span><br><span class="line">        <span class="keyword">val</span> dError = (error - latestError) / delaySinceUpdate</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> newRate = (latestRate - proportional * error -</span><br><span class="line">                                    integral * historicalError -</span><br><span class="line">                                    derivative * dError).max(minRate)</span><br><span class="line">        logTrace(<span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">            | latestRate = $latestRate, error = $error</span></span><br><span class="line"><span class="string">            | latestError = $latestError, historicalError = $historicalError</span></span><br><span class="line"><span class="string">            | delaySinceUpdate = $delaySinceUpdate, dError = $dError</span></span><br><span class="line"><span class="string">            "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        latestTime = time</span><br><span class="line">        <span class="keyword">if</span> (firstRun) &#123;</span><br><span class="line">          latestRate = processingRate</span><br><span class="line">          latestError = <span class="number">0</span>D</span><br><span class="line">          firstRun = <span class="literal">false</span></span><br><span class="line">          logTrace(<span class="string">"First run, rate estimation skipped"</span>)</span><br><span class="line">          <span class="type">None</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          latestRate = newRate</span><br><span class="line">          latestError = error</span><br><span class="line">          logTrace(<span class="string">s"New rate = <span class="subst">$newRate</span>"</span>)</span><br><span class="line">          <span class="type">Some</span>(newRate)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logTrace(<span class="string">"Rate estimation skipped"</span>)</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566</span></span><br></pre></td></tr></table></figure>

<ol>
<li>虽然通过这个公式计算出了一个速率，但最终的速率并不一定是计算出的结果。由代码可知，如果设置了参数spark.streaming.kafka.maxRatePerPartition，则每个分区所取数据最大量为计算出的结果以及设置参数的最小值，否则直接使用计算出的值</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span>[streaming] <span class="function"><span class="keyword">def</span> <span class="title">maxMessagesPerPartition</span></span>(</span><br><span class="line">      offsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>]): <span class="type">Option</span>[<span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> estimatedRateLimit = rateController.map(_.getLatestRate())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate a per-partition rate limit based on current lag</span></span><br><span class="line">    <span class="keyword">val</span> effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ &gt; <span class="number">0</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(rate) =&gt;</span><br><span class="line">        <span class="keyword">val</span> lagPerPartition = offsets.map &#123; <span class="keyword">case</span> (tp, offset) =&gt;</span><br><span class="line">          tp -&gt; <span class="type">Math</span>.max(offset - currentOffsets(tp), <span class="number">0</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> totalLag = lagPerPartition.values.sum</span><br><span class="line"></span><br><span class="line">        lagPerPartition.map &#123; <span class="keyword">case</span> (tp, lag) =&gt;</span><br><span class="line">          <span class="keyword">val</span> backpressureRate = <span class="type">Math</span>.round(lag / totalLag.toFloat * rate)</span><br><span class="line">          tp -&gt; (<span class="keyword">if</span> (maxRateLimitPerPartition &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">Math</span>.min(backpressureRate, maxRateLimitPerPartition)&#125; <span class="keyword">else</span> backpressureRate)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; offsets.map &#123; <span class="keyword">case</span> (tp, offset) =&gt; tp -&gt; maxRateLimitPerPartition &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> maxRateLimitPerPartition: <span class="type">Long</span> = context.sparkContext.getConf.getLong(</span><br><span class="line">      <span class="string">"spark.streaming.kafka.maxRatePerPartition"</span>, <span class="number">0</span>)</span><br><span class="line"><span class="number">12345678910111213141516171819202122</span></span><br></pre></td></tr></table></figure>

<p>一些相关的参数：</p>
<ol>
<li>开启背压机制：设置<strong>spark.streaming.backpressure.enabled</strong> 为true，默认为false</li>
<li>启用反压机制时每个接收器接收第一批数据的初始最大速率。默认值没有设置 <strong>spark.streaming.backpressure.initialRate</strong></li>
<li>速率估算器类，默认值为 pid ，目前 Spark 只支持这个，大家可以根据自己的需要实现 <strong>spark.streaming.backpressure.rateEstimator</strong></li>
<li>用于响应错误的权重（最后批次和当前批次之间的更改）。默认值为1，只能设置成非负值。<em>weight for response to “error” (change between last batch and this batch)</em> <strong>spark.streaming.backpressure.pid.proportional</strong></li>
<li>错误积累的响应权重，具有抑制作用（有效阻尼）。默认值为 0.2 ，只能设置成非负值。<em>weight for the response to the accumulation of error. This has a dampening effect.</em> <strong>spark.streaming.backpressure.pid.integral</strong></li>
<li>对错误趋势的响应权重。 这可能会引起 batch size 的波动，可以帮助快速增加/减少容量。默认值为0，只能设置成非负值。<em>weight for the response to the trend in error. This can cause arbitrary/noise-induced fluctuations in batch size, but can also help react quickly to increased/reduced capacity.</em> <strong>spark.streaming.backpressure.pid.derived</strong></li>
<li>可以估算的最低费率是多少。默认值为 100，只能设置成非负值。 <strong>spark.streaming.backpressure.pid.minRate</strong></li>
</ol>
<p>参考：</p>
<p><a href="https://blog.csdn.net/wangpei1949/article/details/90727805" target="_blank" rel="noopener">https://blog.csdn.net/wangpei1949/article/details/90727805</a></p>
<p><a href="https://blog.csdn.net/zengxiaosen/article/details/72822869" target="_blank" rel="noopener">https://blog.csdn.net/zengxiaosen/article/details/72822869</a></p>
<p><a href="https://www.cnblogs.com/barrenlake/p/5349949.html" target="_blank" rel="noopener">https://www.cnblogs.com/barrenlake/p/5349949.html</a></p>
<p><a href="https://www.iteblog.com/archives/2323.html?from=related" target="_blank" rel="noopener">https://www.iteblog.com/archives/2323.html?from=related</a></p>
<h1 id="Dstream入门"><a href="#Dstream入门" class="headerlink" title="Dstream入门"></a>Dstream入门</h1><h2 id="WordCount案例实操"><a href="#WordCount案例实操" class="headerlink" title="WordCount案例实操"></a>WordCount案例实操</h2><p>需求：使用netcat工具向9999端口不断的发送数据，通过SparkStreaming读取端口数据并统计不同单词出现的次数</p>
<p>1) 添加依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2) 编写代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Wordcount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Todo 1.配置对象</span></span><br><span class="line">    <span class="comment">//初始化Spark配置信息</span></span><br><span class="line">    <span class="comment">//Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行（local至少要两个节点）</span></span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Todo 2.环境对象</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    class StreamingContext private[streaming]加了包权限（私有） 所以主构不能用</span></span><br><span class="line"><span class="comment">    def this(sparkContext: SparkContext, batchDuration: Duration) 辅助构建方法可用（Spark配置信息，批处理持续时间）</span></span><br><span class="line"><span class="comment">    case class Duration (private val millis: Long)样例类 直接用 但是不方便 要自己算毫秒</span></span><br><span class="line"><span class="comment">    new StreamingContext(sparkconf , Duration(1000 * 3))</span></span><br><span class="line"><span class="comment">    所以直接使用伴生对象Seconds()</span></span><br><span class="line"><span class="comment">     object Seconds &#123;</span></span><br><span class="line"><span class="comment">        def apply(seconds: Long): Duration = new Duration(seconds * 1000)</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 初始化SparkStreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">3</span>)) <span class="comment">// 创建对象的第二个参数表示数据的采集周期</span></span><br><span class="line">    <span class="comment">//Todo 3.数据处理</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 从数据源采集数据</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 //内存和磁盘有两个序列化副本（socketStream默认的存储级别）</span></span><br><span class="line"><span class="comment">    ReceiverInputDStream[String]  Receiver:接收器 Input:输入  DStream:离散化流   */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span> )</span><br><span class="line">    <span class="comment">//TODO 将采集数据进行WordCount的处理</span></span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = socketDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在控制台上打印结果</span></span><br><span class="line"><span class="comment">/*    可以看出底层还是RDD</span></span><br><span class="line"><span class="comment">def print(num: Int): Unit = ssc.withScope &#123;</span></span><br><span class="line"><span class="comment">      def foreachFunc: (RDD[T], Time) =&gt; Unit = &#123;</span></span><br><span class="line"><span class="comment">        (rdd: RDD[T], time: Time) =&gt; &#123;</span></span><br><span class="line"><span class="comment">          val firstNum = rdd.take(num + 1)</span></span><br><span class="line"><span class="comment">          // scalastyle:off println</span></span><br><span class="line"><span class="comment">          println("-------------------------------------------")</span></span><br><span class="line"><span class="comment">          println(s"Time: $time")</span></span><br><span class="line"><span class="comment">          println("-------------------------------------------")</span></span><br><span class="line"><span class="comment">          firstNum.take(num).foreach(println)</span></span><br><span class="line"><span class="comment">          if (firstNum.length &gt; num) println("...")</span></span><br><span class="line"><span class="comment">          println()</span></span><br><span class="line"><span class="comment">          // scalastyle:on println</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Todo 4.开启连接环境</span></span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">    和spark、scala不同的是：最后并不关闭连接环境（除非程序升级或者出现故障的时候，因为数据采集是要7*24）</span></span><br><span class="line"><span class="comment">    并且不能让driver程序结束，需要让driver程序等待,等待数据处理的停止或异常时，才会继续执行</span></span><br><span class="line"><span class="comment">      def awaitTermination() &#123;</span></span><br><span class="line"><span class="comment">        waiter.waitForStopOrError()  //等待停止或者异常，如果没有停止和异常现象 程序不结束</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    ssc.start();</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3) 启动程序并通过netcat发送数据：</p>
<blockquote>
<p>nc -lk 9999</p>
<p>hello red</p>
<p>hello world</p>
</blockquote>
<h2 id="WordCount解析"><a href="#WordCount解析" class="headerlink" title="WordCount解析"></a>WordCount解析</h2><p>Discretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，<strong>DStream是一系列连续的RDD来表示</strong>。<u>每个RDD含有一段时间间隔内的数据（段时间内所有数据在一个RDD里）。</u><img src="/bigdata/SparkStreaming/SparkStreaming10.jpg" alt="SparkStreaming10"></p>
<p>对数据的操作也是按照RDD为单位来进行的</p>
<p><img src="/bigdata/SparkStreaming/SparkStreaming11.jpg" alt="SparkStreaming11"></p>
<p>计算过程由Spark Engine来完成</p>
<p><img src="/bigdata/SparkStreaming/SparkStreaming12.jpg" alt="SparkStreaming12"></p>
<h1 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h1><h2 id="RDD队列"><a href="#RDD队列" class="headerlink" title="RDD队列"></a>RDD队列</h2><h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue()"></a>Queue()</h3><h4 id="用法及说明"><a href="#用法及说明" class="headerlink" title="用法及说明"></a>用法及说明</h4><p><font color="red">测试过程</font>中，可以通过使用ssc.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。</p>
<h4 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h4><p>需求：循环创建几个RDD，将RDD放入队列。通过SparkStream创建Dstream，计算WordCount</p>
<p>1) 编写代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDStream</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.初始化Spark配置信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDDStream"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.初始化SparkStreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.创建RDD队列</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.创建QueueInputDStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.处理队列中的RDD数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.打印结果</span></span><br><span class="line"></span><br><span class="line">  reducedStream.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.启动任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//8.循环创建并向RDD队列中放入RDD</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line"></span><br><span class="line">   rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">   <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2) 结果展示</p>
<blockquote>
<p>-——————————————</p>
<p>Time: 1539075280000 ms</p>
<p>-——————————————</p>
<p>(4,60)</p>
<p>(0,60)</p>
<p>(6,60)</p>
<p>(8,60)</p>
<p>(2,60)</p>
<p>(1,60)</p>
<p>(3,60)</p>
<p>(7,60)</p>
<p>(9,60)</p>
<p>(5,60)</p>
<p>-——————————————</p>
<p>Time: 1539075284000 ms</p>
<p>-——————————————</p>
<p>(4,60)</p>
<p>(0,60)</p>
<p>(6,60)</p>
<p>(8,60)</p>
<p>(2,60)</p>
<p>(1,60)</p>
<p>(3,60)</p>
<p>(7,60)</p>
<p>(9,60)</p>
<p>(5,60)</p>
<p>-——————————————</p>
<p>Time: 1539075288000 ms</p>
<p>-——————————————</p>
<p>(4,30)</p>
<p>(0,30)</p>
<p>(6,30)</p>
<p>(8,30)</p>
<p>(2,30)</p>
<p>(1,30)</p>
<p>(3,30)</p>
<p>(7,30)</p>
<p>(9,30)</p>
<p>(5,30)</p>
<p>-——————————————</p>
<p>Time: 1539075292000 ms</p>
<p>-——————————————</p>
</blockquote>
<h3 id="file"><a href="#file" class="headerlink" title="file()"></a>file()</h3><h4 id="用法及说明-1"><a href="#用法及说明-1" class="headerlink" title="用法及说明"></a>用法及说明</h4><p><font color="red">测试过程</font>中，可以通过使用ssc.textFileStream(“in”)来创建DStream，监控文件夹的变化。</p>
<p>从文件夹中读取新的文件数据（拽过去的文件可能读不到），功能不稳定 ，所以不推荐使用</p>
<p>flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume</p>
<h4 id="案例实操-1"><a href="#案例实操-1" class="headerlink" title="案例实操"></a>案例实操</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming03_DStream_File</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 数据处理</span></span><br><span class="line">    <span class="comment">// 从文件夹中读取新的文件数据，功能不稳定 ，所以不推荐使用</span></span><br><span class="line">    <span class="comment">// flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume</span></span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">DStream</span>[<span class="type">String</span>] = ssc.textFileStream(<span class="string">"in"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map( (_, <span class="number">1</span>) )</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭连接环境</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>






<h2 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h2><h3 id="用法及说明-2"><a href="#用法及说明-2" class="headerlink" title="用法及说明"></a>用法及说明</h3><p>需要继承Receiver，并实现onStart、onStop方法来自定义数据源采集。</p>
<h3 id="案例实操-2"><a href="#案例实操-2" class="headerlink" title="案例实操"></a>案例实操</h3><p>需求：自定义数据源，实现监控某个端口号，获取该端口号内容。</p>
<p>1) 自定义数据源</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  自定义数据采集器</span></span><br><span class="line"><span class="comment">    自定义数据采集器</span></span><br><span class="line"><span class="comment">    模仿spark自带的socket采集器</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  abstract class Receiver[T](val storageLevel: StorageLevel) extends Serializable</span></span><br><span class="line"><span class="comment">  StorageLevel :存储级别 MEMORY_ONLY DISK_ONLY MEMORY_AND_DISK</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  步骤： 1. 继承Receiver ,设定泛型（采集数据的类型）, 传递参数</span></span><br><span class="line"><span class="comment">         2. 重写方法</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyRecevier</span>(<span class="params">host : <span class="type">String</span> , port : <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>)</span>&#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   socketTextStream:  socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)</span></span><br><span class="line"><span class="comment">    socketStream：  new SocketInputDStream[T](this, hostname, port, converter, storageLevel)</span></span><br><span class="line"><span class="comment">    SocketInputDStream:  new SocketReceiver(host, port, bytesToObjects, storageLevel)</span></span><br><span class="line"><span class="comment">    SocketReceiver：extends Receiver[T](storageLevel) with Logging</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> socket: <span class="type">Socket</span> = _</span><br><span class="line">    <span class="comment">// SocketReceiver：socket.getInputStream()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">      <span class="comment">//我们需要字符串，所以将字节流转换为缓冲字符流</span></span><br><span class="line">      <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">InputStreamReader</span>(</span><br><span class="line">          <span class="comment">//获取从网络中传递来的数据（字节流）</span></span><br><span class="line">          socket.getInputStream,</span><br><span class="line">          <span class="string">"UTF-8"</span></span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      <span class="keyword">var</span> s:<span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">     <span class="comment">/*</span></span><br><span class="line"><span class="comment">      这个“s = reader.readLine())!= null”语句是错误的，因为在网络编程中获取的数据是没有null的概念</span></span><br><span class="line"><span class="comment">      文件读取时，如果读到结束的时候，获取的结果为null（文件读取这样是对的）</span></span><br><span class="line"><span class="comment">      但是在网络中我可以现在传递一些数据  过一段就再传一次，所以null是无法判断的</span></span><br><span class="line"><span class="comment">      网络编程中，需要明确告知服务器，客户端不再传数据，需要发送特殊的指令</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      <span class="keyword">while</span>(( s = reader.readLine())!= <span class="literal">null</span>)&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需要发送特殊的指令</span></span><br><span class="line">        <span class="keyword">if</span>(s != <span class="string">"-END-"</span>)&#123;</span><br><span class="line">          <span class="comment">//采集到数据后，进行封装(存储)</span></span><br><span class="line">              store(s)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          <span class="comment">// stop</span></span><br><span class="line">          <span class="comment">// close</span></span><br><span class="line">          <span class="comment">// 重启</span></span><br><span class="line">          <span class="comment">//restart("")</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 启动采集器</span></span><br><span class="line">    <span class="comment">// 采集 &amp; 封装</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      socket = <span class="keyword">new</span> <span class="type">Socket</span>(host , port)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"Socket Receiver"</span>) &#123;</span><br><span class="line">        setDaemon(<span class="literal">true</span>)<span class="comment">//守护线程</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">          receive() &#125;</span><br><span class="line">      &#125;.start()<span class="comment">//start()会回调run()方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (socket != <span class="literal">null</span>)&#123;</span><br><span class="line">        socket.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>2) 使用自定义的数据源采集数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamig_DIY</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span>  <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"DIY采集器"</span>)</span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// TODO 数据处理</span></span><br><span class="line">    <span class="comment">// 自定义数据采集器</span></span><br><span class="line">    <span class="keyword">val</span> myDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.receiverStream(<span class="keyword">new</span> <span class="type">MyRecevier</span>(<span class="string">"localhost"</span> , <span class="number">9999</span>))</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = myDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line">    <span class="comment">// TODO 开启连接环境</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h2 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h2><h3 id="版本选型"><a href="#版本选型" class="headerlink" title="版本选型"></a>版本选型</h3><p><strong>ReceiverAPI</strong>：需要一个专门的Executor去接收数据，然后发送给其他的Executor做计算。存在的问题，接收数据的Executor和计算的Executor速度会有所不同，特别在接收数据的Executor速度大于计算的Executor速度，会导致计算数据的节点内存溢出。</p>
<p><strong>DirectAPI</strong>：是由计算的Executor来主动消费Kafka的数据，速度由自身控制。</p>
<p> <img src="/bigdata/SparkStreaming/SparkStreaming13.jpg" alt="SparkStreaming13"></p>
<h3 id="Kafka-0-8-Receiver模式"><a href="#Kafka-0-8-Receiver模式" class="headerlink" title="Kafka 0-8 Receiver模式"></a>Kafka 0-8 Receiver模式</h3><p>1） 需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Kafka</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span>  <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"DIY采集器"</span>)</span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// TODO 数据处理 - 读取Kafka数据创建DStream(基于Receive方式)</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    def createStream(</span></span><br><span class="line"><span class="comment">                      ssc : org.apache.spark.streaming.StreamingContext,</span></span><br><span class="line"><span class="comment">                      zkQuorum : scala.Predef.String, //zookeeper</span></span><br><span class="line"><span class="comment">                      groupId : scala.Predef.String, //消费者组</span></span><br><span class="line"><span class="comment">                      topics : scala.Predef.Map[scala.Predef.String, scala.Int],//分区数</span></span><br><span class="line"><span class="comment">                      storageLevel : org.apache.spark.storage.StorageLevel = &#123;  compiled code  &#125;</span></span><br><span class="line"><span class="comment">                    ) : org.apache.spark.streaming.dstream.ReceiverInputDStream[scala.Tuple2[scala.Predef.String, scala.Predef.String]] = &#123;  compiled code  &#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaDS: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="string">"linux1:2181,linux2:2181,linux3:2181"</span>,</span><br><span class="line">      <span class="string">"red0819"</span>,</span><br><span class="line">      <span class="type">Map</span>(<span class="string">"red0819"</span> -&gt; <span class="number">3</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// Kafka消息传递的时候以k-v对</span></span><br><span class="line">    <span class="comment">// k - 传值的时候提供的，默认为null,主要用于分区</span></span><br><span class="line">    <span class="comment">// v - message</span></span><br><span class="line">    kafkaDS.map((_._2)).print()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Kafka-0-8-Direct模式"><a href="#Kafka-0-8-Direct模式" class="headerlink" title="Kafka 0-8 Direct模式"></a>Kafka 0-8 Direct模式</h3><p>1）需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<h4 id="自动维护offset"><a href="#自动维护offset" class="headerlink" title="自动维护offset"></a>自动维护offset</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIAuto02</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> getSSC1: () =&gt; <span class="type">StreamingContext</span> = () =&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">getSSC</span></span>: <span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//设置CK</span></span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck2"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.定义Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.读取Kafka数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc,</span><br><span class="line"></span><br><span class="line">   kafkaPara,</span><br><span class="line"></span><br><span class="line">   <span class="type">Set</span>(<span class="string">"red"</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.计算WordCount</span></span><br><span class="line"></span><br><span class="line">  kafkaDStream.map(_._2)</span><br><span class="line"></span><br><span class="line">   .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">   .print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.返回数据</span></span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取SSC</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck2"</span>, () =&gt; getSSC)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="手动维护offset"><a href="#手动维护offset" class="headerlink" title="手动维护offset"></a>手动维护offset</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.common.<span class="type">TopicAndPartition</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.message.<span class="type">MessageAndMetadata</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.&#123;<span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">OffsetRange</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIHandler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.获取上一次启动最后保留的Offset=&gt;getOffset(MySQL)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>] = <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>](<span class="type">TopicAndPartition</span>(<span class="string">"red"</span>, <span class="number">0</span>) -&gt; <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.读取Kafka数据创建DStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">String</span>] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>, <span class="type">String</span>](ssc,</span><br><span class="line"></span><br><span class="line">   kafkaPara,</span><br><span class="line"></span><br><span class="line">   fromOffsets,</span><br><span class="line"></span><br><span class="line">   (m: <span class="type">MessageAndMetadata</span>[<span class="type">String</span>, <span class="type">String</span>]) =&gt; m.message())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.创建一个数组用于存放当前消费数据的offset信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> offsetRanges = <span class="type">Array</span>.empty[<span class="type">OffsetRange</span>]</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.获取当前消费数据的offset信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToCountDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = kafkaDStream.transform &#123; rdd =&gt;</span><br><span class="line"></span><br><span class="line">   offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">   rdd</span><br><span class="line"></span><br><span class="line">  &#125;.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//8.打印Offset信息</span></span><br><span class="line"></span><br><span class="line">  wordToCountDStream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span>:<span class="subst">$&#123;o.partition&#125;</span>:<span class="subst">$&#123;o.fromOffset&#125;</span>:<span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//9.开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Kafka-0-10-Direct模式"><a href="#Kafka-0-10-Direct模式" class="headerlink" title="Kafka 0-10 Direct模式"></a>Kafka 0-10 Direct模式</h3><p>1）需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPI</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.定义Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.读取Kafka数据创建DStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line"></span><br><span class="line">   <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"red"</span>), kafkaPara))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.将每条消息的KV取出</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> valueDStream: <span class="type">DStream</span>[<span class="type">String</span>] = kafkaDStream.map(record =&gt; record.value())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.计算WordCount</span></span><br><span class="line"></span><br><span class="line">  valueDStream.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">   .print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="消费Kafka数据模式总结"><a href="#消费Kafka数据模式总结" class="headerlink" title="消费Kafka数据模式总结"></a>消费Kafka数据模式总结</h3><h4 id="0-8"><a href="#0-8" class="headerlink" title="0-8"></a>0-8</h4><h5 id="ReceiverAPI"><a href="#ReceiverAPI" class="headerlink" title="ReceiverAPI"></a>ReceiverAPI</h5><p>1) 专门的Executor读取数据，速度不统一</p>
<p>  数据丢失：预写日志开启</p>
<p>2) 跨机器传输数据</p>
<p>3) Executor读取数据通过多个线程的方式，想要增加并行度，则需要多个流union</p>
<p>4) offset存储在zookeeper中</p>
<h5 id="DirectAPI"><a href="#DirectAPI" class="headerlink" title="DirectAPI"></a>DirectAPI</h5><p>1) Executor读取数据并计算</p>
<p>2) 增加Executor个数来增加消费的并行度</p>
<p>3) offset存储</p>
<p>a. CheckPoint(getActiveOrCreate方式创建StreamingContext)</p>
<p>从checkpoint中读取数据偏移量（不推荐使用）</p>
<blockquote>
<p>理由：</p>
<p>​        checkpoint还保存了计算逻辑，不适合扩展功能<br>​                checkpoint会延续计算，但是可能会压垮内存<br>​                checkpoint一般的存储路径为HDFS，所以会导致小文件过多</p>
</blockquote>
<p>b. 手动维护(有事务的存储系统)</p>
<p>4) 获取offset必须在第一个调用的算子中：</p>
<p>offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</p>
<h4 id="0-10-DirectAPI"><a href="#0-10-DirectAPI" class="headerlink" title="0-10 DirectAPI"></a>0-10 DirectAPI</h4><p>1) Executor读取数据并计算</p>
<p>2) 增加Executor个数来增加消费的并行度</p>
<p>3) offset存储</p>
<p>a. __consumer_offsets系统主题中</p>
<p>b. 手动维护(有事务的存储系统)</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title>基础SQL题（来自牛客网）</title>
    <url>/uncategorized/%E5%9F%BA%E7%A1%80SQL%E9%A2%98%EF%BC%88%E6%9D%A5%E8%87%AA%E7%89%9B%E5%AE%A2%E7%BD%91%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="基础SQL"><a href="#基础SQL" class="headerlink" title="基础SQL"></a>基础SQL</h1><h2 id="查找最晚入职员工的所有信息"><a href="#查找最晚入职员工的所有信息" class="headerlink" title="查找最晚入职员工的所有信息"></a>查找最晚入职员工的所有信息</h2><p>查找最晚入职员工的所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天(sqlite里面的注释为–,mysql为comment)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;employees&#96; (</span><br><span class="line">&#96;emp_no&#96; int(11) NOT NULL, -- &#39;员工编号&#39;</span><br><span class="line">&#96;birth_date&#96; date NOT NULL,</span><br><span class="line">&#96;first_name&#96; varchar(14) NOT NULL,</span><br><span class="line">&#96;last_name&#96; varchar(16) NOT NULL,</span><br><span class="line">&#96;gender&#96; char(1) NOT NULL,</span><br><span class="line">&#96;hire_date&#96; date NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;emp_no&#96;));</span><br></pre></td></tr></table></figure>

<h4 id="输入描述"><a href="#输入描述" class="headerlink" title="输入描述"></a>输入描述</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">无</span><br></pre></td></tr></table></figure>

<h4 id="输出描述"><a href="#输出描述" class="headerlink" title="输出描述"></a>输出描述</h4><p>示例</p>
<table>
<thead>
<tr>
<th>emp_no</th>
<th>birth_date</th>
<th>first_name</th>
<th>last_name</th>
<th>gender</th>
<th>hire_date</th>
</tr>
</thead>
<tbody><tr>
<td>10008</td>
<td>1958-02-19</td>
<td>Saniya</td>
<td>Kalloufi</td>
<td>M</td>
<td>1994-09-15</td>
</tr>
</tbody></table>
<h3 id="我的答案"><a href="#我的答案" class="headerlink" title="我的答案"></a>我的答案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from employees where hire_date &#x3D; (select max(hire_date) from employees );</span><br></pre></td></tr></table></figure>

<p>运行时间: 17ms 占用内存: 3320KB</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from employees order by hire_date desc limit 1</span><br></pre></td></tr></table></figure>

<p>运行时间: 11ms 占用内存: 3320KB</p>
<h3 id="最优答案"><a href="#最优答案" class="headerlink" title="最优答案"></a>最优答案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM employees</span><br><span class="line">WHERE hire_date &#x3D; (SELECT MAX(hire_date) FROM employees)</span><br></pre></td></tr></table></figure>

<p>用户：ktktktkt 运行时间: 10ms 占用内存: 3192KB</p>
<h2 id="查找入职员工时间排名倒数第三的员工所有信息"><a href="#查找入职员工时间排名倒数第三的员工所有信息" class="headerlink" title="查找入职员工时间排名倒数第三的员工所有信息"></a>查找入职员工时间排名倒数第三的员工所有信息</h2><p>查找入职员工时间排名倒数第三的员工所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;employees&#96; (</span><br><span class="line">&#96;emp_no&#96; int(11) NOT NULL,</span><br><span class="line">&#96;birth_date&#96; date NOT NULL,</span><br><span class="line">&#96;first_name&#96; varchar(14) NOT NULL,</span><br><span class="line">&#96;last_name&#96; varchar(16) NOT NULL,</span><br><span class="line">&#96;gender&#96; char(1) NOT NULL,</span><br><span class="line">&#96;hire_date&#96; date NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;emp_no&#96;));</span><br></pre></td></tr></table></figure>

<h4 id="输入描述-1"><a href="#输入描述-1" class="headerlink" title="输入描述"></a>输入描述</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">无</span><br></pre></td></tr></table></figure>

<h4 id="输出描述-1"><a href="#输出描述-1" class="headerlink" title="输出描述"></a>输出描述</h4><p>示例1</p>
<table>
<thead>
<tr>
<th>emp_no</th>
<th>birth_date</th>
<th>first_name</th>
<th>last_name</th>
<th>gender</th>
<th>hire_date</th>
</tr>
</thead>
<tbody><tr>
<td>10005</td>
<td>1955-01-21</td>
<td>Kyoichi</td>
<td>Maliniak</td>
<td>M</td>
<td>1989-09-12</td>
</tr>
</tbody></table>
<h3 id="我的答案-1"><a href="#我的答案-1" class="headerlink" title="我的答案"></a>我的答案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from employees </span><br><span class="line">where hire_date &#x3D; (select hire_date from employees order by hire_date desc limit 2,1</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>运行时间: 12ms 占用内存: 3364KB</p>
<h3 id="最优答案-1"><a href="#最优答案-1" class="headerlink" title="最优答案"></a>最优答案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select *</span><br><span class="line"> &#96;&#96;from employees</span><br><span class="line"> &#96;&#96;order by hire_date desc</span><br><span class="line"> &#96;&#96;limit &#96;&#96;2&#96;&#96;,&#96;&#96;1&#96;&#96;;</span><br></pre></td></tr></table></figure>

<p>用户：Howedata 运行时间: 10ms 占用内存: 3284KB</p>
<h2 id="查找当前薪水详情以及部门编号"><a href="#查找当前薪水详情以及部门编号" class="headerlink" title="查找当前薪水详情以及部门编号"></a>查找当前薪水详情以及部门编号</h2><p>查找最晚入职员工的所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天(sqlite里面的注释为–,mysql为comment)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;employees&#96; (</span><br><span class="line">&#96;emp_no&#96; int(11) NOT NULL, -- &#39;员工编号&#39;</span><br><span class="line">&#96;birth_date&#96; date NOT NULL,</span><br><span class="line">&#96;first_name&#96; varchar(14) NOT NULL,</span><br><span class="line">&#96;last_name&#96; varchar(16) NOT NULL,</span><br><span class="line">&#96;gender&#96; char(1) NOT NULL,</span><br><span class="line">&#96;hire_date&#96; date NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;emp_no&#96;));</span><br></pre></td></tr></table></figure>

<h4 id="输入描述-2"><a href="#输入描述-2" class="headerlink" title="输入描述"></a>输入描述</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">无</span><br></pre></td></tr></table></figure>

<h4 id="输出描述-2"><a href="#输出描述-2" class="headerlink" title="输出描述"></a>输出描述</h4><p>示例</p>
<table>
<thead>
<tr>
<th>emp_no</th>
<th>salary</th>
<th>from_date</th>
<th>to_date</th>
<th>dept_no</th>
</tr>
</thead>
<tbody><tr>
<td>10002</td>
<td>72527</td>
<td>2001-08-02</td>
<td>9999-01-01</td>
<td>d001</td>
</tr>
<tr>
<td>10004</td>
<td>74057</td>
<td>2001-11-27</td>
<td>9999-01-01</td>
<td>d004</td>
</tr>
<tr>
<td>10005</td>
<td>94692</td>
<td>2001-09-09</td>
<td>9999-01-01</td>
<td>d003</td>
</tr>
<tr>
<td>10006</td>
<td>43311</td>
<td>2001-08-02</td>
<td>9999-01-01</td>
<td>d002</td>
</tr>
<tr>
<td>10010</td>
<td>94409</td>
<td>2001-11-23</td>
<td>9999-01-01</td>
<td>d006</td>
</tr>
</tbody></table>
<h3 id="我的答案-2"><a href="#我的答案-2" class="headerlink" title="我的答案"></a>我的答案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select sa.emp_no , sa.salary ,sa.from_date ,sa.to_date ,de.dept_no</span><br><span class="line">from </span><br><span class="line">salaries sa join dept_manager de on </span><br><span class="line">sa.emp_no &#x3D; de.emp_no</span><br><span class="line">where</span><br><span class="line">sa.to_date&#x3D;&#39;9999-01-01&#39; and de.to_date&#x3D;&#39;9999-01-01&#39;</span><br><span class="line">order by </span><br><span class="line">sa.emp_no;</span><br></pre></td></tr></table></figure>

<p>运行时间:12ms 占用内存: 3344KB</p>
<h3 id="最优答案-2"><a href="#最优答案-2" class="headerlink" title="最优答案"></a>最优答案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select salaries.*, dept_manager.dept_no from </span><br><span class="line">salaries,dept_manager on salaries.emp_no &#x3D; dept_manager.emp_no </span><br><span class="line">where salaries.to_date&#x3D;&#39;9999-01-01&#39; and dept_manager.to_date&#x3D;&#39;9999-01-01&#39;;</span><br></pre></td></tr></table></figure>

<p>用户：牛客326549353号 运行时间: 11ms 占用内存: 3248KB</p>
<h2 id="查找所有已经分配部门的员工的last-name和first-name以及dept-no"><a href="#查找所有已经分配部门的员工的last-name和first-name以及dept-no" class="headerlink" title="查找所有已经分配部门的员工的last_name和first_name以及dept_no"></a>查找所有已经分配部门的员工的last_name和first_name以及dept_no</h2><p>查找最晚入职员工的所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天(sqlite里面的注释为–,mysql为comment)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;employees&#96; (</span><br><span class="line">&#96;emp_no&#96; int(11) NOT NULL, -- &#39;员工编号&#39;</span><br><span class="line">&#96;birth_date&#96; date NOT NULL,</span><br><span class="line">&#96;first_name&#96; varchar(14) NOT NULL,</span><br><span class="line">&#96;last_name&#96; varchar(16) NOT NULL,</span><br><span class="line">&#96;gender&#96; char(1) NOT NULL,</span><br><span class="line">&#96;hire_date&#96; date NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;emp_no&#96;));</span><br></pre></td></tr></table></figure>

<h4 id="输入描述-3"><a href="#输入描述-3" class="headerlink" title="输入描述"></a>输入描述</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">无</span><br></pre></td></tr></table></figure>

<h4 id="输出描述-3"><a href="#输出描述-3" class="headerlink" title="输出描述"></a>输出描述</h4><p>示例</p>
<table>
<thead>
<tr>
<th>last_name</th>
<th>first_name</th>
<th>dept_no</th>
</tr>
</thead>
<tbody><tr>
<td>Facello</td>
<td>Georgi</td>
<td>d001</td>
</tr>
<tr>
<td>省略</td>
<td>省略</td>
<td>省略</td>
</tr>
<tr>
<td>Piveteau</td>
<td>Duangkaew</td>
<td>d006</td>
</tr>
</tbody></table>
<h3 id="我的答案-3"><a href="#我的答案-3" class="headerlink" title="我的答案"></a>我的答案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select employees.last_name,employees.first_name,dept_emp.dept_no </span><br><span class="line">from </span><br><span class="line">employees inner join dept_emp </span><br><span class="line">on employees.emp_no&#x3D;dept_emp.emp_no</span><br></pre></td></tr></table></figure>

<p>运行时间:12ms 占用内存: 3300KB</p>
<h3 id="最优答案-3"><a href="#最优答案-3" class="headerlink" title="最优答案"></a>最优答案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select employees.last_name,employees.first_name,dept_emp.dept_no </span><br><span class="line">from employees inner join dept_emp</span><br><span class="line">on employees.emp_no&#x3D;dept_emp.emp_no</span><br></pre></td></tr></table></figure>

<p>用户：牛客180011609号 运行时间: 10ms 占用内存: 3300KB</p>
]]></content>
      <tags>
        <tag>SQL基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Oozie简介</title>
    <url>/bigdata/Oozie%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Oozie简介"><a href="#Oozie简介" class="headerlink" title="Oozie简介"></a>Oozie简介</h1><p>Oozie英文翻译为：驯象人。一个基于工作流引擎的开源框架，由Cloudera公司贡献给Apache，提供对Hadoop<br>MapReduce、Pig Jobs的任务调度与协调。Oozie需要部署到Java<br>Servlet容器中运行。主要用于定时调度任务，多任务可以按照执行的逻辑顺序调度。</p>
<h1 id="Oozie的功能模块介绍"><a href="#Oozie的功能模块介绍" class="headerlink" title="Oozie的功能模块介绍"></a>Oozie的功能模块介绍</h1><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><p><strong>1) Workflow</strong></p>
<p>顺序执行流程节点，支持fork（分支多个节点），join（合并多个节点为一个）</p>
<p><strong>2) Coordinator</strong></p>
<p>定时触发workflow</p>
<p><strong>3) Bundle</strong></p>
<p>绑定多个Coordinator</p>
<h2 id="Workflow常用节点"><a href="#Workflow常用节点" class="headerlink" title="Workflow常用节点"></a>Workflow常用节点</h2><p><strong>1) 控制流节点（Control Flow Nodes）</strong></p>
<p>控制流节点一般都是定义在工作流开始或者结束的位置，比如start,end,kill等。以及提供工作流的执行路径机制，如decision，fork，join等。</p>
<p><strong>2) 动作节点（Action Nodes）</strong></p>
<p>负责执行具体动作的节点，比如：拷贝文件，执行某个Shell脚本等等。</p>
<h1 id="Oozie的部署"><a href="#Oozie的部署" class="headerlink" title="Oozie的部署"></a>Oozie的部署</h1><h2 id="部署Hadoop（CDH版本的）"><a href="#部署Hadoop（CDH版本的）" class="headerlink" title="部署Hadoop（CDH版本的）"></a>部署Hadoop（CDH版本的）</h2><h3 id="修改Hadoop配置"><a href="#修改Hadoop配置" class="headerlink" title="修改Hadoop配置"></a>修改Hadoop配置</h3><p><strong>core-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Oozie Server的Hostname --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.red.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 允许被Oozie代理的用户组 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.red.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>mapred-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>yarn-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 任务历史服务 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop102:19888/jobhistory/logs/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>完成后：记得scp同步到其他机器节点</p>
<h3 id="启动Hadoop集群"><a href="#启动Hadoop集群" class="headerlink" title="启动Hadoop集群"></a>启动Hadoop集群</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 hadoop-5.0-cdh5.6]$ sbin&#x2F;start-dfs.sh</span><br><span class="line"></span><br><span class="line">[red@hadoop103 hadoop-5.0-cdh5.6]$ sbin&#x2F;start-yarn.sh</span><br><span class="line"></span><br><span class="line">[red@hadoop102 hadoop-5.0-cdh5.6]$</span><br><span class="line">sbin&#x2F;mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<p>注意：需要开启JobHistoryServer, 最好执行一个MR任务进行测试。</p>
<h2 id="部署Oozie"><a href="#部署Oozie" class="headerlink" title="部署Oozie"></a>部署Oozie</h2><h3 id="解压Oozie"><a href="#解压Oozie" class="headerlink" title="解压Oozie"></a>解压Oozie</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 software]$ tar -zxvf</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;cdh&#x2F;oozie-4.0.0-cdh5.6.tar.gz -C &#x2F;opt&#x2F;module</span><br></pre></td></tr></table></figure>



<h3 id="在oozie根目录下解压oozie-hadooplibs-4-0-0-cdh5-6-tar-gz"><a href="#在oozie根目录下解压oozie-hadooplibs-4-0-0-cdh5-6-tar-gz" class="headerlink" title="在oozie根目录下解压oozie-hadooplibs-4.0.0-cdh5.6.tar.gz"></a>在oozie根目录下解压oozie-hadooplibs-4.0.0-cdh5.6.tar.gz</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ tar -zxvf</span><br><span class="line">oozie-hadooplibs-4.0.0-cdh5.6.tar.gz -C ..&#x2F;</span><br></pre></td></tr></table></figure>

<p>完成后Oozie目录下会出现hadooplibs目录。</p>
<h3 id="在Oozie目录下创建libext目录"><a href="#在Oozie目录下创建libext目录" class="headerlink" title="在Oozie目录下创建libext目录"></a>在Oozie目录下创建libext目录</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ mkdir libext&#x2F;</span><br></pre></td></tr></table></figure>



<h3 id="拷贝依赖的Jar包"><a href="#拷贝依赖的Jar包" class="headerlink" title="拷贝依赖的Jar包"></a>拷贝依赖的Jar包</h3><p>1）将hadooplibs里面的jar包，拷贝到libext目录下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ cp -ra</span><br><span class="line">hadooplibs&#x2F;hadooplib-5.0-cdh5.6.oozie-4.0.0-cdh5.6&#x2F;* libext&#x2F;</span><br></pre></td></tr></table></figure>

<p>2）拷贝Mysql驱动包到libext目录下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ cp -a</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;mysql-connector-java-5.1.27&#x2F;mysql-connector-java-5.1.27-bin.jar</span><br><span class="line">.&#x2F;libext&#x2F;</span><br></pre></td></tr></table></figure>



<h3 id="将ext-zip拷贝到libext-目录下"><a href="#将ext-zip拷贝到libext-目录下" class="headerlink" title="将ext-zip拷贝到libext/目录下"></a>将ext-zip拷贝到libext/目录下</h3><p>ext是一个js框架，用于展示oozie前端页面：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ cp -a</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;cdh&#x2F;ext-zip libext&#x2F;</span><br></pre></td></tr></table></figure>



<h3 id="修改Oozie配置文件"><a href="#修改Oozie配置文件" class="headerlink" title="修改Oozie配置文件"></a>修改Oozie配置文件</h3><p><strong>oozie-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">属性：oozie.service.JPAService.jdbc.driver</span><br><span class="line"></span><br><span class="line">属性值：com.mysql.jdbc.Driver</span><br><span class="line"></span><br><span class="line">解释：JDBC的驱动</span><br><span class="line"></span><br><span class="line">属性：oozie.service.JPAService.jdbc.url</span><br><span class="line"></span><br><span class="line">属性值：jdbc:mysql://hadoop102:3306/oozie</span><br><span class="line"></span><br><span class="line">解释：oozie所需的数据库地址</span><br><span class="line"></span><br><span class="line">属性：oozie.service.JPAService.jdbc.username</span><br><span class="line"></span><br><span class="line">属性值：root</span><br><span class="line"></span><br><span class="line">解释：数据库用户名</span><br><span class="line"></span><br><span class="line">属性：oozie.service.JPAService.jdbc.password</span><br><span class="line"></span><br><span class="line">属性值：123456</span><br><span class="line"></span><br><span class="line">解释：数据库密码</span><br><span class="line"></span><br><span class="line">属性：oozie.service.HadoopAccessorService.hadoop.configurations</span><br><span class="line"></span><br><span class="line">属性值：*= /opt/module/cdh/hadoop-5.0-cdh5.6/etc/hadoop</span><br><span class="line"></span><br><span class="line">解释：让Oozie引用Hadoop的配置文件</span><br></pre></td></tr></table></figure>



<h3 id="在Mysql中创建Oozie的数据库"><a href="#在Mysql中创建Oozie的数据库" class="headerlink" title="在Mysql中创建Oozie的数据库"></a>在Mysql中创建Oozie的数据库</h3><p>进入Mysql并创建oozie数据库：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mysql -uroot -p123456</span><br><span class="line"></span><br><span class="line">mysql&gt; create database oozie;</span><br></pre></td></tr></table></figure>



<h3 id="初始化Oozie"><a href="#初始化Oozie" class="headerlink" title="初始化Oozie"></a>初始化Oozie</h3><p><strong>1) 上传Oozie目录下的yarn.tar.gz文件到HDFS：</strong></p>
<p>提示：yarn.tar.gz文件会自行解压</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozie-setup.sh</span><br><span class="line">sharelib create -fs hdfs:&#x2F;&#x2F;hadoop102:8020 -locallib</span><br><span class="line">oozie-sharelib-4.0.0-cdh5.6-yarn.tar.gz</span><br></pre></td></tr></table></figure>

<p>执行成功之后，去50070检查对应目录有没有文件生成。</p>
<p><strong>2) 创建oozie.sql文件</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;ooziedb.sh create -sqlfile oozie.sql -run</span><br></pre></td></tr></table></figure>

<p><strong>3) 打包项目，生成war包</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozie-setup.sh prepare-war</span><br></pre></td></tr></table></figure>



<h3 id="Oozie的启动与关闭"><a href="#Oozie的启动与关闭" class="headerlink" title="Oozie的启动与关闭"></a>Oozie的启动与关闭</h3><p>启动命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozied.sh start</span><br></pre></td></tr></table></figure>

<p>关闭命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozied.sh stop</span><br></pre></td></tr></table></figure>



<h3 id="访问Oozie的Web页面"><a href="#访问Oozie的Web页面" class="headerlink" title="访问Oozie的Web页面"></a>访问Oozie的Web页面</h3><p><a href="http://hadoop102:11000/oozie" target="_blank" rel="noopener">http://hadoop102:11000/oozie</a></p>
<h1 id="Oozie的使用"><a href="#Oozie的使用" class="headerlink" title="Oozie的使用"></a>Oozie的使用</h1><h2 id="案例一：Oozie调度shell脚本"><a href="#案例一：Oozie调度shell脚本" class="headerlink" title="案例一：Oozie调度shell脚本"></a>案例一：Oozie调度shell脚本</h2><p>目标：使用Oozie调度Shell脚本</p>
<p>分步实现：</p>
<p>1）创建工作目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ mkdir -p oozie-apps&#x2F;shell</span><br></pre></td></tr></table></figure>

<p>2）在oozie-apps/shell目录下创建两个文件——job.properties和workflow.xml文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 shell]$ touch workflow.xml</span><br><span class="line"></span><br><span class="line">[red@hadoop102 shell]$ touch job.properties</span><br></pre></td></tr></table></figure>

<p>3）编辑job.properties和workflow.xml文件</p>
<p><strong>job.properties</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#HDFS地址</span></span><br><span class="line"></span><br><span class="line"><span class="attr">nameNode</span>=<span class="string">hdfs://hadoop102:8020</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ResourceManager地址</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobTracker</span>=<span class="string">hadoop103:8032</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#队列名称</span></span><br><span class="line"></span><br><span class="line"><span class="attr">queueName</span>=<span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">examplesRoot</span>=<span class="string">oozie-apps</span></span><br><span class="line"></span><br><span class="line"><span class="meta">oozie.wf.application.path</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/shell</span></span><br></pre></td></tr></table></figure>

<p><strong>workflow.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.4"</span> <span class="attr">name</span>=<span class="string">"shell-wf"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--开始节点--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"shell-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--动作节点--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"shell-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--shell动作--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--要执行的脚本--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>mkdir<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>/opt/software/d<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--kill节点--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Shell action failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--结束节点--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>4）上传任务配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-5.0-cdh5.6&#x2F;bin&#x2F;hadoop fs -put oozie-apps&#x2F;</span><br><span class="line">&#x2F;user&#x2F;red</span><br></pre></td></tr></table></figure>

<p>5）执行任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozie job -oozie http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -config oozie-apps&#x2F;shell&#x2F;job.properties -run</span><br></pre></td></tr></table></figure>

<p>6）杀死某个任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozie job -oozie http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -kill 0000004-170425105153692-oozie-z-W</span><br></pre></td></tr></table></figure>



<h2 id="案例二：Oozie逻辑调度执行多个Job"><a href="#案例二：Oozie逻辑调度执行多个Job" class="headerlink" title="案例二：Oozie逻辑调度执行多个Job"></a>案例二：Oozie逻辑调度执行多个Job</h2><p>目标：使用Oozie执行多个Job调度</p>
<p>分步执行：</p>
<p>1）编辑job.properties和workflow.xml文件</p>
<p><strong>job.properties</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">nameNode</span>=<span class="string">hdfs://hadoop102:8020</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobTracker</span>=<span class="string">hadoop103:8032</span></span><br><span class="line"></span><br><span class="line"><span class="attr">queueName</span>=<span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">examplesRoot</span>=<span class="string">oozie-apps</span></span><br><span class="line"></span><br><span class="line"><span class="meta">oozie.wf.application.path</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/shells</span></span><br></pre></td></tr></table></figure>

<p><strong>workflow.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.4"</span> <span class="attr">name</span>=<span class="string">"shells-wf"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"p1-shell-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"p1-shell-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>mkdir<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>/opt/software/d1<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"p2-shell-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"p2-shell-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>mkdir<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>/opt/software/d2<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Shell action failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br><span class="line"></span><br><span class="line">补充：fork节点和join节点</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">fork</span> <span class="attr">name</span>=<span class="string">"forking"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">path</span> <span class="attr">start</span>=<span class="string">"firstparalleljob"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">path</span> <span class="attr">start</span>=<span class="string">"secondparalleljob"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">fork</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">join</span> <span class="attr">name</span>=<span class="string">"joining"</span> <span class="attr">to</span>=<span class="string">"nextaction"</span>/&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）上传任务配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin&#x2F;hadoop fs -rmr &#x2F;user&#x2F;red&#x2F;oozie-apps&#x2F;</span><br><span class="line"></span><br><span class="line">$ bin&#x2F;hadoop fs -put oozie-apps&#x2F;shells &#x2F;user&#x2F;red&#x2F;oozie-apps</span><br></pre></td></tr></table></figure>



<ol start="3">
<li>执行任务</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozie job -oozie http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -config oozie-apps&#x2F;shells&#x2F;job.properties -run</span><br></pre></td></tr></table></figure>



<h2 id="案例三：Oozie调度MapReduce任务"><a href="#案例三：Oozie调度MapReduce任务" class="headerlink" title="案例三：Oozie调度MapReduce任务"></a>案例三：Oozie调度MapReduce任务</h2><p>目标：使用Oozie调度MapReduce任务</p>
<p>分步执行：</p>
<p>1）找到一个可以运行的mapreduce任务的jar包（可以用官方的，也可以是自己写的）</p>
<p>2）拷贝官方模板到oozie-apps</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ cp -r &#x2F;opt&#x2F;module&#x2F;oozie-4.0.0-cdh5.6&#x2F;examples&#x2F;apps&#x2F;map-reduce&#x2F; oozie-apps&#x2F;</span><br></pre></td></tr></table></figure>

<p><strong>3)测试一下wordcount在yarn中的运行</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-5.0-cdh5.6&#x2F;bin&#x2F;yarn jar</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-5.0-cdh5.6&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-5.0-cdh5.6.jar</span><br><span class="line">wordcount &#x2F;input&#x2F; &#x2F;output&#x2F;</span><br></pre></td></tr></table></figure>

<p><strong>4) 配置map-reduce任务的job.properties以及workflow.xml</strong></p>
<p><strong>job.properties</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">nameNode</span>=<span class="string">hdfs://hadoop102:8020</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobTracker</span>=<span class="string">hadoop103:8032</span></span><br><span class="line"></span><br><span class="line"><span class="attr">queueName</span>=<span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">examplesRoot</span>=<span class="string">oozie-apps</span></span><br><span class="line"></span><br><span class="line"><span class="meta">oozie.wf.application.path</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/map-reduce/workflow.xml</span></span><br></pre></td></tr></table></figure>

<p><strong>workflow.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.2"</span> <span class="attr">name</span>=<span class="string">"map-reduce-wf"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"mr-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"mr-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">map-reduce</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">prepare</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">"$&#123;nameNode&#125;/output/"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">prepare</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置调度MR任务时，使用新的API --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.mapper.new-api<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.reducer.new-api<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Job Key输出类型 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.output.key.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.Text<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Job Value输出类型 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.output.value.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.IntWritable<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定输入路径 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.input.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/input/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定输出路径 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.output.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/output/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Map类 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.map.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.examples.WordCount$TokenizerMapper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Reduce类 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.reduce.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.examples.WordCount$IntSumReducer<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.tasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">map-reduce</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Map/Reduce failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>5）拷贝待执行的jar包到map-reduce的lib目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ cp -a &#x2F;opt</span><br><span class="line">&#x2F;module&#x2F;cdh&#x2F;hadoop-5.0-cdh5.6&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-5.0-cdh5.6.jar</span><br><span class="line">oozie-apps&#x2F;map-reduce&#x2F;lib</span><br></pre></td></tr></table></figure>

<p>6）上传配置好的app文件夹到HDFS</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-5.0-cdh5.6&#x2F;bin&#x2F;hdfs dfs -put</span><br><span class="line">oozie-apps&#x2F;map-reduce&#x2F; &#x2F;user&#x2F;admin&#x2F;oozie-apps</span><br></pre></td></tr></table></figure>

<p>7）执行任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozie job -oozie</span><br><span class="line">http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -config</span><br><span class="line">oozie-apps&#x2F;map-reduce&#x2F;job.properties -run</span><br></pre></td></tr></table></figure>



<h2 id="案例四：Oozie定时任务-循环任务"><a href="#案例四：Oozie定时任务-循环任务" class="headerlink" title="案例四：Oozie定时任务/循环任务"></a>案例四：Oozie定时任务/循环任务</h2><p>目标：Coordinator定时的周期性调度任务</p>
<p>分步实现：</p>
<ol>
<li><p>配置Linux时区以及时间服务器</p>
</li>
<li><p>检查系统当前时区：</p>
</li>
</ol>
<blockquote>
<h1 id="date-R"><a href="#date-R" class="headerlink" title="date -R"></a>date -R</h1></blockquote>
<p>注意：如果显示的时区不是+0800，删除localtime文件夹后，再关联一个正确时区的链接过去，命令如下：</p>
<blockquote>
<h1 id="rm-rf-etc-localtime"><a href="#rm-rf-etc-localtime" class="headerlink" title="rm -rf /etc/localtime"></a>rm -rf /etc/localtime</h1><p>ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</p>
</blockquote>
<p>同步时间：</p>
<blockquote>
<p>ntpdate pool.ntp.org</p>
</blockquote>
<p>修改NTP配置文件：</p>
<blockquote>
<p>vi /etc/ntp.conf</p>
</blockquote>
<p>去掉下面这行前面的# ,并把网段修改成自己的网段：</p>
<blockquote>
<p>restrict 19168.120 mask 255.255.255.0 nomodify notrap</p>
</blockquote>
<p>注释掉以下几行：</p>
<blockquote>
<p>#server 0.centos.pool.ntp.org</p>
<p>#server 1.centos.pool.ntp.org</p>
<p>#server centos.pool.ntp.org</p>
</blockquote>
<p>把下面两行前面的#号去掉,如果没有这两行内容,需要手动添加</p>
<blockquote>
<p>server 127.127.1.0 # local clock</p>
<p>fudge 127.127.1.0 stratum 10</p>
</blockquote>
<p>重启NTP服务：</p>
<blockquote>
<p>systemctl start ntpd.service，</p>
</blockquote>
<p>注意，如果是centOS7以下的版本，使用命令：service ntpd start</p>
<blockquote>
<p>systemctl enable ntpd.service，</p>
</blockquote>
<p>注意，如果是centOS7以下的版本，使用命令：chkconfig ntpd on</p>
<p>集群其他节点去同步这台时间服务器时间：</p>
<p>首先需要关闭这两台计算机的ntp服务</p>
<blockquote>
<p>systemctl stop ntpd.service，</p>
</blockquote>
<p>centOS7以下，则：service ntpd stop</p>
<blockquote>
<p>systemctl disable ntpd.service，</p>
</blockquote>
<p>centOS7以下，则：chkconfig ntpd off</p>
<blockquote>
<p>systemctl status ntpd，查看ntp服务状态</p>
<p>pgrep ntpd，查看ntp服务进程id</p>
</blockquote>
<p>同步第一台服务器linux01的时间：</p>
<blockquote>
<p>ntpdate hadoop102</p>
</blockquote>
<p>使用root用户制定计划任务,周期性同步时间：</p>
<blockquote>
<p>crontab -e */10 * * * * /usr/sbin/ntpdate hadoop102</p>
</blockquote>
<p>重启定时任务：</p>
<blockquote>
<p>systemctl restart crond.service，</p>
</blockquote>
<p>centOS7以下使用：service crond restart，</p>
<p>其他台机器的配置同理。</p>
<ol start="3">
<li>配置 oozie-site.xml文件</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">属性：oozie.processing.timezone</span><br><span class="line"></span><br><span class="line">属性值：GMT+0800</span><br></pre></td></tr></table></figure>

<p>解释：修改时区为东八区区时</p>
<p>注：该属性去oozie-default.xml中找到即可</p>
<ol start="4">
<li>修改js框架中的关于时间设置的代码(页面修改以后，省略此步骤，建议在页面修改)</li>
</ol>
<blockquote>
<p>$ vi<br>/opt/module/oozie-4.0.0-cdh5.6/oozie-server/webapps/oozie/oozie-console.js</p>
</blockquote>
<p>修改如下：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getTimeZone</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">Ext.state.Manager.setProvider(<span class="keyword">new</span> Ext.state.CookieProvider());</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> Ext.state.Manager.get(<span class="string">"TimezoneId"</span>,<span class="string">"GMT"</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>5）重启oozie服务，并重启浏览器（一定要注意清除缓存）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozied.sh stop</span><br><span class="line"></span><br><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozied.sh start</span><br></pre></td></tr></table></figure>

<p>6）拷贝官方模板配置定时任务</p>
<blockquote>
<p>$ cp -r examples/apps/cron/ oozie-apps/</p>
</blockquote>
<p>7）修改模板job.properties和coordinator.xml以及workflow.xml</p>
<p><strong>job.properties</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">pnameNode</span>=<span class="string">hdfs://hadoop102:8020</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobTracker</span>=<span class="string">hadoop103:8032</span></span><br><span class="line"></span><br><span class="line"><span class="attr">queueName</span>=<span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="attr">examplesRoot</span>=<span class="string">oozie-apps</span></span><br><span class="line"></span><br><span class="line"><span class="meta">oozie.coord.application.path</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#start：必须设置为未来时间，否则任务失败</span></span><br><span class="line"></span><br><span class="line"><span class="attr">start</span>=<span class="string">2019-11-01T11:05+0800</span></span><br><span class="line"></span><br><span class="line"><span class="attr">end</span>=<span class="string">2019-11-01T12:05+0800</span></span><br><span class="line"><span class="attr">workflowAppUri</span>=<span class="string">$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron</span></span><br></pre></td></tr></table></figure>

<p><strong>coordinator.xml（最小频率5分钟）</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">coordinator-app</span> <span class="attr">name</span>=<span class="string">"cron-coord"</span> <span class="attr">frequency</span>=<span class="string">"$&#123;coord:minutes(5)&#125;"</span></span></span><br><span class="line"><span class="tag"><span class="attr">start</span>=<span class="string">"$&#123;start&#125;"</span> <span class="attr">end</span>=<span class="string">"$&#123;end&#125;"</span> <span class="attr">timezone</span>=<span class="string">"GMT+0800"</span></span></span><br><span class="line"><span class="tag"><span class="attr">xmlns</span>=<span class="string">"uri:oozie:coordinator:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">workflow</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">app-path</span>&gt;</span>$&#123;workflowAppUri&#125;<span class="tag">&lt;/<span class="name">app-path</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>jobTracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>nameNode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>queueName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">coordinator-app</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>workflow.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.5"</span> <span class="attr">name</span>=<span class="string">"one-op-wf"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"shell-node"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"shell-node"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.2"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>p1.sh<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Shell action failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>8）上传配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;cdh&#x2F;hadoop-5.0-cdh5.6&#x2F;bin&#x2F;hdfs dfs -put oozie-apps&#x2F;cron&#x2F;</span><br><span class="line">&#x2F;user&#x2F;red&#x2F;oozie-apps</span><br></pre></td></tr></table></figure>

<p>9）启动任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 oozie-4.0.0-cdh5.6]$ bin&#x2F;oozie job -oozie</span><br><span class="line">http:&#x2F;&#x2F;hadoop102:11000&#x2F;oozie -config oozie-apps&#x2F;cron&#x2F;job.properties -run</span><br></pre></td></tr></table></figure>

<p>注意：oozie允许的最小执行任务的频率是5分钟</p>
<h1 id="常见问题总结"><a href="#常见问题总结" class="headerlink" title="常见问题总结"></a>常见问题总结</h1><p>1）Mysql权限配置</p>
<p>授权所有主机可以使用root用户操作所有数据库和数据表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; grant all on *.* to root@&#39;%&#39; identified by &#39;123456;</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line"></span><br><span class="line">mysql&gt; exit;</span><br></pre></td></tr></table></figure>

<p>2）workflow.xml配置的时候不要忽略file属性</p>
<p>3）jps查看进程时，注意有没有bootstrap</p>
<p>4）关闭oozie</p>
<p>如果bin/oozied.sh stop无法关闭，则可以使用kill -9<br>[pid]，之后oozie-server/temp/xxx.pid文件一定要删除。</p>
<p>5）Oozie重新打包时，一定要注意先关闭进程，删除对应文件夹下面的pid文件。（可以参考第4条目）</p>
<p>6）配置文件一定要生效</p>
<p>起始标签和结束标签无对应则不生效，配置文件的属性写错了，那么则执行默认的属性。</p>
<p>7）libext下边的jar存放于某个文件夹中，导致share/lib创建不成功。</p>
<p>8）调度任务时，找不到指定的脚本，可能是oozie-site.xml里面的Hadoop配置文件没有关联上。</p>
<p>9）修改Hadoop配置文件，需要重启集群。一定要记得scp到其他节点。</p>
<p>10）JobHistoryServer必须开启，集群要重启的。</p>
<p>11）Mysql配置如果没有生效的话，默认使用derby数据库。</p>
<p>12）在本地修改完成的job配置，必须重新上传到HDFS。</p>
<p>13）将HDFS中上传的oozie配置文件下载下来查看是否有错误。</p>
<p>14）Linux用户名和Hadoop的用户名不一致。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Oozie</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkStreaming-2</title>
    <url>/bigdata/SparkStreaming-2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="DStream转换"><a href="#DStream转换" class="headerlink" title="DStream转换"></a>DStream转换</h1><p>DStream上的操作与RDD的类似，分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。</p>
<h2 id="无状态转化操作"><a href="#无状态转化操作" class="headerlink" title="无状态转化操作"></a>无状态转化操作</h2><p>无状态转化操作就是把简单的RDD转化操作应用到每个批次上，也就是转化DStream中的每一个RDD。部分无状态转化操作列在了下表中。注意，针对键值对的DStream转化操作(比如 reduceByKey())要添加import StreamingContext._才能在Scala中使用。</p>
<p><img src="/bigdata/SparkStreaming-2/SparkStreaming14-1600198878207.jpg" alt="SparkStreaming14"></p>
<p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个DStream在内部是由许多RDD（批次）组成，且无状态转化操作是分别应用到每个RDD上的。</p>
<p>例如：reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。</p>
<h3 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h3><p>Transform允许DStream上执行任意的RDD-to-RDD函数。即使这些函数并没有在DStream的API中暴露出来，通过该函数可以方便的扩展Spark API。该函数每一批次调度一次。其实也就是对DStream中的RDD应用转换。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DStream_WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"queue"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf, <span class="type">Seconds</span>(<span class="number">5</span>));</span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resultDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDS.transform(</span><br><span class="line">      rdd =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> flatRDD: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = flatRDD.map((_, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> reduceRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_ + _)</span><br><span class="line">        reduceRDD</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line">    resultDS.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>两个流之间的join需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的RDD进行join，与两个RDD的join效果相同。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JoinTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"JoinTest"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.从端口获取数据创建流</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lineDStream1: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lineDStream2: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux2"</span>, <span class="number">8888</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.将两个流转换为KV类型</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToOneDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lineDStream1.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToADStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lineDStream2.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="string">"a"</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.流的JOIN</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> joinDStream: <span class="type">DStream</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">String</span>))] = wordToOneDStream.join(wordToADStream)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.打印</span></span><br><span class="line"></span><br><span class="line">  joinDStream.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.启动任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="有状态转化操作"><a href="#有状态转化操作" class="headerlink" title="有状态转化操作"></a>有状态转化操作</h2><h3 id="UpdateStateByKey"><a href="#UpdateStateByKey" class="headerlink" title="UpdateStateByKey"></a>UpdateStateByKey</h3><p>UpdateStateByKey原语用于记录历史记录，有时，我们需要在DStream中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey()为我们提供了对一个状态变量的访问，用于键值对形式的DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。</p>
<p>updateStateByKey() 的结果会是一个新的DStream，其内部的RDD 序列是由每个时间区间对应的(键，状态)对组成的。</p>
<p>updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，需要做下面两步：</p>
<ol>
<li><p>定义状态，状态可以是一个任意的数据类型。</p>
</li>
<li><p>定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。</p>
</li>
</ol>
<p>使用updateStateByKey需要对检查点目录进行配置，会使用检查点来保存状态。</p>
<p>更新版的wordcount</p>
<p>1) 编写代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DSream_State</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = socketDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 使用有状态操作保存数据 updateStateByKey</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"scp"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordToOneDS.updateStateByKey[<span class="type">Long</span>](</span><br><span class="line">      <span class="comment">// TODO 第一个参数表示相同key的value数据集合</span></span><br><span class="line">      <span class="comment">// TODO 第二个参数表示相同key的缓冲区的数据</span></span><br><span class="line">      (seq: <span class="type">Seq</span>[<span class="type">Int</span>], opt: <span class="type">Option</span>[<span class="type">Long</span>]) =&gt; &#123;</span><br><span class="line">        <span class="comment">// TODO 返回值表示更新后的缓冲区的值</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> newValue = opt.getOrElse(<span class="number">0</span>L) + seq.sum</span><br><span class="line">        <span class="type">Option</span>(newValue)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2) 启动程序并向9999端口发送数据</p>
<blockquote>
<p>nc -lk 9999</p>
<p>Hello World</p>
<p>Hello Scala</p>
</blockquote>
<p>3) 结果展示</p>
<blockquote>
<p>-——————————————</p>
<p>Time: 1504685175000 ms</p>
<p>-——————————————</p>
<p>-——————————————</p>
<p>Time: 1504685181000 ms</p>
<p>-——————————————</p>
<p>(shi,1)</p>
<p>(shui,1)</p>
<p>(ni,1)</p>
<p>-——————————————</p>
<p>Time: 1504685187000 ms</p>
<p>-——————————————</p>
<p>(shi,1)</p>
<p>(ma,1)</p>
<p>(hao,1)</p>
<p>(shui,1)</p>
</blockquote>
<h3 id="WindowOperations"><a href="#WindowOperations" class="headerlink" title="WindowOperations"></a>WindowOperations</h3><p>Window Operations可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。</p>
<p>窗口时长：计算内容的时间范围；</p>
<p>滑动步长：隔多久触发一次计算。</p>
<p><font color="red"><strong>注意：这两者都必须为采集周期大小的整数倍。</strong></font></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WorldCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Split each line into words</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">// Count each word in each batch</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b),<span class="type">Seconds</span>(<span class="number">12</span>), <span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line"></span><br><span class="line">  wordCounts.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.start()       <span class="comment">// Start the computation</span></span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h4><p><img src="/bigdata/SparkStreaming-2/image-20200914120823766-1600198906010.png" alt="image-20200914120823766"></p>
<h4 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h4><p><img src="/bigdata/SparkStreaming-2/image-20200914120922340-1600198907495.png" alt="image-20200914120922340"></p>
<p>关于Window的操作还有如下方法：</p>
<p>（1）window(windowLength, slideInterval): 基于对源DStream窗化的批次进行计算返回一个新的Dstream；</p>
<p>（2）countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//countByWindow</span></span><br><span class="line">ssc.checkpoint(<span class="string">"scp"</span>)</span><br><span class="line"><span class="keyword">val</span> countDS: <span class="type">DStream</span>[<span class="type">Long</span>] = socketDS.countByWindow(<span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">print(countDS)</span><br></pre></td></tr></table></figure>

<p>（3）reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；</p>
<p>（4）reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的DStream上调用此函数，会返回一个新(K,V)对的DStream，此处通过对滑动窗口中批次数据使用reduce函数来整合每个key的value值。(func 两两聚合)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//reduceByKeyAndWindow</span></span><br><span class="line"><span class="keyword">val</span> wordToOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDS.flatMap(_.split(<span class="string">" "</span>)).map((_ , <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> reduceByKeyAndWindowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKeyAndWindow(</span><br><span class="line">  (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x + y, <span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">3</span>)</span><br><span class="line">)</span><br><span class="line">reduceByKeyAndWindowDS.print()</span><br></pre></td></tr></table></figure>

<p>（5）reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的reduce值都是通过用前一个窗的reduce值来递增计算。通过reduce进入到滑动窗口数据并”反向reduce”离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys的“加”“减”计数。<strong>通过前边介绍可以想到，这个函数只适用于”可逆的reduce函数”，也就是这些reduce函数有相应的”反reduce”函数(以参数invFunc形式传入)。</strong>如前述函数，reduce任务的数量通过可选参数来配置。</p>
<p><img src="/bigdata/SparkStreaming-2/SparkStreaming15-1600198926526.jpg" alt="SparkStreaming15"></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map(logEntry =&gt; (logEntry.getIpAddress(), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> ipCountDStream = ipDStream.reduceByKeyAndWindow(</span><br><span class="line"> &#123;(x, y) =&gt; x + y&#125;,</span><br><span class="line"> &#123;(x, y) =&gt; x - y&#125;,</span><br><span class="line"> <span class="type">Seconds</span>(<span class="number">30</span>),</span><br><span class="line"> <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>


<p> //加上新进入窗口的批次中的元素 //移除离开窗口的老批次中的元素 //窗口时长// 滑动步长</p>
<p>countByWindow()和countByValueAndWindow()作为对数据进行计数操作的简写。countByWindow()返回一个表示每个窗口中元素个数的DStream，而countByValueAndWindow()返回的DStream则包含窗口中每个值的个数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map&#123;entry =&gt; entry.getIpAddress()&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ipAddressRequestCount = ipDStream.countByValueAndWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>)) </span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> requestCount = accessLogsDStream.countByWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>



<h1 id="DStream输出"><a href="#DStream输出" class="headerlink" title="DStream输出"></a>DStream输出</h1><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与RDD中的惰性求值类似，如果一个DStream及其派生出的DStream都没有被执行输出操作，那么这些DStream就都不会被求值。如果StreamingContext中没有设定输出操作，整个context就都不会启动。</p>
<p>输出操作如下：</p>
<p><font color="blue">print()</font>：在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫print()。</p>
<p><font color="blue">saveAsTextFiles(prefix, [suffix])</font>：以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”。</p>
<p><font color="blue">saveAsObjectFiles(prefix, [suffix])</font>：以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles . 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python中目前不可用。</p>
<p><font color="blue">saveAsHadoopFiles(prefix, [suffix])</font>：将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”。Python API 中目前不可用。</p>
<p><font color="blue">foreachRDD(func)</font>：这是最通用的输出操作，即将函数 func 用于产生于 stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">DriverManager</span>, <span class="type">PreparedStatement</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DStream_Output</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">//将数据保存到mysql数据库中</span></span><br><span class="line">    socketDS.foreachRDD(</span><br><span class="line">      rdd =&gt;&#123;</span><br><span class="line">        rdd.foreach(</span><br><span class="line">          data =&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = data.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> id: <span class="type">Int</span> = datas(<span class="number">0</span>).toInt</span><br><span class="line">            <span class="keyword">val</span> name: <span class="type">String</span> = datas(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">val</span> age: <span class="type">Int</span> = datas(<span class="number">0</span>).toInt</span><br><span class="line"></span><br><span class="line">            <span class="comment">//TODO 加载数据库驱动</span></span><br><span class="line">            <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">            <span class="comment">// TODO 建立链接和操作对象</span></span><br><span class="line">            <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(</span><br><span class="line">              <span class="string">"jdbc:mysql://3306/studenttest"</span>,</span><br><span class="line">              <span class="string">"root"</span>,</span><br><span class="line">              <span class="string">"000000"</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">val</span> sql = <span class="string">"insert into student(id,name,age) values (?,?,?)"</span></span><br><span class="line">            <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = conn.prepareStatement(sql)</span><br><span class="line">            statement.setInt(<span class="number">1</span>,id)</span><br><span class="line">            statement.setString(<span class="number">2</span>,name)</span><br><span class="line">            statement.setInt(<span class="number">3</span>,age)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// TODO 操作数据</span></span><br><span class="line">            statement.executeUpdate()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// TODO 关闭连接</span></span><br><span class="line">            statement.close()</span><br><span class="line">            conn.close()</span><br><span class="line"></span><br><span class="line">          &#125;)</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong><font color="red">注：但是在真实场景中数据传播速度快，传递量大，所以上述代码并不适合用在实际操作中。</font></strong></p>
<p><strong><font color="red">原因：数据来一次建立一次链接，数据库链接创建太多，显然是不合理的。在数据量巨大的情况下用连接池也是不合理的，处理不过来。</font></strong></p>
<p>通用的输出操作foreachRDD()，它用来对DStream中的RDD运行任意计算。这和transform() 有些类似，都可以让我们访问任意RDD。在foreachRDD()中，可以重用我们在Spark中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如MySQL的外部数据库中。 </p>
<p>注意：</p>
<p>1) 连接不能写在driver层面（序列化）</p>
<p>2) 如果写在foreach则每个RDD中的每一条数据都创建，得不偿失；</p>
<p>3) 增加foreachPartition，在分区创建（获取）。</p>
<p>rdd.foreachPartition()：以分区为单位进行遍历，不需要返回</p>
<p>rdd.foreachPartitions()：以分区为单位进行转换，需要返回</p>
<h1 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h1><p>流式任务需要7*24小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。</p>
<p>使用外部文件系统来控制内部程序关闭。</p>
<p>MonitorStop</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.net.<span class="type">URI</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FileSystem</span>, <span class="type">Path</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">StreamingContext</span>, <span class="type">StreamingContextState</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MonitorStop</span>(<span class="params">ssc: <span class="type">StreamingContext</span></span>) <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fs: <span class="type">FileSystem</span> = <span class="type">FileSystem</span>.get(<span class="keyword">new</span> <span class="type">URI</span>(<span class="string">"hdfs://linux1:9000"</span>), <span class="keyword">new</span> <span class="type">Configuration</span>(), <span class="string">"red"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//优雅关闭判断条件</span></span><br><span class="line"><span class="comment">//stop方法不能放在driver的主线程中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//一般标志不在Driver端，在第三方软件中 eg:redis/zk/mysql/hdfs</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span></span><br><span class="line"></span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">catch</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">InterruptedException</span> =&gt;</span><br><span class="line"></span><br><span class="line">     e.printStackTrace()</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//获取当前线程状态</span></span><br><span class="line">   <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = ssc.getState</span><br><span class="line"> </span><br><span class="line"><span class="comment">// TODO 设置标记，让当前关闭线程可以访问，可以动态改变状态</span></span><br><span class="line">   <span class="keyword">val</span> bool: <span class="type">Boolean</span> = fs.exists(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">"hdfs://linux1:9000/stopSpark"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//为了防止有新线程了之后SparkStreaming直接关闭，所以应该加一个判断条件，</span></span><br><span class="line"><span class="comment">// 并且循环判断，避免判断一次之后不再判断</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (bool) &#123;</span><br><span class="line"><span class="comment">//判断当前状态，如果当前不是激活状态的话根本不用关闭</span></span><br><span class="line">    <span class="keyword">if</span> (state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span>) &#123;</span><br><span class="line"></span><br><span class="line">     ssc.stop(stopSparkContext = <span class="literal">true</span>, stopGracefully = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">     <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>SparkTest</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">createSSC</span></span>(): _root_.org.apache.spark.streaming.<span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> update: (<span class="type">Seq</span>[<span class="type">Int</span>], <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; <span class="type">Some</span>[<span class="type">Int</span>] = (values: <span class="type">Seq</span>[<span class="type">Int</span>], status: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line"></span><br><span class="line">   <span class="comment">//当前批次内容的计算</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> sum: <span class="type">Int</span> = values.sum</span><br><span class="line"></span><br><span class="line">   <span class="comment">//取出状态信息中上一次状态</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> lastStatu: <span class="type">Int</span> = status.getOrElse(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">   <span class="type">Some</span>(sum + lastStatu)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[4]"</span>).setAppName(<span class="string">"SparkTest"</span>)</span><br><span class="line">  <span class="comment">//****设置优雅的关闭*****</span></span><br><span class="line"></span><br><span class="line">  sparkConf.set(<span class="string">"spark.streaming.stopGracefullyOnShutdown"</span>, <span class="string">"true"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> line: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line">  <span class="keyword">val</span> word: <span class="type">DStream</span>[<span class="type">String</span>] = line.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">  <span class="keyword">val</span> wordAndOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = word.map((_, <span class="number">1</span>))</span><br><span class="line">  <span class="keyword">val</span> wordAndCount: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.updateStateByKey(update)</span><br><span class="line"> wordAndCount.print()</span><br><span class="line">ssc</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck"</span>, () =&gt; createSSC())</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">MonitorStop</span>(ssc)).start()</span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>优雅关闭是要判断当前有没有数据没有处理完，如果有，先把当前数据处理完再关闭。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo博客的一些坑</title>
    <url>/hexo/hexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h4 id="设置统计人数和字数："><a href="#设置统计人数和字数：" class="headerlink" title="设置统计人数和字数："></a>设置统计人数和字数：</h4><p>1.之前一定要先下好相应插件。发现如果这个插件没下好就直接运行，报错报的是有方法过时而不是说没有下插件。</p>
<h4 id="设置自动打开左面目录的时候，如果发现文章左面目录打不开："><a href="#设置自动打开左面目录的时候，如果发现文章左面目录打不开：" class="headerlink" title="设置自动打开左面目录的时候，如果发现文章左面目录打不开："></a>设置自动打开左面目录的时候，如果发现文章左面目录打不开：</h4><p>1.字数超过一定限制（具体多少不清楚），将文章分成两篇文章之后自动就好了。</p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo避雷</tag>
      </tags>
  </entry>
  <entry>
    <title>常用软件安装地址汇总</title>
    <url>/tools/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E5%9C%B0%E5%9D%80%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="ETL工具"><a href="#ETL工具" class="headerlink" title="ETL工具"></a>ETL工具</h1><ol>
<li><p>datastage：<a href="http://pan.baidu.com/share/link?shareid=172289&amp;uk=67437475" target="_blank" rel="noopener">http://pan.baidu.com/share/link?shareid=172289&amp;uk=67437475</a></p>
</li>
<li><p>informatica：<a href="http://pan.baidu.com/share/link?shareid=183201&amp;uk=67437475" target="_blank" rel="noopener">http://pan.baidu.com/share/link?shareid=183201&amp;uk=67437475</a></p>
</li>
<li><p>kettle：<a href="http://kettle.pentaho.com/" target="_blank" rel="noopener">http://kettle.pentaho.com/</a>  </p>
</li>
<li><p>ODI: <a href="http://www.oracle.com/technetwork/cn/testcontent/index-091026-zhs.html" target="_blank" rel="noopener">www.oracle.com/technetwork/cn/testcontent/index-091026-zhs.html</a></p>
</li>
<li><p>Cognos：<a href="http://pan.baidu.com/share/link?shareid=172288&amp;uk=67437475" target="_blank" rel="noopener">http://pan.baidu.com/share/link?shareid=172288&amp;uk=67437475</a></p>
</li>
<li><p>beeload: <a href="http://www.livbee.com/" target="_blank" rel="noopener">www.livbee.com</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>工具集</category>
      </categories>
      <tags>
        <tag>软件安装网址</tag>
      </tags>
  </entry>
  <entry>
    <title>git常用命令</title>
    <url>/tools/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="基本"><a href="#基本" class="headerlink" title="基本"></a>基本</h1><ol>
<li><p>git config - - 可以配置git的参数，可以使用 git config –list查看已经配置的git参数。<br>其中有三个级别的保存位置，</p>
<ol>
<li>–system（本系统）</li>
<li>–global（当前用户，全局）</li>
<li>–local（本地配置，当前目录）</li>
<li>默认使用–local</li>
</ol>
<p>配置用户名及邮箱<br>git config –global user.name “Cilerry”<br>git config –global user.email <a href="mailto:xxxxxxx@qq.com">xxxxxxx@qq.com</a></p>
<p>查看用户名和邮箱地址：</p>
<p>git config user.name</p>
<p>git config user.email</p>
<p>修改用户名和邮箱地址</p>
<p>git config –global user.name “xxxx”</p>
<p>git config –global user.email “xxxx”</p>
</li>
<li><p>git init - - 初始化代码仓库</p>
</li>
<li><p>git clone - - 克隆远程仓库<br>PS:如果希望在克隆的时候，自己定义要新建的项目目录名称，可以在上面的命令末尾指定新的名字：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git clone git:&#x2F;&#x2F;github.com&#x2F;schacon&#x2F;grit.git mygrit</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<ol>
<li>git add - - 把需要提交的所有修改放到暂存区（Stage）<ul>
<li>git add file – 提交指定文件</li>
<li>git add . || git add -A – 提交所有文件</li>
<li>git add *.js – 提交所有.js格式文件</li>
<li>git add -f file – 强制添加</li>
</ul>
</li>
<li>git diff - - 查看当前目录的所有修改(#当暂存区中没有文件时，git diff比较的是，工作区中的文件与上次提交到版本库中的文件。<br>#当暂存区中有文件时，git diff则比较的是，当前工作区中的文件与暂存区中的文)<ul>
<li>git diff HEAD - - file – 比较工作区中的文件与版本库中文件的差异。HEAD指向的是版本库中的当前版本，而file指的是当前工作区中的文件。</li>
</ul>
</li>
<li>git commit -m “message” - - 提交代码</li>
</ol>
<h1 id="本地"><a href="#本地" class="headerlink" title="本地"></a>本地</h1><p>git status ——git status 查看当前状态 </p>
<p>git log ——看你commit的日志</p>
<p>git checkout ——检出到工作区、切换或创建分支</p>
<ul>
<li>–track origin/dev  切换到远程dev分支</li>
<li>-b dev  建立一个新的本地分支dev</li>
<li>dev  切换到本地dev分支</li>
<li>branch_1.0/master  切换到branch_1.0/master分支</li>
<li>-index  从暂存区拷贝文件至工作区</li>
<li>-b master_copy  从当前分支创建新分支master_copy并检出 </li>
</ul>
<p>git rm ——  删除文件</p>
<ul>
<li>[file name] 删除一个文件</li>
<li>文件名(包括路径) 从git中删除指定文件支</li>
</ul>
<p>git reset ——重置改变分支“游标”指向</p>
<p>git revert ——反转提交</p>
<h1 id="与远程交互"><a href="#与远程交互" class="headerlink" title="与远程交互"></a>与远程交互</h1><p>git remote ——远程版本库管理</p>
<p>git fetch —— 获取远程版本库的提交</p>
<p>git pull —— 拉回远程版本库的提交</p>
<p>git push —— 推送至远程版本库</p>
<h1 id="与不同分支交互"><a href="#与不同分支交互" class="headerlink" title="与不同分支交互"></a>与不同分支交互</h1><p>git branch ——查看本地所有分支</p>
<p>git checkout ——检出到工作区、切换或创建分支</p>
<p>git rebase —— 分支变基</p>
<p>git merge ——分支合并</p>
<p>git cherry-pick —— 提交拣选</p>
]]></content>
      <categories>
        <category>工具集</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>常用端口号一览（二）</title>
    <url>/tools/%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7%E4%B8%80%E8%A7%88%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>TCP 端口（动态端口）</p>
<table>
<thead>
<tr>
<th>端口类型</th>
<th>端口号</th>
<th>注释</th>
</tr>
</thead>
<tbody><tr>
<td>TCP</td>
<td>1024</td>
<td>NetSpy.698(YAI)</td>
</tr>
<tr>
<td>TCP</td>
<td>1025</td>
<td>NetSpy.698,Unused Windows Services Block</td>
</tr>
<tr>
<td>TCP</td>
<td>1026</td>
<td>Unused Windows Services Block</td>
</tr>
<tr>
<td>TCP</td>
<td>1027</td>
<td>Unused Windows Services Block</td>
</tr>
<tr>
<td>TCP</td>
<td>1028</td>
<td>Unused Windows Services Block</td>
</tr>
<tr>
<td>TCP</td>
<td>1029</td>
<td>Unused Windows Services Block</td>
</tr>
<tr>
<td>TCP</td>
<td>1030</td>
<td>Unused Windows Services Block</td>
</tr>
<tr>
<td>TCP</td>
<td>1033</td>
<td>Netspy</td>
</tr>
<tr>
<td>TCP</td>
<td>1035</td>
<td>Multidropper</td>
</tr>
<tr>
<td>TCP</td>
<td>1042</td>
<td>Bla</td>
</tr>
<tr>
<td>TCP</td>
<td>1045</td>
<td>Rasmin</td>
</tr>
<tr>
<td>TCP</td>
<td>1047</td>
<td>GateCrasher</td>
</tr>
<tr>
<td>TCP</td>
<td>1050</td>
<td>MiniCommand</td>
</tr>
<tr>
<td>TCP</td>
<td>1059</td>
<td>nimreg</td>
</tr>
<tr>
<td>TCP</td>
<td>1069</td>
<td>Backdoor.TheefServer.202</td>
</tr>
<tr>
<td>TCP</td>
<td>1070</td>
<td>Voice,Psyber Stream Server,Streaming Audio Trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>1080</td>
<td>Wingate,Worm.BugBear.B,Worm.Novarg.B</td>
</tr>
<tr>
<td>TCP</td>
<td>1090</td>
<td>Xtreme,VDOLive</td>
</tr>
<tr>
<td>TCP</td>
<td>1092</td>
<td>LoveGate</td>
</tr>
<tr>
<td>TCP</td>
<td>1095</td>
<td>Rat</td>
</tr>
<tr>
<td>TCP</td>
<td>1097</td>
<td>Rat</td>
</tr>
<tr>
<td>TCP</td>
<td>1098</td>
<td>Rat</td>
</tr>
<tr>
<td>TCP</td>
<td>1099</td>
<td>Rat</td>
</tr>
<tr>
<td>TCP</td>
<td>1110</td>
<td>nfsd-keepalive</td>
</tr>
<tr>
<td>TCP</td>
<td>1111</td>
<td>Backdoor.AIMVision</td>
</tr>
<tr>
<td>TCP</td>
<td>1155</td>
<td>Network File Access</td>
</tr>
<tr>
<td>TCP</td>
<td>1170</td>
<td>Psyber Stream Server,Streaming Audio trojan,Voice</td>
</tr>
<tr>
<td>TCP</td>
<td>1200</td>
<td>NoBackO</td>
</tr>
<tr>
<td>TCP</td>
<td>1201</td>
<td>NoBackO</td>
</tr>
<tr>
<td>TCP</td>
<td>1207</td>
<td>Softwar</td>
</tr>
<tr>
<td>TCP</td>
<td>1212</td>
<td>Nirvana,Visul Killer</td>
</tr>
<tr>
<td>TCP</td>
<td>1234</td>
<td>Ultors</td>
</tr>
<tr>
<td>TCP</td>
<td>1243</td>
<td>BackDoor-G,SubSeven,SubSeven Apocalypse</td>
</tr>
<tr>
<td>TCP</td>
<td>1245</td>
<td>VooDoo Doll</td>
</tr>
<tr>
<td>TCP</td>
<td>1269</td>
<td>Mavericks Matrix</td>
</tr>
<tr>
<td>TCP</td>
<td>1313</td>
<td>Nirvana</td>
</tr>
<tr>
<td>TCP</td>
<td>1349</td>
<td>BioNet</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">1433</font></strong></td>
<td><strong><font color="blue">Microsoft SQL服务</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>1441</td>
<td>Remote Storm</td>
</tr>
<tr>
<td>TCP</td>
<td>1492</td>
<td>FTP99CMP(BackOriffice.FTP)</td>
</tr>
<tr>
<td>TCP</td>
<td>1503</td>
<td>NetMeeting T.120</td>
</tr>
<tr>
<td>TCP</td>
<td>1509</td>
<td>Psyber Streaming Server</td>
</tr>
<tr>
<td>TCP</td>
<td>1600</td>
<td>Shivka-Burka</td>
</tr>
<tr>
<td>TCP</td>
<td>1688</td>
<td>Key Management Service(密钥管理服务)</td>
</tr>
<tr>
<td>TCP</td>
<td>1703</td>
<td>Exloiter 1.1</td>
</tr>
<tr>
<td>TCP</td>
<td>1720</td>
<td>NetMeeting H.233 call Setup</td>
</tr>
<tr>
<td>TCP</td>
<td>1723</td>
<td>VPN 网关（PPTP）</td>
</tr>
<tr>
<td>TCP</td>
<td>1731</td>
<td>NetMeeting音频调用控制</td>
</tr>
<tr>
<td>TCP</td>
<td>1807</td>
<td>SpySender</td>
</tr>
<tr>
<td>TCP</td>
<td>1966</td>
<td>Fake FTP 2000</td>
</tr>
<tr>
<td>TCP</td>
<td>1976</td>
<td>Custom port</td>
</tr>
<tr>
<td>TCP</td>
<td>1981</td>
<td>Shockrave</td>
</tr>
<tr>
<td>TCP</td>
<td>1990</td>
<td>stun-p1 cisco STUN Priority 1 port</td>
</tr>
<tr>
<td>TCP</td>
<td>1990</td>
<td>stun-p1 cisco STUN Priority 1 port</td>
</tr>
<tr>
<td>TCP</td>
<td>1991</td>
<td>stun-p2 cisco STUN Priority 2 port</td>
</tr>
<tr>
<td>TCP</td>
<td>1992</td>
<td>stun-p3 cisco STUN Priority 3 port,ipsendmsg IPsendmsg</td>
</tr>
<tr>
<td>TCP</td>
<td>1993</td>
<td>snmp-tcp-port cisco SNMP TCP port</td>
</tr>
<tr>
<td>TCP</td>
<td>1994</td>
<td>stun-port cisco serial tunnel port</td>
</tr>
<tr>
<td>TCP</td>
<td>1995</td>
<td>perf-port cisco perf port</td>
</tr>
<tr>
<td>TCP</td>
<td>1996</td>
<td>tr-rsrb-port cisco Remote SRB port</td>
</tr>
<tr>
<td>TCP</td>
<td>1997</td>
<td>gdp-port cisco Gateway Discovery Protocol</td>
</tr>
<tr>
<td>TCP</td>
<td>1998</td>
<td>x25-svc-port cisco X.25 service (XOT)</td>
</tr>
<tr>
<td>TCP</td>
<td>1999</td>
<td>BackDoor,TransScout</td>
</tr>
<tr>
<td>TCP</td>
<td>2000</td>
<td>Der Spaeher,INsane Network</td>
</tr>
<tr>
<td>TCP</td>
<td>2002</td>
<td>W32. Beagle .AX @mm</td>
</tr>
<tr>
<td>TCP</td>
<td>2001</td>
<td>Transmisson scout</td>
</tr>
<tr>
<td>TCP</td>
<td>2002</td>
<td>Transmisson scout</td>
</tr>
<tr>
<td>TCP</td>
<td>2003</td>
<td>Transmisson scout</td>
</tr>
<tr>
<td>TCP</td>
<td>2004</td>
<td>Transmisson scout</td>
</tr>
<tr>
<td>TCP</td>
<td>2005</td>
<td>TTransmisson scout</td>
</tr>
<tr>
<td>TCP</td>
<td>2011</td>
<td>cypress</td>
</tr>
<tr>
<td>TCP</td>
<td>2015</td>
<td>raid-cs</td>
</tr>
<tr>
<td>TCP</td>
<td>2023</td>
<td>Ripper,Pass Ripper,Hack City Ripper Pro</td>
</tr>
<tr>
<td>TCP</td>
<td>2049</td>
<td>NFS</td>
</tr>
<tr>
<td>TCP</td>
<td>2115</td>
<td>Bugs</td>
</tr>
<tr>
<td>TCP</td>
<td>2121</td>
<td>Nirvana</td>
</tr>
<tr>
<td>TCP</td>
<td>2140</td>
<td>Deep Throat,The Invasor</td>
</tr>
<tr>
<td>TCP</td>
<td>2155</td>
<td>Nirvana</td>
</tr>
<tr>
<td>TCP</td>
<td>2208</td>
<td>RuX</td>
</tr>
<tr>
<td>TCP</td>
<td>2255</td>
<td>Illusion Mailer</td>
</tr>
<tr>
<td>TCP</td>
<td>2283</td>
<td>HVL Rat5</td>
</tr>
<tr>
<td>TCP</td>
<td>2300</td>
<td>PC Explorer</td>
</tr>
<tr>
<td>TCP</td>
<td>2311</td>
<td>Studio54</td>
</tr>
<tr>
<td>TCP</td>
<td>2556</td>
<td>Worm.Bbeagle.q</td>
</tr>
<tr>
<td>TCP</td>
<td>2565</td>
<td>Striker</td>
</tr>
<tr>
<td>TCP</td>
<td>2583</td>
<td>WinCrash</td>
</tr>
<tr>
<td>TCP</td>
<td>2600</td>
<td>Digital RootBeer</td>
</tr>
<tr>
<td>TCP</td>
<td>2716</td>
<td>Prayer Trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>2745</td>
<td>Worm.BBeagle.k</td>
</tr>
<tr>
<td>TCP</td>
<td>2773</td>
<td>Backdoor,SubSeven</td>
</tr>
<tr>
<td>TCP</td>
<td>2774</td>
<td>SubSeven2.1&amp;2.2</td>
</tr>
<tr>
<td>TCP</td>
<td>2801</td>
<td>Phineas Phucker</td>
</tr>
<tr>
<td>TCP</td>
<td>2989</td>
<td>Rat</td>
</tr>
<tr>
<td>TCP</td>
<td>3024</td>
<td>WinCrash trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>3127</td>
<td>Worm.Novarg</td>
</tr>
<tr>
<td>TCP</td>
<td>3128</td>
<td>RingZero,Worm.Novarg.B</td>
</tr>
<tr>
<td>TCP</td>
<td>3129</td>
<td>Masters Paradise</td>
</tr>
<tr>
<td>TCP</td>
<td>3150</td>
<td>Deep Throat,The Invasor</td>
</tr>
<tr>
<td>TCP</td>
<td>3198</td>
<td>Worm.Novarg</td>
</tr>
<tr>
<td>TCP</td>
<td>3210</td>
<td>SchoolBus</td>
</tr>
<tr>
<td>TCP</td>
<td>3332</td>
<td>Worm.Cycle.a</td>
</tr>
<tr>
<td>TCP</td>
<td>3333</td>
<td>Prosiak</td>
</tr>
<tr>
<td>TCP</td>
<td>3389</td>
<td>超级终端（远程桌面）</td>
</tr>
<tr>
<td>TCP</td>
<td>3456</td>
<td>Terror</td>
</tr>
<tr>
<td>TCP</td>
<td>3459</td>
<td>Eclipse 2000</td>
</tr>
<tr>
<td>TCP</td>
<td>3700</td>
<td>Portal of Doom</td>
</tr>
<tr>
<td>TCP</td>
<td>3791</td>
<td>Eclypse</td>
</tr>
<tr>
<td>TCP</td>
<td>3801</td>
<td>Eclypse</td>
</tr>
<tr>
<td>TCP</td>
<td>3996</td>
<td>Portal of Doom,RemoteAnything</td>
</tr>
<tr>
<td>TCP</td>
<td>4000</td>
<td>腾讯QQ客户端</td>
</tr>
<tr>
<td>TCP</td>
<td>4060</td>
<td>Portal of Doom,RemoteAnything</td>
</tr>
<tr>
<td>TCP</td>
<td>4092</td>
<td>WinCrash</td>
</tr>
<tr>
<td>TCP</td>
<td>4242</td>
<td>VHM</td>
</tr>
<tr>
<td>TCP</td>
<td>4267</td>
<td>SubSeven2.1&amp;2.2</td>
</tr>
<tr>
<td>TCP</td>
<td>4321</td>
<td>BoBo</td>
</tr>
<tr>
<td>TCP</td>
<td>4444</td>
<td>Prosiak,Swift remote</td>
</tr>
<tr>
<td>TCP</td>
<td>4500</td>
<td>W32.HLLW.Tufas</td>
</tr>
<tr>
<td>TCP</td>
<td>4567</td>
<td>File Nail</td>
</tr>
<tr>
<td>TCP</td>
<td>4590</td>
<td>ICQTrojan</td>
</tr>
<tr>
<td>TCP</td>
<td>4899</td>
<td>Remote Administrator服务器</td>
</tr>
<tr>
<td>TCP</td>
<td>4950</td>
<td>ICQTrojan</td>
</tr>
<tr>
<td>TCP</td>
<td>5000</td>
<td>WindowsXP服务器，Blazer 5,Bubbel,Back Door Setup,Sockets de Troie</td>
</tr>
<tr>
<td>TCP</td>
<td>5001</td>
<td>Back Door Setup,Sockets de Troie</td>
</tr>
<tr>
<td>TCP</td>
<td>5002</td>
<td>cd00r,Shaft</td>
</tr>
<tr>
<td>TCP</td>
<td>5011</td>
<td>One of the Last Trojans (OOTLT)</td>
</tr>
<tr>
<td>TCP</td>
<td>5025</td>
<td>WM Remote KeyLogger</td>
</tr>
<tr>
<td>TCP</td>
<td>5031</td>
<td>Firehotcker,Metropolitan,NetMetro</td>
</tr>
<tr>
<td>TCP</td>
<td>5032</td>
<td>Metropolitan</td>
</tr>
<tr>
<td>TCP</td>
<td>5190</td>
<td>ICQ Query</td>
</tr>
<tr>
<td>TCP</td>
<td>5321</td>
<td>Firehotcker</td>
</tr>
<tr>
<td>TCP</td>
<td>5333</td>
<td>Backage Trojan Box 3</td>
</tr>
<tr>
<td>TCP</td>
<td>5343</td>
<td>WCrat</td>
</tr>
<tr>
<td>TCP</td>
<td>5400</td>
<td>Blade Runner,BackConstruction1.2</td>
</tr>
<tr>
<td>TCP</td>
<td>5401</td>
<td>Blade Runner,Back Construction</td>
</tr>
<tr>
<td>TCP</td>
<td>5402</td>
<td>Blade Runner,Back Construction</td>
</tr>
<tr>
<td>TCP</td>
<td>5471</td>
<td>WinCrash</td>
</tr>
<tr>
<td>TCP</td>
<td>5512</td>
<td>Illusion Mailer</td>
</tr>
<tr>
<td>TCP</td>
<td>5521</td>
<td>Illusion Mailer</td>
</tr>
<tr>
<td>TCP</td>
<td>5550</td>
<td>Xtcp,INsane Network</td>
</tr>
<tr>
<td>TCP</td>
<td>5554</td>
<td>Worm.Sasser</td>
</tr>
<tr>
<td>TCP</td>
<td>5555</td>
<td>ServeMe</td>
</tr>
<tr>
<td>TCP</td>
<td>5556</td>
<td>BO Facil</td>
</tr>
<tr>
<td>TCP</td>
<td>5557</td>
<td>BO Facil</td>
</tr>
<tr>
<td>TCP</td>
<td>5569</td>
<td>Robo-Hack</td>
</tr>
<tr>
<td>TCP</td>
<td>5598</td>
<td>BackDoor 2.03</td>
</tr>
<tr>
<td>TCP</td>
<td>5631</td>
<td>PCAnyWhere data</td>
</tr>
<tr>
<td>TCP</td>
<td>5632</td>
<td>PCAnyWhere</td>
</tr>
<tr>
<td>TCP</td>
<td>5637</td>
<td>PC Crasher</td>
</tr>
<tr>
<td>TCP</td>
<td>5638</td>
<td>PC Crasher</td>
</tr>
<tr>
<td>TCP</td>
<td>5698</td>
<td>BackDoor</td>
</tr>
<tr>
<td>TCP</td>
<td>5714</td>
<td>Wincrash3</td>
</tr>
<tr>
<td>TCP</td>
<td>5741</td>
<td>WinCrash3</td>
</tr>
<tr>
<td>TCP</td>
<td>5742</td>
<td>WinCrash</td>
</tr>
<tr>
<td>TCP</td>
<td>5760</td>
<td>Portmap Remote Root Linux Exploit</td>
</tr>
<tr>
<td>TCP</td>
<td>5880</td>
<td>Y3K RAT</td>
</tr>
<tr>
<td>TCP</td>
<td>5881</td>
<td>Y3K RAT</td>
</tr>
<tr>
<td>TCP</td>
<td>5882</td>
<td>Y3K RAT</td>
</tr>
<tr>
<td>TCP</td>
<td>5888</td>
<td>Y3K RAT</td>
</tr>
<tr>
<td>TCP</td>
<td>5889</td>
<td>Y3K RAT</td>
</tr>
<tr>
<td>TCP</td>
<td>5900</td>
<td>WinVnc</td>
</tr>
<tr>
<td>TCP</td>
<td>6000</td>
<td>Backdoor.AB</td>
</tr>
<tr>
<td>TCP</td>
<td>6006</td>
<td>Noknok8</td>
</tr>
<tr>
<td>TCP</td>
<td>6129</td>
<td>Dameware Nt Utilities服务器</td>
</tr>
<tr>
<td>TCP</td>
<td>6272</td>
<td>SecretService</td>
</tr>
<tr>
<td>TCP</td>
<td>6267</td>
<td>广外女生</td>
</tr>
<tr>
<td>TCP</td>
<td>6400</td>
<td>Backdoor.AB,The Thing</td>
</tr>
<tr>
<td>TCP</td>
<td>6500</td>
<td>Devil 1.03</td>
</tr>
<tr>
<td>TCP</td>
<td>6661</td>
<td>Teman</td>
</tr>
<tr>
<td>TCP</td>
<td>6666</td>
<td>TCPshell.c</td>
</tr>
<tr>
<td>TCP</td>
<td>6667</td>
<td>NT Remote Control,Wise 播放器接收端口</td>
</tr>
<tr>
<td>TCP</td>
<td>6668</td>
<td>Wise Video广播端口</td>
</tr>
<tr>
<td>TCP</td>
<td>6669</td>
<td>Vampyre</td>
</tr>
<tr>
<td>TCP</td>
<td>6670</td>
<td>DeepThroat,iPhone</td>
</tr>
<tr>
<td>TCP</td>
<td>6671</td>
<td>Deep Throat 3.0</td>
</tr>
<tr>
<td>TCP</td>
<td>6711</td>
<td>SubSeven</td>
</tr>
<tr>
<td>TCP</td>
<td>6712</td>
<td>SubSeven1.x</td>
</tr>
<tr>
<td>TCP</td>
<td>6713</td>
<td>SubSeven</td>
</tr>
<tr>
<td>TCP</td>
<td>6723</td>
<td>Mstream</td>
</tr>
<tr>
<td>TCP</td>
<td>6767</td>
<td>NT Remote Control</td>
</tr>
<tr>
<td>TCP</td>
<td>6771</td>
<td>DeepThroat</td>
</tr>
<tr>
<td>TCP</td>
<td>6776</td>
<td>BackDoor-G,SubSeven,2000 Cracks</td>
</tr>
<tr>
<td>TCP</td>
<td>6777</td>
<td>Worm.BBeagle</td>
</tr>
<tr>
<td>TCP</td>
<td>6789</td>
<td>Doly Trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>6838</td>
<td>Mstream</td>
</tr>
<tr>
<td>TCP</td>
<td>6883</td>
<td>DeltaSource</td>
</tr>
<tr>
<td>TCP</td>
<td>6912</td>
<td>Shit Heep</td>
</tr>
<tr>
<td>TCP</td>
<td>6939</td>
<td>Indoctrination</td>
</tr>
<tr>
<td>TCP</td>
<td>6969</td>
<td>GateCrasher,Priority,IRC 3</td>
</tr>
<tr>
<td>TCP</td>
<td>6970</td>
<td>RealAudio,GateCrasher</td>
</tr>
<tr>
<td>TCP</td>
<td>7000</td>
<td>Remote Grab,NetMonitor,SubSeven1.x</td>
</tr>
<tr>
<td>TCP</td>
<td>7001</td>
<td>Freak88</td>
</tr>
<tr>
<td>TCP</td>
<td>7201</td>
<td>NetMonitor</td>
</tr>
<tr>
<td>TCP</td>
<td>7215</td>
<td>BackDoor-G,SubSeven</td>
</tr>
<tr>
<td>TCP</td>
<td>7001</td>
<td>Freak88,Freak2k</td>
</tr>
<tr>
<td>TCP</td>
<td>7300</td>
<td>NetMonitor</td>
</tr>
<tr>
<td>TCP</td>
<td>7301</td>
<td>NetMonitor</td>
</tr>
<tr>
<td>TCP</td>
<td>7306</td>
<td>NetMonitor,NetSpy 1.0</td>
</tr>
<tr>
<td>TCP</td>
<td>7307</td>
<td>NetMonitor,ProcSpy</td>
</tr>
<tr>
<td>TCP</td>
<td>7308</td>
<td>NetMonitor,X Spy</td>
</tr>
<tr>
<td>TCP</td>
<td>7323</td>
<td>Sygate服务器端</td>
</tr>
<tr>
<td>TCP</td>
<td>7424</td>
<td>Host Control</td>
</tr>
<tr>
<td>TCP</td>
<td>7511</td>
<td>聪明基因</td>
</tr>
<tr>
<td>TCP</td>
<td>7597</td>
<td>Qaz</td>
</tr>
<tr>
<td>TCP</td>
<td>7609</td>
<td>Snid X2</td>
</tr>
<tr>
<td>TCP</td>
<td>7626</td>
<td>冰河</td>
</tr>
<tr>
<td>TCP</td>
<td>7777</td>
<td>The Thing</td>
</tr>
<tr>
<td>TCP</td>
<td>7789</td>
<td>Back Door Setup,ICQKiller</td>
</tr>
<tr>
<td>TCP</td>
<td>7983</td>
<td>Mstream</td>
</tr>
<tr>
<td>TCP</td>
<td>8000</td>
<td>腾讯OICQ服务器端，XDMA</td>
</tr>
<tr>
<td>TCP</td>
<td>8010</td>
<td>Wingate,Logfile</td>
</tr>
<tr>
<td>TCP</td>
<td>8011</td>
<td>WAY2.4</td>
</tr>
<tr>
<td>TCP</td>
<td>8080</td>
<td>WWW 代理，Ring Zero,Chubo,Worm.Novarg.B</td>
</tr>
<tr>
<td>TCP</td>
<td>8102</td>
<td>网络神偷</td>
</tr>
<tr>
<td>TCP</td>
<td>8181</td>
<td>W32.Erkez.D@mm</td>
</tr>
<tr>
<td>TCP</td>
<td>8520</td>
<td>W32.Socay.Worm</td>
</tr>
<tr>
<td>TCP</td>
<td>8594</td>
<td>I-Worm/Bozori.a</td>
</tr>
<tr>
<td>TCP</td>
<td>8787</td>
<td>BackOfrice 2000</td>
</tr>
<tr>
<td>TCP</td>
<td>8888</td>
<td>Winvnc</td>
</tr>
<tr>
<td>TCP</td>
<td>8897</td>
<td>Hack Office,Armageddon</td>
</tr>
<tr>
<td>TCP</td>
<td>8989</td>
<td>Recon</td>
</tr>
<tr>
<td>TCP</td>
<td>9000</td>
<td>Netministrator</td>
</tr>
<tr>
<td>TCP</td>
<td>9325</td>
<td>Mstream</td>
</tr>
<tr>
<td>TCP</td>
<td>9400</td>
<td>InCommand 1.0</td>
</tr>
<tr>
<td>TCP</td>
<td>9401</td>
<td>InCommand 1.0</td>
</tr>
<tr>
<td>TCP</td>
<td>9402</td>
<td>InCommand 1.0</td>
</tr>
<tr>
<td>TCP</td>
<td>9872</td>
<td>Portal of Doom</td>
</tr>
<tr>
<td>TCP</td>
<td>9873</td>
<td>Portal of Doom</td>
</tr>
<tr>
<td>TCP</td>
<td>9874</td>
<td>Portal of Doom</td>
</tr>
<tr>
<td>TCP</td>
<td>9875</td>
<td>Portal of Doom</td>
</tr>
<tr>
<td>TCP</td>
<td>9876</td>
<td>Cyber Attacker</td>
</tr>
<tr>
<td>TCP</td>
<td>9878</td>
<td>TransScout</td>
</tr>
<tr>
<td>TCP</td>
<td>9989</td>
<td>Ini-Killer</td>
</tr>
<tr>
<td>TCP</td>
<td>9898</td>
<td>Worm.Win32.Dabber.a</td>
</tr>
<tr>
<td>TCP</td>
<td>9999</td>
<td>Prayer Trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>10067</td>
<td>Portal of Doom</td>
</tr>
<tr>
<td>TCP</td>
<td>10080</td>
<td>Worm.Novarg.B</td>
</tr>
<tr>
<td>TCP</td>
<td>10084</td>
<td>Syphillis</td>
</tr>
<tr>
<td>TCP</td>
<td>10085</td>
<td>Syphillis</td>
</tr>
<tr>
<td>TCP</td>
<td>10086</td>
<td>Syphillis</td>
</tr>
<tr>
<td>TCP</td>
<td>10101</td>
<td>BrainSpy</td>
</tr>
<tr>
<td>TCP</td>
<td>10167</td>
<td>Portal Of Doom</td>
</tr>
<tr>
<td>TCP</td>
<td>10168</td>
<td>Worm.Supnot.78858.c,Worm.LovGate.T</td>
</tr>
<tr>
<td>TCP</td>
<td>10520</td>
<td>Acid Shivers</td>
</tr>
<tr>
<td>TCP</td>
<td>10607</td>
<td>Coma trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>10666</td>
<td>Ambush</td>
</tr>
<tr>
<td>TCP</td>
<td>11000</td>
<td>Senna Spy</td>
</tr>
<tr>
<td>TCP</td>
<td>11050</td>
<td>Host Control</td>
</tr>
<tr>
<td>TCP</td>
<td>11051</td>
<td>Host Control</td>
</tr>
<tr>
<td>TCP</td>
<td>11223</td>
<td>Progenic,Hack ’99KeyLogger</td>
</tr>
<tr>
<td>TCP</td>
<td>11831</td>
<td>TROJ_LATINUS.SVR</td>
</tr>
<tr>
<td>TCP</td>
<td>12076</td>
<td>Gjamer,MSH.104b</td>
</tr>
<tr>
<td>TCP</td>
<td>12223</td>
<td>Hack’99 KeyLogger</td>
</tr>
<tr>
<td>TCP</td>
<td>12345</td>
<td>GabanBus,NetBus 1.6/1.7,Pie Bill Gates,X-bill</td>
</tr>
<tr>
<td>TCP</td>
<td>12346</td>
<td>GabanBus,NetBus 1.6/1.7,X-bill</td>
</tr>
<tr>
<td>TCP</td>
<td>12349</td>
<td>BioNet</td>
</tr>
<tr>
<td>TCP</td>
<td>12361</td>
<td>Whack-a-mole</td>
</tr>
<tr>
<td>TCP</td>
<td>12362</td>
<td>Whack-a-mole</td>
</tr>
<tr>
<td>TCP</td>
<td>12363</td>
<td>Whack-a-mole</td>
</tr>
<tr>
<td>TCP</td>
<td>12378</td>
<td>W32/Gibe@MM</td>
</tr>
<tr>
<td>TCP</td>
<td>12456</td>
<td>NetBus</td>
</tr>
<tr>
<td>TCP</td>
<td>12623</td>
<td>DUN Control</td>
</tr>
<tr>
<td>TCP</td>
<td>12624</td>
<td>Buttman</td>
</tr>
<tr>
<td>TCP</td>
<td>12631</td>
<td>WhackJob,WhackJob.NB1.7</td>
</tr>
<tr>
<td>TCP</td>
<td>12701</td>
<td>Eclipse2000</td>
</tr>
<tr>
<td>TCP</td>
<td>12754</td>
<td>Mstream</td>
</tr>
<tr>
<td>TCP</td>
<td>13000</td>
<td>Senna Spy</td>
</tr>
<tr>
<td>TCP</td>
<td>13010</td>
<td>Hacker Brazil</td>
</tr>
<tr>
<td>TCP</td>
<td>13013</td>
<td>Psychward</td>
</tr>
<tr>
<td>TCP</td>
<td>13223</td>
<td>Tribal Voice的聊天程序PowWow</td>
</tr>
<tr>
<td>TCP</td>
<td>13700</td>
<td>Kuang2 The Virus</td>
</tr>
<tr>
<td>TCP</td>
<td>14456</td>
<td>Solero</td>
</tr>
<tr>
<td>TCP</td>
<td>14500</td>
<td>PC Invader</td>
</tr>
<tr>
<td>TCP</td>
<td>14501</td>
<td>PC Invader</td>
</tr>
<tr>
<td>TCP</td>
<td>14502</td>
<td>PC Invader</td>
</tr>
<tr>
<td>TCP</td>
<td>14503</td>
<td>PC Invader</td>
</tr>
<tr>
<td>TCP</td>
<td>15000</td>
<td>NetDaemon 1.0</td>
</tr>
<tr>
<td>TCP</td>
<td>15092</td>
<td>Host Control</td>
</tr>
<tr>
<td>TCP</td>
<td>15104</td>
<td>Mstream</td>
</tr>
<tr>
<td>TCP</td>
<td>16484</td>
<td>Mosucker</td>
</tr>
<tr>
<td>TCP</td>
<td>16660</td>
<td>Stacheldraht (DDoS)</td>
</tr>
<tr>
<td>TCP</td>
<td>16772</td>
<td>ICQ Revenge</td>
</tr>
<tr>
<td>TCP</td>
<td>16959</td>
<td>Priority</td>
</tr>
<tr>
<td>TCP</td>
<td>16969</td>
<td>Priority</td>
</tr>
<tr>
<td>TCP</td>
<td>17027</td>
<td>提供广告服务的Conducent”adbot”共享软件</td>
</tr>
<tr>
<td>TCP</td>
<td>17166</td>
<td>Mosaic</td>
</tr>
<tr>
<td>TCP</td>
<td>17300</td>
<td>Kuang2 The Virus</td>
</tr>
<tr>
<td>TCP</td>
<td>17490</td>
<td>CrazyNet</td>
</tr>
<tr>
<td>TCP</td>
<td>17500</td>
<td>CrazyNet</td>
</tr>
<tr>
<td>TCP</td>
<td>17569</td>
<td>Infector 1.4.x + 1.6.x</td>
</tr>
<tr>
<td>TCP</td>
<td>17777</td>
<td>Nephron</td>
</tr>
<tr>
<td>TCP</td>
<td>18753</td>
<td>Shaft (DDoS)</td>
</tr>
<tr>
<td>TCP</td>
<td>19191</td>
<td>蓝色火焰</td>
</tr>
<tr>
<td>TCP</td>
<td>19864</td>
<td>ICQ Revenge</td>
</tr>
<tr>
<td>TCP</td>
<td>20000</td>
<td>Millennium II (GrilFriend)</td>
</tr>
<tr>
<td>TCP</td>
<td>20001</td>
<td>Millennium II (GrilFriend)</td>
</tr>
<tr>
<td>TCP</td>
<td>20002</td>
<td>AcidkoR</td>
</tr>
<tr>
<td>TCP</td>
<td>20034</td>
<td>NetBus 2 Pro</td>
</tr>
<tr>
<td>TCP</td>
<td>20168</td>
<td>Lovgate</td>
</tr>
<tr>
<td>TCP</td>
<td>20203</td>
<td>Logged,Chupacabra</td>
</tr>
<tr>
<td>TCP</td>
<td>20331</td>
<td>Bla</td>
</tr>
<tr>
<td>TCP</td>
<td>20432</td>
<td>Shaft (DDoS)</td>
</tr>
<tr>
<td>TCP</td>
<td>20808</td>
<td>Worm.LovGate.v.QQ</td>
</tr>
<tr>
<td>TCP</td>
<td>21335</td>
<td>Tribal Flood Network,Trinoo</td>
</tr>
<tr>
<td>TCP</td>
<td>21544</td>
<td>Schwindler 1.82,GirlFriend</td>
</tr>
<tr>
<td>TCP</td>
<td>21554</td>
<td>Schwindler 1.82,GirlFriend,Exloiter 1.0.1.2</td>
</tr>
<tr>
<td>TCP</td>
<td>22222</td>
<td>Prosiak,RuXUploader2.0</td>
</tr>
<tr>
<td>TCP</td>
<td>22784</td>
<td>Backdoor.Intruzzo</td>
</tr>
<tr>
<td>TCP</td>
<td>23432</td>
<td>Asylum 0.1.3</td>
</tr>
<tr>
<td>TCP</td>
<td>23444</td>
<td>网络公牛</td>
</tr>
<tr>
<td>TCP</td>
<td>23456</td>
<td>Evil FTP,Ugly FTP,WhackJob</td>
</tr>
<tr>
<td>TCP</td>
<td>23476</td>
<td>Donald Dick</td>
</tr>
<tr>
<td>TCP</td>
<td>23477</td>
<td>Donald Dick</td>
</tr>
<tr>
<td>TCP</td>
<td>23777</td>
<td>INet Spy</td>
</tr>
<tr>
<td>TCP</td>
<td>26274</td>
<td>Delta</td>
</tr>
<tr>
<td>TCP</td>
<td>26681</td>
<td>Spy Voice</td>
</tr>
<tr>
<td>TCP</td>
<td>27374</td>
<td>Sub Seven 2.0+,Backdoor.Baste</td>
</tr>
<tr>
<td>TCP</td>
<td>27444</td>
<td>Tribal Flood Network,Trinoo</td>
</tr>
<tr>
<td>TCP</td>
<td>27665</td>
<td>Tribal Flood Network,Trinoo</td>
</tr>
<tr>
<td>TCP</td>
<td>29431</td>
<td>Hack Attack</td>
</tr>
<tr>
<td>TCP</td>
<td>29432</td>
<td>Hack Attack</td>
</tr>
<tr>
<td>TCP</td>
<td>29104</td>
<td>Host Control</td>
</tr>
<tr>
<td>TCP</td>
<td>29559</td>
<td>TROJ_LATINUS.SVR</td>
</tr>
<tr>
<td>TCP</td>
<td>29891</td>
<td>The Unexplained</td>
</tr>
<tr>
<td>TCP</td>
<td>30001</td>
<td>Terr0r32</td>
</tr>
<tr>
<td>TCP</td>
<td>30003</td>
<td>Death,Lamers Death</td>
</tr>
<tr>
<td>TCP</td>
<td>30029</td>
<td>AOL trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>30100</td>
<td>NetSphere 1.27a,NetSphere 1.31</td>
</tr>
<tr>
<td>TCP</td>
<td>30101</td>
<td>NetSphere 1.31,NetSphere 1.27a</td>
</tr>
<tr>
<td>TCP</td>
<td>30102</td>
<td>NetSphere 1.27a,NetSphere 1.31</td>
</tr>
<tr>
<td>TCP</td>
<td>30103</td>
<td>NetSphere 1.31</td>
</tr>
<tr>
<td>TCP</td>
<td>30303</td>
<td>Sockets de Troie</td>
</tr>
<tr>
<td>TCP</td>
<td>30722</td>
<td>W32.Esbot.A</td>
</tr>
<tr>
<td>TCP</td>
<td>30947</td>
<td>Intruse</td>
</tr>
<tr>
<td>TCP</td>
<td>30999</td>
<td>Kuang2</td>
</tr>
<tr>
<td>TCP</td>
<td>31336</td>
<td>Bo Whack</td>
</tr>
<tr>
<td>TCP</td>
<td>31337</td>
<td>Baron Night,BO client,BO2,Bo Facil,BackFire,Back Orifice,DeepBO,Freak2k,NetSpy</td>
</tr>
<tr>
<td>TCP</td>
<td>31338</td>
<td>NetSpy,Back Orifice,DeepBO</td>
</tr>
<tr>
<td>TCP</td>
<td>31339</td>
<td>NetSpy DK</td>
</tr>
<tr>
<td>TCP</td>
<td>31554</td>
<td>Schwindler</td>
</tr>
<tr>
<td>TCP</td>
<td>31666</td>
<td>BOWhack</td>
</tr>
<tr>
<td>TCP</td>
<td>31778</td>
<td>Hack Attack</td>
</tr>
<tr>
<td>TCP</td>
<td>31785</td>
<td>Hack Attack</td>
</tr>
<tr>
<td>TCP</td>
<td>31787</td>
<td>Hack Attack</td>
</tr>
<tr>
<td>TCP</td>
<td>31789</td>
<td>Hack Attack</td>
</tr>
<tr>
<td>TCP</td>
<td>31791</td>
<td>Hack Attack</td>
</tr>
<tr>
<td>TCP</td>
<td>31792</td>
<td>Hack Attack</td>
</tr>
<tr>
<td>TCP</td>
<td>32100</td>
<td>PeanutBrittle</td>
</tr>
<tr>
<td>TCP</td>
<td>32418</td>
<td>Acid Battery</td>
</tr>
<tr>
<td>TCP</td>
<td>33333</td>
<td>Prosiak,Blakharaz 1.0</td>
</tr>
<tr>
<td>TCP</td>
<td>33577</td>
<td>Son Of Psychward</td>
</tr>
<tr>
<td>TCP</td>
<td>33777</td>
<td>Son Of Psychward</td>
</tr>
<tr>
<td>TCP</td>
<td>33911</td>
<td>Spirit 2001a</td>
</tr>
<tr>
<td>TCP</td>
<td>34324</td>
<td>BigGluck,TN,Tiny Telnet Server</td>
</tr>
<tr>
<td>TCP</td>
<td>34555</td>
<td>Trin00 (Windows) (DDoS)</td>
</tr>
<tr>
<td>TCP</td>
<td>35555</td>
<td>Trin00 (Windows) (DDoS)</td>
</tr>
<tr>
<td>TCP</td>
<td>36794</td>
<td>Worm.Bugbear-A</td>
</tr>
<tr>
<td>TCP</td>
<td>37651</td>
<td>YAT</td>
</tr>
<tr>
<td>TCP</td>
<td>40412</td>
<td>The Spy</td>
</tr>
<tr>
<td>TCP</td>
<td>40421</td>
<td>Agent 40421,Masters Paradise.96</td>
</tr>
<tr>
<td>TCP</td>
<td>40422</td>
<td>Masters Paradise</td>
</tr>
<tr>
<td>TCP</td>
<td>40423</td>
<td>Masters Paradise.97</td>
</tr>
<tr>
<td>TCP</td>
<td>40425</td>
<td>Masters Paradise</td>
</tr>
<tr>
<td>TCP</td>
<td>40426</td>
<td>Masters Paradise 3.x</td>
</tr>
<tr>
<td>TCP</td>
<td>41666</td>
<td>Remote Boot</td>
</tr>
<tr>
<td>TCP</td>
<td>43210</td>
<td>Schoolbus 1.6/2.0</td>
</tr>
<tr>
<td>TCP</td>
<td>44444</td>
<td>Delta Source</td>
</tr>
<tr>
<td>TCP</td>
<td>44445</td>
<td>Happypig</td>
</tr>
<tr>
<td>TCP</td>
<td>45576</td>
<td>未知代理</td>
</tr>
<tr>
<td>TCP</td>
<td>47252</td>
<td>Prosiak</td>
</tr>
<tr>
<td>TCP</td>
<td>47262</td>
<td>Delta</td>
</tr>
<tr>
<td>TCP</td>
<td>47878</td>
<td>BirdSpy2</td>
</tr>
<tr>
<td>TCP</td>
<td>49301</td>
<td>Online Keylogger</td>
</tr>
<tr>
<td>TCP</td>
<td>50505</td>
<td>Sockets de Troie</td>
</tr>
<tr>
<td>TCP</td>
<td>50766</td>
<td>Fore,Schwindler</td>
</tr>
<tr>
<td>TCP</td>
<td>51966</td>
<td>CafeIni</td>
</tr>
<tr>
<td>TCP</td>
<td>53001</td>
<td>Remote Windows Shutdown</td>
</tr>
<tr>
<td>TCP</td>
<td>53217</td>
<td>Acid Battery 2000</td>
</tr>
<tr>
<td>TCP</td>
<td>54283</td>
<td>Back Door-G,Sub7</td>
</tr>
<tr>
<td>TCP</td>
<td>54320</td>
<td>Back Orifice 2000,Sheep</td>
</tr>
<tr>
<td>TCP</td>
<td>54321</td>
<td>School Bus .69-1.11,Sheep,BO2K</td>
</tr>
<tr>
<td>TCP</td>
<td>57341</td>
<td>NetRaider</td>
</tr>
<tr>
<td>TCP</td>
<td>58008</td>
<td>BackDoor.Tron</td>
</tr>
<tr>
<td>TCP</td>
<td>58009</td>
<td>BackDoor.Tron</td>
</tr>
<tr>
<td>TCP</td>
<td>58339</td>
<td>ButtFunnel</td>
</tr>
<tr>
<td>TCP</td>
<td>59211</td>
<td>BackDoor.DuckToy</td>
</tr>
<tr>
<td>TCP</td>
<td>60000</td>
<td>Deep Throat</td>
</tr>
<tr>
<td>TCP</td>
<td>60068</td>
<td>Xzip 6000068</td>
</tr>
<tr>
<td>TCP</td>
<td>60268</td>
<td>DaYangou_bigppig</td>
</tr>
<tr>
<td>TCP</td>
<td>60411</td>
<td>Connection</td>
</tr>
<tr>
<td>TCP</td>
<td>60606</td>
<td>TROJ_BCKDOR.G2.A</td>
</tr>
<tr>
<td>TCP</td>
<td>61466</td>
<td>Telecommando</td>
</tr>
<tr>
<td>TCP</td>
<td>61603</td>
<td>Bunker-kill</td>
</tr>
<tr>
<td>TCP</td>
<td>63485</td>
<td>Bunker-kill</td>
</tr>
<tr>
<td>TCP</td>
<td>65000</td>
<td>Devil,DDoS</td>
</tr>
<tr>
<td>TCP</td>
<td>65432</td>
<td>Th3tr41t0r,The Traitor</td>
</tr>
<tr>
<td>TCP</td>
<td>65530</td>
<td>TROJ_WINMITE.10</td>
</tr>
<tr>
<td>TCP</td>
<td>65535</td>
<td>RC,Adore Worm/Linux</td>
</tr>
</tbody></table>
<p>整理来自于：今天超市开门了吗</p>
<p>（非本人整理）</p>
]]></content>
      <categories>
        <category>工具集</category>
      </categories>
      <tags>
        <tag>常用端口</tag>
      </tags>
  </entry>
  <entry>
    <title>常用端口号一览（三）</title>
    <url>/tools/%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7%E4%B8%80%E8%A7%88%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>UDP 端口（静态端口）</p>
<table>
<thead>
<tr>
<th>端口类型</th>
<th>端口号</th>
<th>注释</th>
</tr>
</thead>
<tbody><tr>
<td>UDP</td>
<td>1</td>
<td>Sockets des Troie</td>
</tr>
<tr>
<td>UDP</td>
<td>9</td>
<td>Chargen</td>
</tr>
<tr>
<td>UDP</td>
<td>19</td>
<td>Chargen</td>
</tr>
<tr>
<td>UDP</td>
<td>69</td>
<td>Pasana</td>
</tr>
<tr>
<td>UDP</td>
<td>80</td>
<td>Penrox</td>
</tr>
<tr>
<td>UDP</td>
<td>371</td>
<td>ClearCase版本管理软件</td>
</tr>
<tr>
<td><strong><font color="blue">UDP</font></strong></td>
<td><strong><font color="blue">445</font></strong></td>
<td><strong><font color="blue">公共Internet文件系统（CIFS)</font></strong></td>
</tr>
<tr>
<td>UDP</td>
<td>500</td>
<td>Internet密钥交换（IP安全性 ,IKE)</td>
</tr>
<tr>
<td>UDP</td>
<td>520</td>
<td>Rip</td>
</tr>
</tbody></table>
<p>整理来自于：今天超市开门了吗</p>
<p>（非本人整理）</p>
]]></content>
      <categories>
        <category>工具集</category>
      </categories>
      <tags>
        <tag>常用端口</tag>
      </tags>
  </entry>
  <entry>
    <title>常用端口号一览（四）</title>
    <url>/tools/%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7%E4%B8%80%E8%A7%88%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>UDP 端口（动态端口）</p>
<table>
<thead>
<tr>
<th>端口类型</th>
<th>端口号</th>
<th>注释</th>
</tr>
</thead>
<tbody><tr>
<td>UDP</td>
<td>1025</td>
<td>Maverick’s Matrix 1.2 - 2.0</td>
</tr>
<tr>
<td>UDP</td>
<td>1026</td>
<td>Remote Explorer 2000</td>
</tr>
<tr>
<td>UDP</td>
<td>1027</td>
<td>UC聊天软件，Trojan.Huigezi.e</td>
</tr>
<tr>
<td>UDP</td>
<td>1028</td>
<td>3721上网助手（用途不明，建议用户警惕！），KiLo,SubSARI</td>
</tr>
<tr>
<td>UDP</td>
<td>1029</td>
<td>SubSARI</td>
</tr>
<tr>
<td>UDP</td>
<td>1031</td>
<td>Xot</td>
</tr>
<tr>
<td>UDP</td>
<td>1032</td>
<td>Akosch4</td>
</tr>
<tr>
<td>UDP</td>
<td>1104</td>
<td>RexxRave</td>
</tr>
<tr>
<td>UDP</td>
<td>1111</td>
<td>Daodan</td>
</tr>
<tr>
<td>UDP</td>
<td>1116</td>
<td>Lurker</td>
</tr>
<tr>
<td>UDP</td>
<td>1122</td>
<td>Last 2000,Singularity</td>
</tr>
<tr>
<td>UDP</td>
<td>1183</td>
<td>Cyn,SweetHeart</td>
</tr>
<tr>
<td>UDP</td>
<td>1200</td>
<td>NoBackO</td>
</tr>
<tr>
<td>UDP</td>
<td>1201</td>
<td>NoBackO</td>
</tr>
<tr>
<td>UDP</td>
<td>1342</td>
<td>BLA trojan</td>
</tr>
<tr>
<td>UDP</td>
<td>1344</td>
<td>Ptakks</td>
</tr>
<tr>
<td>UDP</td>
<td>1349</td>
<td>BO dll</td>
</tr>
<tr>
<td>UDP</td>
<td>1561</td>
<td>MuSka52</td>
</tr>
<tr>
<td>UDP</td>
<td>1701</td>
<td>VPN网关（L2TP）</td>
</tr>
<tr>
<td>UDP</td>
<td>1772</td>
<td>NetControle</td>
</tr>
<tr>
<td>UDP</td>
<td>1978</td>
<td>Slapper</td>
</tr>
<tr>
<td>UDP</td>
<td>1985</td>
<td>Black Diver</td>
</tr>
<tr>
<td>UDP</td>
<td>2000</td>
<td>A-trojan,Fear,Force,GOTHIC Intruder,Last 2000,Real 2000</td>
</tr>
<tr>
<td>UDP</td>
<td>2001</td>
<td>Scalper</td>
</tr>
<tr>
<td>UDP</td>
<td>2002</td>
<td>Slapper</td>
</tr>
<tr>
<td>UDP</td>
<td>2015</td>
<td>raid-cs</td>
</tr>
<tr>
<td>UDP</td>
<td>2018</td>
<td>rellpack</td>
</tr>
<tr>
<td>UDP</td>
<td>2130</td>
<td>Mini BackLash</td>
</tr>
<tr>
<td>UDP</td>
<td>2140</td>
<td>Deep Throat,Foreplay,The Invasor</td>
</tr>
<tr>
<td>UDP</td>
<td>2222</td>
<td>SweetHeart,Way</td>
</tr>
<tr>
<td>UDP</td>
<td>2339</td>
<td>Voice Spy</td>
</tr>
<tr>
<td>UDP</td>
<td>2702</td>
<td>Black Diver</td>
</tr>
<tr>
<td>UDP</td>
<td>2989</td>
<td>RAT</td>
</tr>
<tr>
<td>UDP</td>
<td>3150</td>
<td>Deep Throat</td>
</tr>
<tr>
<td>UDP</td>
<td>3215</td>
<td>XHX</td>
</tr>
<tr>
<td>UDP</td>
<td>3333</td>
<td>Daodan</td>
</tr>
<tr>
<td>UDP</td>
<td>3801</td>
<td>Eclypse</td>
</tr>
<tr>
<td>UDP</td>
<td>3996</td>
<td>Remote Anything</td>
</tr>
<tr>
<td>UDP</td>
<td>4128</td>
<td>RedShad</td>
</tr>
<tr>
<td>UDP</td>
<td>4156</td>
<td>Slapper</td>
</tr>
<tr>
<td>UDP</td>
<td>4500</td>
<td>sae-urn/ (IP安全性，IKE NAT遍历）</td>
</tr>
<tr>
<td>UDP</td>
<td>5419</td>
<td>DarkSky</td>
</tr>
<tr>
<td>UDP</td>
<td>5503</td>
<td>Remote Shell Trojan</td>
</tr>
<tr>
<td>UDP</td>
<td>5555</td>
<td>Daodan</td>
</tr>
<tr>
<td>UDP</td>
<td>5882</td>
<td>Y3K RAT</td>
</tr>
<tr>
<td>UDP</td>
<td>5888</td>
<td>Y3K RAT</td>
</tr>
<tr>
<td>UDP</td>
<td>6112</td>
<td>Battle .net Game</td>
</tr>
<tr>
<td>UDP</td>
<td>6666</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>6667</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>6766</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>6767</td>
<td>KiLo,UandMe</td>
</tr>
<tr>
<td>UDP</td>
<td>6838</td>
<td>Mstream Agent-handler</td>
</tr>
<tr>
<td>UDP</td>
<td>7028</td>
<td>未知木马</td>
</tr>
<tr>
<td>UDP</td>
<td>7424</td>
<td>Host Control</td>
</tr>
<tr>
<td>UDP</td>
<td>7788</td>
<td>Singularity</td>
</tr>
<tr>
<td>UDP</td>
<td>7983</td>
<td>MStream handler-agent</td>
</tr>
<tr>
<td>UDP</td>
<td>8012</td>
<td>Ptakks</td>
</tr>
<tr>
<td>UDP</td>
<td>8090</td>
<td>Aphex’s Remote Packet Sniffer</td>
</tr>
<tr>
<td>UDP</td>
<td>8127</td>
<td>9_119,Chonker</td>
</tr>
<tr>
<td>UDP</td>
<td>8488</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>8489</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>8787</td>
<td>BackOrifice 2000</td>
</tr>
<tr>
<td>UDP</td>
<td>8879</td>
<td>BackOrifice 2000</td>
</tr>
<tr>
<td>UDP</td>
<td>9325</td>
<td>MStream Agent-handler</td>
</tr>
<tr>
<td>UDP</td>
<td>10000</td>
<td>XHX</td>
</tr>
<tr>
<td>UDP</td>
<td>10067</td>
<td>Portal of Doom</td>
</tr>
<tr>
<td>UDP</td>
<td>10084</td>
<td>Syphillis</td>
</tr>
<tr>
<td>UDP</td>
<td>10100</td>
<td>Slapper</td>
</tr>
<tr>
<td>UDP</td>
<td>10167</td>
<td>Portal of Doom</td>
</tr>
<tr>
<td>UDP</td>
<td>10498</td>
<td>Mstream</td>
</tr>
<tr>
<td>UDP</td>
<td>10666</td>
<td>Ambush</td>
</tr>
<tr>
<td>UDP</td>
<td>11225</td>
<td>Cyn</td>
</tr>
<tr>
<td>UDP</td>
<td>12321</td>
<td>Protoss</td>
</tr>
<tr>
<td>UDP</td>
<td>12345</td>
<td>BlueIce 2000</td>
</tr>
<tr>
<td>UDP</td>
<td>12378</td>
<td>W32/Gibe@MM</td>
</tr>
<tr>
<td>UDP</td>
<td>12623</td>
<td>ButtMan,DUN Control</td>
</tr>
<tr>
<td>UDP</td>
<td>15210</td>
<td>UDP remote shell backdoor server</td>
</tr>
<tr>
<td>UDP</td>
<td>15486</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>16514</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>16515</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>18753</td>
<td>Shaft handler to Agent</td>
</tr>
<tr>
<td>UDP</td>
<td>20433</td>
<td>Shaft</td>
</tr>
<tr>
<td>UDP</td>
<td>21554</td>
<td>GirlFriend</td>
</tr>
<tr>
<td>UDP</td>
<td>22784</td>
<td>Backdoor.Intruzzo</td>
</tr>
<tr>
<td>UDP</td>
<td>23476</td>
<td>Donald Dick</td>
</tr>
<tr>
<td>UDP</td>
<td>25123</td>
<td>MOTD</td>
</tr>
<tr>
<td>UDP</td>
<td>26274</td>
<td>Delta Source</td>
</tr>
<tr>
<td>UDP</td>
<td>26374</td>
<td>Sub-7 2.1</td>
</tr>
<tr>
<td>UDP</td>
<td>26444</td>
<td>Trin00/TFN2K</td>
</tr>
<tr>
<td>UDP</td>
<td>26573</td>
<td>Sub-7 2.1</td>
</tr>
<tr>
<td>UDP</td>
<td>27184</td>
<td>Alvgus trojan 2000</td>
</tr>
<tr>
<td>UDP</td>
<td>27444</td>
<td>Trinoo</td>
</tr>
<tr>
<td>UDP</td>
<td>29589</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>29891</td>
<td>The Unexplained</td>
</tr>
<tr>
<td>UDP</td>
<td>30103</td>
<td>NetSphere</td>
</tr>
<tr>
<td>UDP</td>
<td>31320</td>
<td>Little Witch</td>
</tr>
<tr>
<td>UDP</td>
<td>31335</td>
<td>Trin00 DoS Attack</td>
</tr>
<tr>
<td>UDP</td>
<td>31337</td>
<td>Baron Night,BO client,BO2,Bo Facil,BackFire,Back Orifice,DeepBO</td>
</tr>
<tr>
<td>UDP</td>
<td>31338</td>
<td>Back Orifice,NetSpy DK,DeepBO</td>
</tr>
<tr>
<td>UDP</td>
<td>31339</td>
<td>Little Witch</td>
</tr>
<tr>
<td>UDP</td>
<td>31340</td>
<td>Little Witch</td>
</tr>
<tr>
<td>UDP</td>
<td>31416</td>
<td>Lithium</td>
</tr>
<tr>
<td>UDP</td>
<td>31787</td>
<td>Hack aTack</td>
</tr>
<tr>
<td>UDP</td>
<td>31789</td>
<td>Hack aTack</td>
</tr>
<tr>
<td>UDP</td>
<td>31790</td>
<td>Hack aTack</td>
</tr>
<tr>
<td>UDP</td>
<td>31791</td>
<td>Hack aTack</td>
</tr>
<tr>
<td>UDP</td>
<td>33390</td>
<td>未知木马</td>
</tr>
<tr>
<td>UDP</td>
<td>34555</td>
<td>Trinoo</td>
</tr>
<tr>
<td>UDP</td>
<td>35555</td>
<td>Trinoo</td>
</tr>
<tr>
<td>UDP</td>
<td>43720</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>44014</td>
<td>Iani</td>
</tr>
<tr>
<td>UDP</td>
<td>44767</td>
<td>School Bus</td>
</tr>
<tr>
<td>UDP</td>
<td>46666</td>
<td>Taskman</td>
</tr>
<tr>
<td>UDP</td>
<td>47262</td>
<td>Delta Source</td>
</tr>
<tr>
<td>UDP</td>
<td>47785</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>49301</td>
<td>OnLine keyLogger</td>
</tr>
<tr>
<td>UDP</td>
<td>49683</td>
<td>Fenster</td>
</tr>
<tr>
<td>UDP</td>
<td>49698</td>
<td>KiLo</td>
</tr>
<tr>
<td>UDP</td>
<td>52901</td>
<td>Omega</td>
</tr>
<tr>
<td>UDP</td>
<td>54320</td>
<td>Back Orifice</td>
</tr>
<tr>
<td>UDP</td>
<td>54321</td>
<td>Back Orifice 2000</td>
</tr>
<tr>
<td>UDP</td>
<td>54341</td>
<td>NetRaider Trojan</td>
</tr>
<tr>
<td>UDP</td>
<td>61746</td>
<td>KiLO</td>
</tr>
<tr>
<td>UDP</td>
<td>61747</td>
<td>KiLO</td>
</tr>
<tr>
<td>UDP</td>
<td>61748</td>
<td>KiLO</td>
</tr>
<tr>
<td>UDP</td>
<td>65432</td>
<td>The Traitor</td>
</tr>
</tbody></table>
<p>整理来自于：今天超市开门了吗</p>
<p>（非本人整理）</p>
]]></content>
      <categories>
        <category>工具集</category>
      </categories>
      <tags>
        <tag>常用端口</tag>
      </tags>
  </entry>
  <entry>
    <title>ETL介绍（一）</title>
    <url>/bigdata/ETL%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="ETL简介"><a href="#ETL简介" class="headerlink" title="ETL简介"></a>ETL简介</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>ETL （Extract-Transform-Load）用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。</p>
<p>ETL一般是指将业务系统的数据经过抽取、清洗转换之后加载到数据仓库的过程，目的是将企业中的分散、零乱、标准不统一的数据整合到一起，为企业的决策提供分析依据。较常用在数据仓库，但其对象<u>并不限于数据仓库</u>。</p>
<p>ETL是BI（商业智能）项目重要的一个环节。</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p><strong>数据同步</strong>（周期型）、<strong>数据量大</strong></p>
<h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>抽取、清洗、转换、装载</p>
<p>抽取：是从数据库读取数据的过程。在此阶段，通常从多种不同类型的来源收集数据。</p>
<p>清洗：主要将不完整数据、错误数据、重复数据进行处理</p>
<p>转换：所提取的数据从先前的形式进入需要它的另一个数据库的形式。通过使用规则或查找表或通过将数据与其他数据组合来进行转换。</p>
<p>装载：将数据写入目标数据库的过程。</p>
<h2 id="实现方法与设计部分"><a href="#实现方法与设计部分" class="headerlink" title="实现方法与设计部分"></a>实现方法与设计部分</h2><p>实现方法：ETL工具实现、SQL方式实现、ETL工具和SQL相结合。</p>
<table>
<thead>
<tr>
<th align="center">实现方法名称</th>
<th align="center">优点</th>
<th align="center">缺点</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ETL工具实现</td>
<td align="center">快速建立起ETL工程、减少复杂编码任务、提高速度、降低难度</td>
<td align="center">缺少灵活性</td>
</tr>
<tr>
<td align="center">SQL方式实现</td>
<td align="center">灵活、提高ETL运行效率</td>
<td align="center">编码复杂</td>
</tr>
<tr>
<td align="center">ETL工具和SQL相结合</td>
<td align="center">综合了前面二种的优点，极大地提高ETL的开发速度和效率</td>
<td align="center">——</td>
</tr>
</tbody></table>
<p>设计部分：数据抽取、数据的清洗转换、数据的加载</p>
<h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol>
<li>ETL可以分担数据库系统的负载（采用单独的硬件服务器）</li>
<li>ETL相对于EL-T架构可以实现更为复杂的数据转化逻辑</li>
<li>ETL采用单独的硬件服务器。.</li>
<li>ETL与底层的数据库数据存储无关.</li>
</ol>
<h2 id="模式介绍"><a href="#模式介绍" class="headerlink" title="模式介绍"></a>模式介绍</h2><p>指路CSDN：<a href="https://blog.csdn.net/jianzhang11/article/details/104240047" target="_blank" rel="noopener">https://blog.csdn.net/jianzhang11/article/details/104240047</a></p>
<h1 id="ETL架构与ELT架构"><a href="#ETL架构与ELT架构" class="headerlink" title="ETL架构与ELT架构"></a>ETL架构与ELT架构</h1><p>ETL所描述的过程，一般常见的作法包含ETL或是ELT（Extract-Load-Transform），并且混合使用。通常愈大量的数据、复杂的转换逻辑、目的端为较强运算能力的数据库，愈偏向使用ELT，以便运用目的端数据库的平行处理能力。</p>
<h2 id="ELT"><a href="#ELT" class="headerlink" title="ELT"></a>ELT</h2><p>ELT（Extraction-Loading-Transformation）是利用数据库的处理能力，E=从源数据库抽取数据，L=把数据加载到目标库的临时表中，T=对临时表中的数据进行转换，然后加载到目标库目标表中。<br>在ELT架构中，ELT只负责提供图形化的界面来设计业务规则，数据的整个加工过程都在目标和源的数据库之间流动，ELT协调相关的数据库系统来执行相关的应用，数据加工过程既可以在源数据库端执行，也可以在目标数据仓库端执行（主要取决于系统的架构设计和数据属性）。当ETL过程需要提高效率，则可以通过对相关数据库进行调优，或者改变执行加工的服务器就可以达到。一般数据库厂商会力推该中架构，像Oracle和Teradata都极力宣传ELT架构。</p>
<h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>对临时表中的数据进行转换</p>
<h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><p>在转换服务器中进行的</p>
<h3 id="过程-1"><a href="#过程-1" class="headerlink" title="过程"></a>过程</h3><p>Extraction-Loading-Transformation</p>
<h3 id="优势-1"><a href="#优势-1" class="headerlink" title="优势"></a>优势</h3><ol>
<li><p>ELT主要通过数据库引擎来实现系统的可扩展性（尤其是当数据加工过程在晚上时，可以充分利用数据库引擎的资源） [2] </p>
</li>
<li><p>ELT可以保持所有的数据始终在数据库当中，避免数据的加载和导出，从而保证效率，提高系统的可监控性。</p>
</li>
<li><p>ELT可以根据数据的分布情况进行并行处理优化，并可以利用数据库的固有功能优化磁盘I/O。</p>
</li>
<li><p>ELT的可扩展性取决于数据库引擎和其硬件服务器的可扩展性。</p>
</li>
<li><p>通过对相关数据库进行性能调优，ETL过程获得3到4倍的效率提升一般不是特别困难。<br>ELT工具编辑<br>和基于ETL架构的工具（Kettle、Talend、Datastage、Informatica）相比，基于ELT架构的工具目前并不多（OWB、HaoheDI）。</p>
<p>优势介绍来源：<a href="https://www.cnblogs.com/Jesse-Li/p/8821893.html" target="_blank" rel="noopener">https://www.cnblogs.com/Jesse-Li/p/8821893.html</a></p>
</li>
</ol>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p><img src="/bigdata/ETL%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/20190527181906835-1600193601044.png" alt="20190527181906835-1600193601044"></p>
<p>图源：<a href="https://blog.csdn.net/zhangjing5566/article/details/90609221" target="_blank" rel="noopener">https://blog.csdn.net/zhangjing5566/article/details/90609221</a></p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center"><font color="green">ETL</font></th>
<th align="center"><font color="green">ELT</font></th>
</tr>
</thead>
<tbody><tr>
<td align="center">顺序</td>
<td align="center">提取 - 转换 - 装载</td>
<td align="center">提取 - 装载 - 转换</td>
</tr>
<tr>
<td align="center">速度</td>
<td align="center">——</td>
<td align="center">更快</td>
</tr>
<tr>
<td align="center">数据转化逻辑</td>
<td align="center">可实现更复杂的</td>
<td align="center">——</td>
</tr>
<tr>
<td align="center">硬件服务器</td>
<td align="center">单独</td>
<td align="center">——</td>
</tr>
<tr>
<td align="center">与底层的数据库数据存储关系</td>
<td align="center">无关</td>
<td align="center">所有数据都在数据库中</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>ETL</tag>
        <tag>常用名词介绍</tag>
      </tags>
  </entry>
  <entry>
    <title>ETL介绍（二）</title>
    <url>/bigdata/ETL%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="ETL过程详解"><a href="#ETL过程详解" class="headerlink" title="ETL过程详解"></a>ETL过程详解</h1><p>ETL过程详解转载自：<a href="https://blog.csdn.net/weixin_34220179/article/details/85952403" target="_blank" rel="noopener">https://blog.csdn.net/weixin_34220179/article/details/85952403</a></p>
<h2 id="数据抽取"><a href="#数据抽取" class="headerlink" title="数据抽取"></a>数据抽取</h2><p>这一部分需要在调研阶段做大量的工作，首先要搞清楚数据是从几个业务系统中来,各个业务系统的数据库服务器运行什么DBMS,是否存在手工数据，手工数据量有多大，是否存在非结构化的数据等等，当收集完这些信息之后才可以进行数据抽取的设计。</p>
<p><strong>1、对于与存放DW的数据库系统相同的数据源处理方法</strong></p>
<p>这一类数据源在设计上比较容易。一般情况下，DBMS(SQLServer、Oracle)都会提供数据库链接功能，在DW数据库服务器和原业务系统之间建立直接的链接关系就可以写Select 语句直接访问。</p>
<p><strong>2、对于与DW数据库系统不同的数据源的处理方法</strong></p>
<p>对于这一类数据源，一般情况下也可以通过ODBC的方式建立数据库链接——如SQL Server和Oracle之间。如果不能建立数据库链接，可以有两种方式完成，一种是通过工具将源数据导出成.txt或者是.xls文件，然后再将这些源系统文件导入到ODS中。另外一种方法是通过程序接口来完成。</p>
<p><strong>3、对于文件类型数据源(.txt,.xls)</strong>，可以培训业务人员利用数据库工具将这些数据导入到指定的数据库，然后从指定的数据库中抽取。或者还可以借助工具实现。</p>
<p><strong>4、增量更新的问题</strong></p>
<p>对于数据量大的系统，必须考虑增量抽取。一般情况下，业务系统会记录业务发生的时间，我们可以用来做增量的标志,每次抽取之前首先判断ODS中记录最大的时间，然后根据这个时间去业务系统取大于这个时间所有的记录。利用业务系统的时间戳，一般情况下，业务系统没有或者部分有时间戳。</p>
<h2 id="数据的清洗转换"><a href="#数据的清洗转换" class="headerlink" title="数据的清洗转换"></a>数据的清洗转换</h2><p>ETL在转化的过程中，主要体现在：</p>
<p><strong>空值处理</strong>：可捕获字段空值，进行加载或替换为其他含义数据，并可根据字段空值实现分流加载到不同目标库。</p>
<p><strong>规范化数据格式</strong>：可实现字段格式约束定义，对于数据源中时间、数值、字符等数据，可自定义加载格式。</p>
<p><strong>拆分数据</strong>：依据业务需求对字段可进行分解。例，主叫号 861082585313-8148，可进行区域码和电话号码分解。</p>
<p><strong>验证数据正确性</strong>：可利用Lookup及拆分功能进行数据验证。例如，主叫号861082585313-8148，进行区域码和电话号码分解后，可利用Lookup返回主叫网关或交换机记载的主叫地区，进行数据验证。</p>
<p><strong>数据替换</strong>：对于因业务因素，可实现无效数据、缺失数据的替换。<br>Lookup：查获丢失数据 Lookup实现子查询，并返回用其他手段获取的缺失字段，保证字段完整性。</p>
<p><strong>建立ETL过程的主外键约束</strong>：对无依赖性的非法数据，可替换或导出到错误数据文件中，保证主键唯一记录的加载。</p>
<p>ETL在转化的体现转载自：<a href="https://www.cnblogs.com/Jesse-Li/p/8821893.html" target="_blank" rel="noopener">https://www.cnblogs.com/Jesse-Li/p/8821893.html</a></p>
<p>—————————————————————————————————————</p>
<h3 id="1、数据清洗"><a href="#1、数据清洗" class="headerlink" title="1、数据清洗"></a>1、数据清洗</h3><p>数据清洗的任务是过滤那些不符合要求的数据，将过滤的结果交给业务主管部门，确认是否过滤掉还是由业务单位修正之后再进行抽取。</p>
<p><strong>不符合要求的数据主要是有不完整的数据、错误的数据、重复的数据三大类。</strong></p>
<p>(1)不完整的数据：这一类数据主要是一些应该有的信息缺失，如供应商的名称、分公司的名称、客户的区域信息缺失、业务系统中主表与明细表不能匹配等。对于这一类数据过滤出来，按缺失的内容分别写入不同Excel文件向客户提交，要求在规定的时间内补全。补全后才写入数据仓库。</p>
<p>(2)错误的数据：这一类错误产生的原因是业务系统不够健全，在接收输入后没有进行判断直接写入后台数据库造成的，比如数值数据输成全角数字字符、字符串数据后面有一个回车操作、日期格式不正确、日期越界等。这一类数据也要分类，对于类似于全角字符、数据前后有不可见字符的问题，<strong>只能通过写SQL语句的方式找出来，然后要求客户在业务系统修正之后抽取</strong>。日期格式不正确的或者是日期越界的这一类错误会导致ETL运行失败，这一类错误需要去业务系统数据库用SQL的方式挑出来，交给业务主管部门要求限期修正，修正之后再抽取。</p>
<p>(3)重复的数据：对于这一类数据——特别是维表中会出现这种情况——将重复数据记录的所有字段导出来，让客户确认并整理。</p>
<p>数据清洗是一个反复的过程，不可能在几天内完成，只有不断的发现问题，解决问题。对于是否过滤，是否修正一般要求客户确认，对于过滤掉的数据，写入Excel文件或者将过滤数据写入数据表，在ETL开发的初期可以每天向业务单位发送过滤数据的邮件，促使他们尽快地修正错误,同时也可以做为将来验证数据的依据。数据清洗需要注意的是不要将有用的数据过滤掉，对于每个过滤规则认真进行验证，并要用户确认。</p>
<h3 id="2、数据转换"><a href="#2、数据转换" class="headerlink" title="2、数据转换"></a>2、数据转换</h3><p>数据转换的任务主要进行不一致的数据转换、数据粒度的转换，以及一些商务规则的计算。</p>
<p>(1)不一致数据转换：这个过程是一个整合的过程，将不同业务系统的相同类型的数据统一，比如同一个供应商在结算系统的编码是XX0001,而在CRM中编码是YY0001，这样在抽取过来之后统一转换成一个编码。</p>
<p>(2)数据粒度的转换：<strong>业务系统一般存储非常明细的数据，而数据仓库中数据是用来分析的，不需要非常明细的数据。一般情况下，</strong>会将业务系统数据按照数据仓库粒度进行聚合。</p>
<p>(3)<strong>商务规则的计算</strong>：不同的企业有不同的业务规则、不同的数据指标，这些指标有的时候不是简单的加加减减就能完成，这个时候需要在ETL中将这些数据指标计算好了之后存储在数据仓库中，以供分析使用。</p>
<h2 id="数据的加载"><a href="#数据的加载" class="headerlink" title="数据的加载"></a>数据的加载</h2><p>暂无</p>
<h2 id="ETL日志、警告发送"><a href="#ETL日志、警告发送" class="headerlink" title="ETL日志、警告发送"></a>ETL日志、警告发送</h2><h2 id="ETL日志"><a href="#ETL日志" class="headerlink" title="ETL日志"></a>ETL日志</h2><p>记录日志的目的是随时可以知道ETL运行情况，如果出错了，可以知道哪里出错。</p>
<p>ETL日志分为三类：</p>
<p>1、执行过程日志，这一部分日志是在ETL执行过程中每执行一步的记录，记录每次运行每一步骤的起始时间，影响了多少行数据，流水账形式。</p>
<p>2、错误日志，当某个模块出错的时候写错误日志，记录每次出错的时间、出错的模块以及出错的信息等。</p>
<p>3、日志是总体日志，只记录ETL开始时间、结束时间是否成功信息。如果使用ETL工具,ETL工具会自动产生一些日志，这一类日志也可以作为ETL日志的一部分。</p>
<h2 id="警告发送"><a href="#警告发送" class="headerlink" title="警告发送"></a>警告发送</h2><p>如果ETL出错了，不仅要形成ETL出错日志，而且要向系统管理员发送警告。发送警告的方式多种，一般常用的就是给系统管理员发送邮件，并附上出错的信息，方便管理员排查错误。</p>
<h1 id="ELT工具"><a href="#ELT工具" class="headerlink" title="ELT工具"></a>ELT工具</h1><h2 id="选择依据"><a href="#选择依据" class="headerlink" title="选择依据"></a>选择依据</h2><p>对平台的支持程度</p>
<p>抽取和装载的性能是不是较高，且对业务系统的性能影响大不大，侵入性高不高</p>
<p>对数据源的支持程度</p>
<p>是否具有良好的集成性和开放性</p>
<p>数据转换和加工的功能强不强</p>
<p>是否具有管理和调度的功能</p>
<h2 id="常见工具"><a href="#常见工具" class="headerlink" title="常见工具"></a>常见工具</h2><ol>
<li><p>datastage</p>
</li>
<li><p>informatica</p>
</li>
<li><p>kettle</p>
</li>
<li><p>ODI</p>
</li>
<li><p>5.Cognos</p>
</li>
<li><p>beeload</p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>ETL</tag>
        <tag>常用名词介绍</tag>
      </tags>
  </entry>
  <entry>
    <title>ElasticSearch介绍</title>
    <url>/bigdata/ElasticSearch%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Elasticearch概述"><a href="#Elasticearch概述" class="headerlink" title="Elasticearch概述"></a>Elasticearch概述</h1><h2 id="什么是搜索？"><a href="#什么是搜索？" class="headerlink" title="什么是搜索？"></a>什么是搜索？</h2><p>百度：我们比如说想找寻任何的信息的时候，就会上百度去搜索一下，比如说找一部自己喜欢的电影，或者说找一本喜欢的书，或者找一条感兴趣的新闻（提到搜索的第一印象）。百度<br>!= 搜索</p>
<p>1）互联网的搜索：电商网站，招聘网站，新闻网站，各种app</p>
<p>2）IT系统的搜索：OA软件，办公自动化软件，会议管理，日程管理，项目管理。</p>
<p>搜索，就是在任何场景下，找寻你想要的信息，这个时候，会输入一段你要搜索的关键字，然后就期望找到这个关键字相关的有些信息。</p>
<h2 id="如果用数据库做搜索会怎么样？"><a href="#如果用数据库做搜索会怎么样？" class="headerlink" title="如果用数据库做搜索会怎么样？"></a>如果用数据库做搜索会怎么样？</h2><p><img src="/bigdata/ElasticSearch%E4%BB%8B%E7%BB%8D/1.png" alt="1"></p>
<p>用数据库来实现搜索，是不太靠谱的。通常来说，性能会很差的。</p>
<h2 id="什么是全文检索和Lucene？"><a href="#什么是全文检索和Lucene？" class="headerlink" title="什么是全文检索和Lucene？"></a>什么是全文检索和Lucene？</h2><p>1）全文检索，倒排索引</p>
<p>全文检索是指计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。全文搜索搜索引擎数据库中的数据。</p>
<p><img src="/bigdata/ElasticSearch%E4%BB%8B%E7%BB%8D/2.png" alt="2"></p>
<p>2）lucene，就是一个jar包，里面包含了封装好的各种建立倒排索引，以及进行搜索的代码，包括各种算法。我们就用java开发的时候，引入lucene jar，然后基于lucene的api进行去进行开发就可以了。</p>
<h2 id="什么是Elasticsearch？"><a href="#什么是Elasticsearch？" class="headerlink" title="什么是Elasticsearch？"></a>什么是Elasticsearch？</h2><p>Elasticsearch，基于Lucene，隐藏复杂性，提供简单易用的RestfulAPI接口、JavaAPI接口（还有其他语言的API接口）。</p>
<p>关于Elasticsearch的一个传说，有一个程序员失业了，陪着自己老婆去英国伦敦学习厨师课程。程序员在失业期间想给老婆写一个菜谱搜索引擎，觉得Lucene实在太复杂了，就开发了一个封装了Lucene的开源项目：Compass。后来程序员找到了工作，是做分布式的高性能项目的，觉得Compass不够，就写了Elasticsearch，让Lucene变成分布式的系统。</p>
<p>Elasticsearch是一个实时分布式搜索和分析引擎。它用于全文搜索、结构化搜索、分析。</p>
<p>全文检索：将非结构化数据中的一部分信息提取出来,重新组织,使其变得有一定结构,然后对此有一定结构的数据进行搜索,从而达到搜索相对较快的目的。</p>
<p>结构化检索：我想搜索商品分类为日化用品的商品都有哪些，select * from<br>products where category_id=’日化用品’。</p>
<p>数据分析：电商网站，最近7天牙膏这种商品销量排名前10的商家有哪些；新闻网站，最近1个月访问量排名前3的新闻版块是哪些。</p>
<h2 id="Elasticsearch的适用场景"><a href="#Elasticsearch的适用场景" class="headerlink" title="Elasticsearch的适用场景"></a>Elasticsearch的适用场景</h2><p>1）维基百科，类似百度百科，牙膏，牙膏的维基百科，全文检索，高亮，搜索推荐。</p>
<p>2）The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击，浏览，收藏，评论）+<br>社交网络数据（对某某新闻的相关看法），数据分析，给到每篇新闻文章的作者，让他知道他的文章的公众反馈（好，坏，热门，垃圾，鄙视，崇拜）。</p>
<p>3）Stack Overflow（国外的程序异常讨论论坛），IT问题，程序的报错，提交上去，有人会跟你讨论和回答，全文检索，搜索相关问题和答案，程序报错了，就会将报错信息粘贴到里面去，搜索有没有对应的答案。</p>
<p>4）GitHub（开源代码管理），搜索上千亿行代码。</p>
<p>5）国内：站内搜索（电商，招聘，门户，等等），IT系统搜索（OA，CRM，ERP，等等），数据分析（ES热门的一个使用场景）。</p>
<h2 id="Elasticsearch的特点"><a href="#Elasticsearch的特点" class="headerlink" title="Elasticsearch的特点"></a>Elasticsearch的特点</h2><p>1）可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上，服务小公司</p>
<p>2）Elasticsearch不是什么新技术，主要是将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的ES；lucene（全文检索），商用的数据分析软件（也是有的），分布式数据库（mycat）</p>
<p>3）对用户而言，是开箱即用的，非常简单，作为中小型的应用，直接3分钟部署一下ES，就可以作为生产环境的系统来使用了，数据量不大，操作不是太复杂</p>
<p>4）数据库的功能面对很多领域是不够用的（事务，还有各种联机事务型的操作）；特殊的功能，比如全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理；Elasticsearch作为传统数据库的一个补充，提供了数据库所不能提供的很多功能</p>
<h2 id="Elasticsearch的核心概念"><a href="#Elasticsearch的核心概念" class="headerlink" title="Elasticsearch的核心概念"></a>Elasticsearch的核心概念</h2><h3 id="近实时"><a href="#近实时" class="headerlink" title="近实时"></a>近实时</h3><p>近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级。</p>
<h3 id="Cluster（集群）"><a href="#Cluster（集群）" class="headerlink" title="Cluster（集群）"></a>Cluster（集群）</h3><p>集群包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常</p>
<h3 id="Node（节点）"><a href="#Node（节点）" class="headerlink" title="Node（节点）"></a>Node（节点）</h3><p>集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为”elasticsearch”的集群，如果直接启动一堆节点，那么它们会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群。</p>
<h3 id="Index（索引-数据库）"><a href="#Index（索引-数据库）" class="headerlink" title="Index（索引-数据库）"></a>Index（索引-数据库）</h3><p>索引包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。</p>
<h3 id="Type（类型-表）"><a href="#Type（类型-表）" class="headerlink" title="Type（类型-表）"></a>Type（类型-表）</h3><p>6.0版本之前每个索引里都可以有多个type；</p>
<p>6.0版本之后每个索引里面只能有一个Type，一般使用_doc代替了。</p>
<p>7.0版本之后没有Type概念。</p>
<p>商品index，里面存放了所有的商品数据，商品document</p>
<p>商品type：product_id，product_name，product_desc，category_id，category_name，service_period</p>
<p>每一个type里面，都会包含一堆document</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="string">"product_id"</span>: <span class="string">"1"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"product_name"</span>: <span class="string">"长虹电视机"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"product_desc"</span>: <span class="string">"4k高清"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"category_id"</span>: <span class="string">"3"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"category_name"</span>: <span class="string">"电器"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"service_period"</span>: <span class="string">"1年"</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="string">"product_id"</span>: <span class="string">"2"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"product_name"</span>: <span class="string">"基围虾"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"product_desc"</span>: <span class="string">"纯天然，冰岛产"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"category_id"</span>: <span class="string">"4"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"category_name"</span>: <span class="string">"生鲜"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"eat_period"</span>: <span class="string">"7天"</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Document（文档-行）"><a href="#Document（文档-行）" class="headerlink" title="Document（文档-行）"></a>Document（文档-行）</h3><p>文档是ES中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。</p>
<h3 id="Field（字段-列）"><a href="#Field（字段-列）" class="headerlink" title="Field（字段-列）"></a>Field（字段-列）</h3><p>Field是Elasticsearch的最小单位。一个document里面有多个field，每个field就是一个数据字段。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">product document</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="string">"product_id"</span>: <span class="string">"1"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"product_name"</span>: <span class="string">"高露洁牙膏"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"product_desc"</span>: <span class="string">"高效美白"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"category_id"</span>: <span class="string">"2"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"category_name"</span>: <span class="string">"日化用品"</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Mapping（映射-约束）"><a href="#Mapping（映射-约束）" class="headerlink" title="Mapping（映射-约束）"></a>Mapping（映射-约束）</h3><p>数据如何存放到索引对象上，需要有一个映射配置，包括：数据类型、是否存储、是否分词等。</p>
<p>Mapping用来定义Document中每个字段的类型，即所使用的分词器、是否索引等属性，非常关键等。创建Mapping<br>的代码示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">PUT student**(index_name-&gt;database)**</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="string">"mappings"</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="string">"_doc"</span>:&#123;**(type_name-&gt;table)**</span><br><span class="line"></span><br><span class="line"><span class="string">"properties"</span>:&#123;</span><br><span class="line"></span><br><span class="line"><span class="string">"stu_id"</span>:&#123;**(field_name-&gt;colume)**</span><br><span class="line"></span><br><span class="line"><span class="string">"type"</span>:<span class="string">"keyword"</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">"store"</span>:<span class="string">"true"</span></span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="string">"name"</span>:&#123;**(field_name-&gt;colume)**</span><br><span class="line"></span><br><span class="line"><span class="string">"type"</span>:<span class="string">"keyword"</span></span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="string">"birth"</span>:&#123;**(field_name-&gt;colume)**</span><br><span class="line"></span><br><span class="line"><span class="string">"type"</span>:<span class="string">"date"</span>(yyyy-MM-dd HH:mm)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ElasticSearch与数据库的类比"><a href="#ElasticSearch与数据库的类比" class="headerlink" title="ElasticSearch与数据库的类比"></a>ElasticSearch与数据库的类比</h3><table>
<thead>
<tr>
<th>关系型数据库（比如Mysql)</th>
<th>非关系型数据库（Elasticsearch）</th>
</tr>
</thead>
<tbody><tr>
<td>数据库Database</td>
<td>索引Index</td>
</tr>
<tr>
<td>表Table</td>
<td>类型Type(6.0版本之后在一个索引下面只能有一个，7.0版本之后取消了Type)</td>
</tr>
<tr>
<td>数据行Row</td>
<td>文档Document(JSON格式)</td>
</tr>
<tr>
<td>数据列Column</td>
<td>字段Field</td>
</tr>
<tr>
<td>约束 Schema</td>
<td>映射Mapping</td>
</tr>
</tbody></table>
<hr>
<h3 id="ElasticSearch存入数据和搜索数据机制"><a href="#ElasticSearch存入数据和搜索数据机制" class="headerlink" title="ElasticSearch存入数据和搜索数据机制"></a>ElasticSearch存入数据和搜索数据机制</h3><p><img src="/bigdata/ElasticSearch%E4%BB%8B%E7%BB%8D/3.png" alt="3"></p>
<p>1）索引对象（blog）：存储数据的表结构 ，任何搜索数据，存放在索引对象上。</p>
<p>2）映射（mapping）：数据如何存放到索引对象上，需要有一个映射配置，<br>包括：数据类型、是否存储、是否分词等。</p>
<p>3）文档（document）：一条数据记录，存在索引对象上。</p>
<p>4）文档类型（type）：一个索引对象，存放多种类型数据，数据用文档类型进行标识。</p>
<h1 id="Elasticsearch快速入门"><a href="#Elasticsearch快速入门" class="headerlink" title="Elasticsearch快速入门"></a>Elasticsearch快速入门</h1><h2 id="安装包下载"><a href="#安装包下载" class="headerlink" title="安装包下载"></a>安装包下载</h2><p>1）ElasticSearch官网：<br><a href="https://www.elastic.co/cn/downloads/elasticsearch" target="_blank" rel="noopener">https://www.elastic.co/cn/downloads/elasticsearch</a></p>
<p><img src="/bigdata/ElasticSearch%E4%BB%8B%E7%BB%8D/%E5%9B%BE%E7%89%871.png" alt="图片1"></p>
<h2 id="Elasticsearch安装"><a href="#Elasticsearch安装" class="headerlink" title="Elasticsearch安装"></a>Elasticsearch安装</h2><p>1）解压elasticsearch-6.6.0.tar.gz到/opt/module目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 software]$ tar -zxvf elasticsearch-6.6.0.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure>

<p>2）在/opt/module/elasticsearch-6.6.0路径下创建data文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 elasticsearch-6.6.0]$ mkdir data</span><br></pre></td></tr></table></figure>

<p>3）修改配置文件/opt/module/elasticsearch-6.6.0/config/elasticsearch.yml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 config]$ pwd</span><br><span class="line"></span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;elasticsearch-6.6.0&#x2F;config</span><br><span class="line"></span><br><span class="line">[red@hadoop102 config]$ vi elasticsearch.yml</span><br></pre></td></tr></table></figure>

<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#-----------------------Cluster-----------------------</span></span><br><span class="line"></span><br><span class="line"><span class="attr">cluster.name:</span> <span class="string">my-application</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-----------------------Node-----------------------</span></span><br><span class="line"></span><br><span class="line"><span class="attr">node.name:</span> <span class="string">node-102</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-----------------------Paths-----------------------</span></span><br><span class="line"></span><br><span class="line"><span class="attr">path.data:</span> <span class="string">/opt/module/elasticsearch-6.6.0/data</span></span><br><span class="line"></span><br><span class="line"><span class="attr">path.logs:</span> <span class="string">/opt/module/elasticsearch-6.6.0/logs</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-----------------------Memory-----------------------</span></span><br><span class="line"></span><br><span class="line"><span class="attr">bootstrap.memory_lock:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">bootstrap.system_call_filter:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-----------------------Network-----------------------</span></span><br><span class="line"></span><br><span class="line"><span class="attr">network.host:</span> <span class="number">192.168</span><span class="number">.9</span><span class="number">.102</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-----------------------Discovery-----------------------</span></span><br><span class="line"></span><br><span class="line"><span class="attr">discovery.zen.ping.unicast.hosts:</span> <span class="string">["192.168.9.102"]</span></span><br></pre></td></tr></table></figure>

<p>（1）cluster.name 如果要配置集群需要两个节点上的elasticsearch配置的cluster.name相同，都启动可以自动组成集群，这里如果不改cluster.name则默认是<code>cluster.name=my-application</code></p>
<p>（2）nodename随意取但是集群内的各节点不能相同</p>
<p>（3）修改后的每行前面不能有空格，修改后的”：”后面必须有一个空格</p>
<p>分发至hadoop103以及hadoop104，分发之后修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 module]$ xsync elasticsearch-6.6.0&#x2F;</span><br><span class="line"></span><br><span class="line">node.name: node-103</span><br><span class="line"></span><br><span class="line">network.host: 192.168.9.103</span><br><span class="line"></span><br><span class="line">node.name: node-104</span><br><span class="line"></span><br><span class="line">network.host: 192.168.9.104</span><br></pre></td></tr></table></figure>

<p>5）配置linux系统环境（参考：<a href="http://blog.csdn.net/satiling/article/details/59697916" target="_blank" rel="noopener">http://blog.csdn.net/satiling/article/details/59697916</a>）</p>
<p><strong>问题1：max file descriptors [4096] for elasticsearch process likely<br>too low, increase to at least [65536] elasticsearch</strong></p>
<p>（1）切换到root用户，编辑limits.conf 添加类似如下内容</p>
<p>[root@hadoop102s elasticsearch-6.6.0]# vi /etc/security/limits.conf</p>
<p>添加如下内容:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* soft nofile 65536</span><br><span class="line"></span><br><span class="line">* hard nofile 131072</span><br><span class="line"></span><br><span class="line">* soft nproc 2048</span><br><span class="line"></span><br><span class="line">* hard nproc 4096</span><br></pre></td></tr></table></figure>

<p><strong>问题2：max number of threads [1024] for user [judy2] likely too<br>low, increase to at least [4096]</strong> （CentOS7.x 不用改）</p>
<p>（2）切换到root用户，进入limits.d目录下修改配置文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 elasticsearch-6.6.0]# vi</span><br><span class="line">&#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;90-nproc.conf</span><br><span class="line"></span><br><span class="line">修改如下内容：</span><br><span class="line"></span><br><span class="line">* soft nproc 1024</span><br><span class="line"></span><br><span class="line">#修改为</span><br><span class="line"></span><br><span class="line">* soft nproc 4096</span><br></pre></td></tr></table></figure>

<p><strong>问题3：max virtual memory areas vm.max_map_count [65530] likely too<br>low, increase to at least [262144]</strong> （CentOS7.x 不用改）</p>
<p>（3）切换到root用户修改配置sysctl.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 elasticsearch-6.6.0]# vi &#x2F;etc&#x2F;sysctl.conf</span><br></pre></td></tr></table></figure>

<p>添加下面配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vm.max_map_count&#x3D;655360</span><br></pre></td></tr></table></figure>

<p>并执行命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 elasticsearch-6.6.0]# sysctl -p</span><br></pre></td></tr></table></figure>

<p>以上修改的Linux配置需要分发至其他节点。</p>
<p>然后，重新启动Linux，必须重启！！！</p>
<p>6）启动Elasticsearch</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 elasticsearch-6.6.0]$ bin&#x2F;elasticsearch</span><br></pre></td></tr></table></figure>

<p>7）测试elasticsearch</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 elasticsearch-6.6.0]$ curl http:&#x2F;&#x2F;hadoop102:9200</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;name&quot; : &quot;node-102&quot;,</span><br><span class="line"></span><br><span class="line">&quot;cluster_name&quot; : &quot;my-application&quot;,</span><br><span class="line"></span><br><span class="line">&quot;cluster_uuid&quot; : &quot;KOpuhMgVRzW_9OTjMsHf2Q&quot;,</span><br><span class="line"></span><br><span class="line">&quot;version&quot; : &#123;</span><br><span class="line"></span><br><span class="line">&quot;number&quot; : &quot;6.6.0&quot;,</span><br><span class="line"></span><br><span class="line">&quot;build_flavor&quot; : &quot;default&quot;,</span><br><span class="line"></span><br><span class="line">&quot;build_type&quot; : &quot;tar&quot;,</span><br><span class="line"></span><br><span class="line">&quot;build_hash&quot; : &quot;eb782d0&quot;,</span><br><span class="line"></span><br><span class="line">&quot;build_date&quot; : &quot;2018-06-29T21:59:26.107521Z&quot;,</span><br><span class="line"></span><br><span class="line">&quot;build_snapshot&quot; : false,</span><br><span class="line"></span><br><span class="line">&quot;lucene_version&quot; : &quot;7.3.1&quot;,</span><br><span class="line"></span><br><span class="line">&quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,</span><br><span class="line"></span><br><span class="line">&quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;tagline&quot; : &quot;You Know, for Search&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>8）停止集群</p>
<p><code>kill -9 进程号</code></p>
<p>9）群起脚本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 bin]$ vi es.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">es_home=/opt/module/elasticsearch</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line"><span class="string">"start"</span>) &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"==============<span class="variable">$i</span>=============="</span></span><br><span class="line"></span><br><span class="line">ssh <span class="variable">$i</span> <span class="string">"source /etc/profile;<span class="variable">$&#123;es_home&#125;</span>/bin/elasticsearch &gt;/dev/null</span></span><br><span class="line"><span class="string">2&gt;&amp;1 &amp;"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="string">"stop"</span>) &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"==============<span class="variable">$i</span>=============="</span></span><br><span class="line"></span><br><span class="line">ssh <span class="variable">$i</span> <span class="string">"ps -ef|grep <span class="variable">$es_home</span> |grep -v grep|awk '&#123;print</span></span><br><span class="line"><span class="string"><span class="variable">$2</span>&#125;'|xargs kill"</span> &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure>

<h2 id="Elasticsearch操作工具"><a href="#Elasticsearch操作工具" class="headerlink" title="Elasticsearch操作工具"></a>Elasticsearch操作工具</h2><h3 id="浏览器"><a href="#浏览器" class="headerlink" title="浏览器"></a>浏览器</h3><p><img src="/bigdata/ElasticSearch%E4%BB%8B%E7%BB%8D/%E5%9B%BE%E7%89%872.png" alt="图片2"></p>
<h3 id="Linux命令行"><a href="#Linux命令行" class="headerlink" title="Linux命令行"></a>Linux命令行</h3><p>请求：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@linux1 es] curl -XPOST &#39;http:&#x2F;&#x2F;192.168.9.102:9200&#x2F;red&#x2F;_doc&#39; -i -H</span><br><span class="line">&quot;Content-Type:application&#x2F;json&quot; -d &#39;&#123;&quot;name&quot;:&quot;haha&quot;,&quot;age&quot;:&quot;10&quot;&#125;&#39;</span><br></pre></td></tr></table></figure>

<p>响应：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HTTP&#x2F;1.1 201 Created</span><br><span class="line"></span><br><span class="line">Location: &#x2F;atguig&#x2F;doc&#x2F;Hm7z1GwBEd-elda4prGr</span><br><span class="line"></span><br><span class="line">content-type: application&#x2F;json; charset&#x3D;UTF-8</span><br><span class="line"></span><br><span class="line">content-length: 172</span><br><span class="line"></span><br><span class="line">&#123;&quot;_index&quot;:&quot;red&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;Hm7z1GwBEd-elda4prGr&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Kibana的Dev-Tools"><a href="#Kibana的Dev-Tools" class="headerlink" title="Kibana的Dev Tools"></a>Kibana的Dev Tools</h3><p><img src="/bigdata/ElasticSearch%E4%BB%8B%E7%BB%8D/%E5%9B%BE%E7%89%873.png" alt="图片3"></p>
<h1 id="Elasticsearch使用"><a href="#Elasticsearch使用" class="headerlink" title="Elasticsearch使用"></a>Elasticsearch使用</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="核心数据类型"><a href="#核心数据类型" class="headerlink" title="核心数据类型"></a>核心数据类型</h3><p>字符串型：text(分词)、keyword(不分词)</p>
<p>数值型：long、integer、short、byte、double、float、half_float、scaled_float</p>
<p>日期类型：date</p>
<p>布尔类型：boolean</p>
<p>二进制类型：binary</p>
<p>范围类型：integer_range、float_range、long_range、double_range、date_range</p>
<h3 id="复杂数据类型"><a href="#复杂数据类型" class="headerlink" title="复杂数据类型"></a>复杂数据类型</h3><p>数组类型：array</p>
<p>对象类型：object</p>
<p>嵌套类型：nested object</p>
<h3 id="地理位置数据类型"><a href="#地理位置数据类型" class="headerlink" title="地理位置数据类型"></a>地理位置数据类型</h3><p>geo_point(点)、geo_shape(形状)</p>
<h3 id="专用类型"><a href="#专用类型" class="headerlink" title="专用类型"></a>专用类型</h3><p>记录IP地址ip</p>
<p>实现自动补全completion</p>
<p>记录分词数：token_count</p>
<p>记录字符串hash值murmur3</p>
<p>多字段特性multi-fields</p>
<h2 id="Mapping"><a href="#Mapping" class="headerlink" title="Mapping"></a>Mapping</h2><h3 id="手动创建"><a href="#手动创建" class="headerlink" title="手动创建"></a>手动创建</h3><p>1）创建操作</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">PUT my_index1</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">"mappings": &#123;</span><br><span class="line"></span><br><span class="line">"_doc":&#123;</span><br><span class="line"></span><br><span class="line">"properties":&#123;</span><br><span class="line"></span><br><span class="line">"username":&#123;</span><br><span class="line"></span><br><span class="line">"type": "text",</span><br><span class="line"></span><br><span class="line">"fields": &#123;</span><br><span class="line"></span><br><span class="line">"pinyin":&#123;</span><br><span class="line"></span><br><span class="line">"type": "text"</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）创建文档</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT my_index1&#x2F;_doc&#x2F;1&#96;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;username&quot;:&quot;haha heihei&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）查询</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET my_index1&#x2F;_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;query&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;match&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;username.pinyin&quot;: &quot;haha&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="自动创建"><a href="#自动创建" class="headerlink" title="自动创建"></a>自动创建</h3><p>ES可以自动识别文档字段类型，从而降低用户使用成本</p>
<p>1）直接插入文档</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT &#x2F;test_index&#x2F;_doc&#x2F;1</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;username&quot;:&quot;alfred&quot;,</span><br><span class="line"></span><br><span class="line">&quot;age&quot;:1,</span><br><span class="line"></span><br><span class="line">&quot;birth&quot;:&quot;1991-12-15&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）查看mapping</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;test_index&#x2F;doc&#x2F;_mapping</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;test_index&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;mappings&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;doc&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;properties&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;age&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;long&quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;birth&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;date&quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;username&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;text&quot;,</span><br><span class="line"></span><br><span class="line">&quot;fields&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;keyword&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;keyword&quot;,</span><br><span class="line"></span><br><span class="line">&quot;ignore_above&quot;: 256</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>age自动识别为long类型，username识别为text类型</p>
<p>3）日期类型的自动识别</p>
<p>日期的自动识别可以自行配置日期格式，以满足各种需求。</p>
<p>（1）自定义日期识别格式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT my_index</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;mappings&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;_doc&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;dynamic_date_formats&quot;: [&quot;yyyy-MM-dd&quot;,&quot;yyyy&#x2F;MM&#x2F;dd&quot;]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）关闭日期自动识别</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT my_index</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;mappings&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;_doc&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;date_detection&quot;: false</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4）字符串是数字时，默认不会自动识别为整形，因为字符串中出现数字时完全合理的</p>
<p>Numeric_datection可以开启字符串中数字的自动识别</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT my_index</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;mappings&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;doc&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;numeric_datection&quot;: true</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="IK分词器"><a href="#IK分词器" class="headerlink" title="IK分词器"></a>IK分词器</h2><h3 id="为什么使用分词器"><a href="#为什么使用分词器" class="headerlink" title="为什么使用分词器"></a>为什么使用分词器</h3><p>分词器主要应用在中文上，在ES中字符串类型有keyword和text两种。keyword默认不进行分词，而text是将每一个汉字拆开称为独立的词，这两种都是不适用于生产环境，所以我们需要有其他的分词器帮助我们完成这些事情，其中IK分词器是应用最为广泛的一个分词器。</p>
<p>1）keyword类型的分词</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET _analyze</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;keyword&quot;:&quot;我是程序员&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果展示（报错）</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;error&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;root_cause&quot;: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;illegal_argument_exception&quot;,</span><br><span class="line"></span><br><span class="line">&quot;reason&quot;: &quot;Unknown parameter [keyword] in request body or parameter</span><br><span class="line">is of the wrong type[VALUE_STRING] &quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">],</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;illegal_argument_exception&quot;,</span><br><span class="line"></span><br><span class="line">&quot;reason&quot;: &quot;Unknown parameter [keyword] in request body or parameter</span><br><span class="line">is of the wrong type[VALUE_STRING] &quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;status&quot;: 400</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）text类型的分词</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET _analyze</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;text&quot;:&quot;我是程序员&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果展示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;tokens&quot;: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot;: &quot;我&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot;: 0,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot;: 1,</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot;: 0</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot;: &quot;是&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot;: 1,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot;: 2,</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot;: 1</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot;: &quot;程&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot;: 2,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot;: 3,</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot;: 2</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot;: &quot;序&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot;: 3,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot;: 4,</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot;: 3</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot;: &quot;员&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot;: 4,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot;: 5,</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot;: 4</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="IK分词器安装"><a href="#IK分词器安装" class="headerlink" title="IK分词器安装"></a>IK分词器安装</h3><p>1）下载与安装的ES相对应的版本</p>
<p>2）解压elasticsearch-analysis-ik-6.6.0.zip，将解压后的IK文件夹拷贝到ES安装目录下的plugins目录下，并重命名文件夹为ik（什么名称都OK）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 plugins]$ mkdir ik</span><br><span class="line"></span><br><span class="line">[red@hadoop102 software]$ unzip elasticsearch-analysis-ik-6.6.0.zip -d</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;elasticsearch-6.6.0&#x2F;plugins&#x2F;ik&#x2F;</span><br></pre></td></tr></table></figure>

<p>3）分发分词器目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 elasticsearch-6.6.0]$ xsync plugins&#x2F;</span><br></pre></td></tr></table></figure>

<p>4）重新启动Elasticsearch，即可加载IK分词器</p>
<h3 id="IK分词器测试"><a href="#IK分词器测试" class="headerlink" title="IK分词器测试"></a>IK分词器测试</h3><p>IK提供了两个分词算法ik_smart 和 ik_max_word，其中 ik_smart为最少切分，ik_max_word为最细粒度划分。</p>
<p>1）最少划分ik_smart</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get _analyze</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;analyzer&quot;: &quot;ik_smart&quot;,</span><br><span class="line"></span><br><span class="line">&quot;text&quot;:&quot;我是程序员&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果展示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;tokens&quot; : [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot; : &quot;我&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot; : 0,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot; : 1,</span><br><span class="line"></span><br><span class="line">&quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot; : 0</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot; : &quot;是&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot; : 1,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot; : 2,</span><br><span class="line"></span><br><span class="line">&quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot; : 1</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot; : &quot;程序员&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot; : 2,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot; : 5,</span><br><span class="line"></span><br><span class="line">&quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot; : 2</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）最细切分ik_max_word</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get _analyze</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;analyzer&quot;: &quot;ik_max_word&quot;,</span><br><span class="line"></span><br><span class="line">&quot;text&quot;:&quot;我是程序员&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出的结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;tokens&quot; : [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot; : &quot;我&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot; : 0,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot; : 1,</span><br><span class="line"></span><br><span class="line">&quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot; : 0</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot; : &quot;是&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot; : 1,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot; : 2,</span><br><span class="line"></span><br><span class="line">&quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot; : 1</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot; : &quot;程序员&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot; : 2,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot; : 5,</span><br><span class="line"></span><br><span class="line">&quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot; : 2</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot; : &quot;程序&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot; : 2,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot; : 4,</span><br><span class="line"></span><br><span class="line">&quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot; : 3</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;token&quot; : &quot;员&quot;,</span><br><span class="line"></span><br><span class="line">&quot;start_offset&quot; : 4,</span><br><span class="line"></span><br><span class="line">&quot;end_offset&quot; : 5,</span><br><span class="line"></span><br><span class="line">&quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line"></span><br><span class="line">&quot;position&quot; : 4</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="检索文档"><a href="#检索文档" class="headerlink" title="检索文档"></a>检索文档</h2><p>向Elasticsearch增加数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT &#x2F;red&#x2F;doc&#x2F;1</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;first_name&quot; : &quot;John&quot;,</span><br><span class="line"></span><br><span class="line">&quot;last_name&quot; : &quot;Smith&quot;,</span><br><span class="line"></span><br><span class="line">&quot;age&quot; : 25,</span><br><span class="line"></span><br><span class="line">&quot;about&quot; : &quot;I love to go rock climbing&quot;,</span><br><span class="line"></span><br><span class="line">&quot;interests&quot;: [&quot;sports&quot;, &quot;music&quot;]</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果在关系型数据库Mysql中主键查询数据一般会执行下面的SQL语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from red where id &#x3D; 1;</span><br></pre></td></tr></table></figure>

<p>但在Elasticsearch中需要采用特殊的方式 ： # 协议方法 索引/类型/文档编号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;red&#x2F;doc&#x2F;1</span><br><span class="line"></span><br><span class="line">响应</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;_index&quot;: &quot;red&quot;,</span><br><span class="line"></span><br><span class="line">&quot;_type&quot;: &quot;doc&quot;,</span><br><span class="line"></span><br><span class="line">&quot;_id&quot;: &quot;1&quot;,</span><br><span class="line"></span><br><span class="line">&quot;_version&quot;: 1,</span><br><span class="line"></span><br><span class="line">&quot;found&quot;: true,</span><br><span class="line"></span><br><span class="line">&quot;_source&quot;: &#123; &#x2F;&#x2F; 文档的原始数据JSON数据</span><br><span class="line"></span><br><span class="line">&quot;first_name&quot;: &quot;John&quot;,</span><br><span class="line"></span><br><span class="line">&quot;last_name&quot;: &quot;Smith&quot;,</span><br><span class="line"></span><br><span class="line">&quot;age&quot;: 25,</span><br><span class="line"></span><br><span class="line">&quot;about&quot;: &quot;I love to go rock climbing&quot;,</span><br><span class="line"></span><br><span class="line">&quot;interests&quot;: [</span><br><span class="line"></span><br><span class="line">&quot;sports&quot;,</span><br><span class="line"></span><br><span class="line">&quot;music&quot;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们通过HTTP方法GET来检索文档，同样的，我们可以使用DELETE方法删除文档，使用HEAD方法检查某文档是否存在。如果想更新已存在的文档，我们只需再PUT一次。</p>
<h3 id="元数据查询"><a href="#元数据查询" class="headerlink" title="元数据查询"></a>元数据查询</h3><p>GET _cat/indices</p>
<table>
<thead>
<tr>
<th>health</th>
<th>green(集群完整) yellow(单点正常、集群不完整) red(单点不正常)</th>
</tr>
</thead>
<tbody><tr>
<td>status</td>
<td>是否能使用</td>
</tr>
<tr>
<td>index</td>
<td>索引名</td>
</tr>
<tr>
<td>uuid</td>
<td>索引统一编号</td>
</tr>
<tr>
<td>pri</td>
<td>主节点几个</td>
</tr>
<tr>
<td>rep</td>
<td>从节点几个</td>
</tr>
<tr>
<td>docs.count</td>
<td>文档数</td>
</tr>
<tr>
<td>docs.deleted</td>
<td>文档被删了多少</td>
</tr>
<tr>
<td>store.size</td>
<td>整体占空间大小</td>
</tr>
<tr>
<td>pri.store.size</td>
<td>主节点占</td>
</tr>
</tbody></table>
<hr>
<h3 id="全文档检索"><a href="#全文档检索" class="headerlink" title="全文档检索"></a>全文档检索</h3><p>如果在关系型数据库Mysql中查询所有数据一般会执行下面的SQL语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from user;</span><br></pre></td></tr></table></figure>

<p>但在Elasticsearch中需要采用特殊的方式</p>
<h1 id="协议方法-索引-类型-search"><a href="#协议方法-索引-类型-search" class="headerlink" title="协议方法 索引/类型/_search"></a>协议方法 索引/类型/_search</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;red&#x2F;_doc&#x2F;**_search**</span><br></pre></td></tr></table></figure>

<p>响应内容不仅会告诉我们哪些文档被匹配到，而且这些文档内容完整的被包含在其中—我们在给用户展示搜索结果时需要用到的所有信息都有了。</p>
<h3 id="字段全值匹配检索"><a href="#字段全值匹配检索" class="headerlink" title="字段全值匹配检索"></a>字段全值匹配检索</h3><p>如果在关系型数据库Mysql中查询多字段匹配数据（字段检索）</p>
<p>一般会执行下面的SQL语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from red where name &#x3D; &#39;haha&#39;;</span><br></pre></td></tr></table></figure>

<p>但在Elasticsearch中需要采用特殊的方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET red&#x2F;_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;query&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;bool&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;filter&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;term&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;about&quot;: &quot;I love to go rock climbing&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="字段分词匹配检索"><a href="#字段分词匹配检索" class="headerlink" title="字段分词匹配检索"></a>字段分词匹配检索</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET red&#x2F;_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;query&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;match&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;about&quot;: &quot;I&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="字段模糊匹配检索"><a href="#字段模糊匹配检索" class="headerlink" title="字段模糊匹配检索"></a>字段模糊匹配检索</h3><p>如果在关系型数据库Mysql中模糊查询多字段数据</p>
<p>一般会执行下面的SQL语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from user where name like &#39;%haha%&#39;</span><br></pre></td></tr></table></figure>

<p>但在Elasticsearch中需要采用特殊的方式，查询出所有文档字段值分词后包含haha的文档</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET test&#x2F;_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;query&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;fuzzy&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;aa&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;value&quot;: &quot;我是程序&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="聚合检索"><a href="#聚合检索" class="headerlink" title="聚合检索"></a>聚合检索</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET test&#x2F;_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;aggs&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;groupby_aa&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;terms&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;field&quot;: &quot;aa&quot;,</span><br><span class="line"></span><br><span class="line">&quot;size&quot;: 10</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="分页检索"><a href="#分页检索" class="headerlink" title="分页检索"></a>分页检索</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET movie_index&#x2F;movie&#x2F;_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;,</span><br><span class="line"></span><br><span class="line">&quot;from&quot;: 1,</span><br><span class="line"></span><br><span class="line">&quot;size&quot;: 1</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="索引别名-aliases"><a href="#索引别名-aliases" class="headerlink" title="索引别名 _aliases"></a>索引别名 _aliases</h2><p>索引<em>别名</em>就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何一个需要索引名的API来使用。别名带给我们极大的灵活性，允许我们做下面这些：</p>
<p>1）给多个索引分组 (例如， last_three_months)</p>
<p>2）给索引的一个子集创建视图</p>
<p>3）在运行的集群中可以无缝的从一个索引切换到另一个索引</p>
<h3 id="创建索引别名"><a href="#创建索引别名" class="headerlink" title="创建索引别名"></a>创建索引别名</h3><p>建表时直接声明</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT movie_chn_2020</span><br><span class="line"></span><br><span class="line">&#123; &quot;aliases&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;movie_chn_2020-query&quot;: &#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;mappings&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;movie&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;properties&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;id&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;long&quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;name&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;text&quot;</span><br><span class="line"></span><br><span class="line">, &quot;analyzer&quot;: &quot;ik_smart&quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;doubanScore&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;double&quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;actorList&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;properties&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;id&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;:&quot;long&quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;name&quot;:&#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;:&quot;keyword&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为已存在的索引增加别名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST _aliases</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;actions&quot;: [</span><br><span class="line"></span><br><span class="line">&#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;:</span><br><span class="line">&quot;movie_chn_2020-query&quot; &#125;&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>也可以通过加过滤条件缩小查询范围，建立一个子集视图</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST _aliases</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;actions&quot;: [</span><br><span class="line"></span><br><span class="line">&#123; &quot;add&quot;:</span><br><span class="line"></span><br><span class="line">&#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;,</span><br><span class="line"></span><br><span class="line">&quot;alias&quot;: &quot;movie_chn0919-query-zhhy&quot;,</span><br><span class="line"></span><br><span class="line">&quot;filter&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;term&quot;: &#123; &quot;actorList.id&quot;: &quot;3&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="查询别名"><a href="#查询别名" class="headerlink" title="查询别名"></a>查询别名</h3><p>与使用普通索引没有区别</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET movie_chn_2020-query&#x2F;_search</span><br></pre></td></tr></table></figure>

<h3 id="删除某个索引的别名"><a href="#删除某个索引的别名" class="headerlink" title="删除某个索引的别名"></a>删除某个索引的别名</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST _aliases</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;actions&quot;: [</span><br><span class="line"></span><br><span class="line">&#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;:</span><br><span class="line">&quot;movie_chn_2020-query&quot; &#125;&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="为某个别名进行无缝切换"><a href="#为某个别名进行无缝切换" class="headerlink" title="为某个别名进行无缝切换"></a>为某个别名进行无缝切换</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F;_aliases</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;actions&quot;: [</span><br><span class="line"></span><br><span class="line">&#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;movie_chn_xxxx&quot;, &quot;alias&quot;:</span><br><span class="line">&quot;movie_chn_2020-query&quot; &#125;&#125;,</span><br><span class="line"></span><br><span class="line">&#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;movie_chn_yyyy&quot;, &quot;alias&quot;:</span><br><span class="line">&quot;movie_chn_2020-query&quot; &#125;&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="查询别名列表"><a href="#查询别名列表" class="headerlink" title="查询别名列表"></a>查询别名列表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET _cat&#x2F;aliases?v</span><br></pre></td></tr></table></figure>

<h2 id="索引模板"><a href="#索引模板" class="headerlink" title="索引模板 "></a>索引模板 </h2><p>Index Template索引模板，顾名思义，就是创建索引的模具，其中可以定义一系列规则来帮助我们构建符合特定业务需求的索引的mappings和settings，通过使用 Index Template 可以让我们的索引具备可预知的一致性。</p>
<h3 id="常见的场景-分割索引"><a href="#常见的场景-分割索引" class="headerlink" title="常见的场景: 分割索引"></a>常见的场景: 分割索引</h3><p>分割索引就是根据时间间隔把一个业务索引切分成多个索引。比如把order_info<br>变成 order_info_20200101,order_info_20200102 …..</p>
<p>这样做的好处有两个：</p>
<p>结构变化的灵活性：因为elasticsearch不允许对数据结构进行修改。但是实际使用中索引的结构和配置难免变化，那么只要对下一个间隔的索引进行修改，原来的索引位置原状。这样就有了一定的灵活性。</p>
<p>查询范围优化：因为一般情况并不会查询全部时间周期的数据，那么通过切分索引，物理上减少了扫描数据的范围，也是对性能的优化。</p>
<h3 id="创建模板"><a href="#创建模板" class="headerlink" title="创建模板"></a>创建模板</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT _template&#x2F;template_movie2020</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;index_patterns&quot;: [&quot;movie_test*&quot;],</span><br><span class="line"></span><br><span class="line">&quot;settings&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;number_of_shards&quot;: 1</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;aliases&quot; : &#123;</span><br><span class="line"></span><br><span class="line">&quot;&#123;index&#125;-query&quot;: &#123;&#125;,</span><br><span class="line"></span><br><span class="line">&quot;movie_test-query&quot;:&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;mappings&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;_doc&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;properties&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;id&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;keyword&quot;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;movie_name&quot;: &#123;</span><br><span class="line"></span><br><span class="line">&quot;type&quot;: &quot;text&quot;,</span><br><span class="line"></span><br><span class="line">&quot;analyzer&quot;: &quot;ik_smart&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中 “index_patterns”: [“movie_test*”],<br>的含义就是凡是往movie_test开头的索引写入数据时，如果索引不存在，那么es会根据此模板自动建立索引。</p>
<blockquote>
<p>在 “aliases” 中用{index}表示，获得真正的创建的索引名。</p>
<p>测试</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST movie_test_2020xxxx&#x2F;_doc</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;id&quot;:&quot;333&quot;,</span><br><span class="line"></span><br><span class="line">&quot;name&quot;:&quot;zhang3&quot;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="查看系统中已有的模板清单"><a href="#查看系统中已有的模板清单" class="headerlink" title="查看系统中已有的模板清单"></a>查看系统中已有的模板清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET _cat&#x2F;templates</span><br></pre></td></tr></table></figure>



<h3 id="查看某个模板详情"><a href="#查看某个模板详情" class="headerlink" title="查看某个模板详情"></a>查看某个模板详情</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET _template&#x2F;template_movie2020</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET _template&#x2F;template_movie*</span><br></pre></td></tr></table></figure>



<h2 id="API操作"><a href="#API操作" class="headerlink" title="API操作"></a>API操作</h2><p>新建工程并导入依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.httpcomponents<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>httpclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>4.5.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.httpcomponents<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>httpmime<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.searchbox<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jest<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>5.3.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.java.dev.jna<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jna<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>4.5.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.codehaus.janino<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-compiler<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>6.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="写数据"><a href="#写数据" class="headerlink" title="写数据"></a>写数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.创建ES客户端连接池</span></span><br><span class="line"></span><br><span class="line">JestClientFactory factory = <span class="keyword">new</span> JestClientFactory();</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.创建ES客户端连接地址</span></span><br><span class="line"></span><br><span class="line">HttpClientConfig httpClientConfig = <span class="keyword">new</span></span><br><span class="line">HttpClientConfig.Builder(<span class="string">"http://hadoop102:9200"</span>).build();</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.设置ES连接地址</span></span><br><span class="line"></span><br><span class="line">factory.setHttpClientConfig(httpClientConfig);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.获取ES客户端连接</span></span><br><span class="line"></span><br><span class="line">JestClient jestClient = factory.getObject();</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.构建ES插入数据对象</span></span><br><span class="line"></span><br><span class="line">Index index = <span class="keyword">new</span> Index.Builder(<span class="string">"&#123;n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">" "</span>name<span class="string">":"</span>zhangsan<span class="string">",n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">" "</span>age<span class="string">":17n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">"&#125;"</span>).index(<span class="string">"test5"</span>).type(<span class="string">"_doc"</span>).id(<span class="string">"2"</span>).build();</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.执行插入数据操作</span></span><br><span class="line"></span><br><span class="line">jestClient.execute(index);</span><br><span class="line"></span><br><span class="line"><span class="comment">//7.关闭连接</span></span><br><span class="line"></span><br><span class="line">jestClient.shutdownClient();</span><br></pre></td></tr></table></figure>

<h3 id="读数据"><a href="#读数据" class="headerlink" title="读数据"></a>读数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.创建ES客户端连接池</span></span><br><span class="line"></span><br><span class="line">JestClientFactory factory = <span class="keyword">new</span> JestClientFactory();</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.创建ES客户端连接地址</span></span><br><span class="line"></span><br><span class="line">HttpClientConfig httpClientConfig = <span class="keyword">new</span></span><br><span class="line">HttpClientConfig.Builder(<span class="string">"http://hadoop102:9200"</span>).build();</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.设置ES连接地址</span></span><br><span class="line"></span><br><span class="line">factory.setHttpClientConfig(httpClientConfig);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.获取ES客户端连接</span></span><br><span class="line"></span><br><span class="line">JestClient jestClient = factory.getObject();</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.构建查询数据对象</span></span><br><span class="line"></span><br><span class="line">Search search = <span class="keyword">new</span> Search.Builder(<span class="string">"&#123;n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">" "</span>query<span class="string">": &#123;n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">" "</span>match<span class="string">": &#123;n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">" "</span>name<span class="string">": "</span>zhangsan<span class="string">"n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">" &#125;n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">" &#125;n"</span> +</span><br><span class="line"></span><br><span class="line"><span class="string">"&#125;"</span>).addIndex(<span class="string">"test5"</span>).addType(<span class="string">"_doc"</span>).build();</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.执行查询操作</span></span><br><span class="line"></span><br><span class="line">SearchResult searchResult = jestClient.execute(search);</span><br><span class="line"></span><br><span class="line"><span class="comment">//7.解析查询结果</span></span><br><span class="line"></span><br><span class="line">System.out.println(searchResult.getTotal());</span><br><span class="line"></span><br><span class="line">List&lt;SearchResult.Hit&lt;Map, Void&gt;&gt; hits =</span><br><span class="line">searchResult.getHits(Map<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (SearchResult.Hit&lt;Map, Void&gt; hit : hits) &#123;</span><br><span class="line"></span><br><span class="line">System.out.println(hit.index + <span class="string">"--"</span> + hit.id);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//8.关闭连接</span></span><br><span class="line"></span><br><span class="line">jestClient.shutdownClient();</span><br></pre></td></tr></table></figure>



<h1 id="Kibana"><a href="#Kibana" class="headerlink" title="Kibana"></a>Kibana</h1><p>Kibana的安装</p>
<p>将kibana压缩包上传到虚拟机指定目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 software]$ tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure>

<p>修改相关配置，连接Elasticsearch</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kibana]$ vi config&#x2F;kibana.yml</span><br></pre></td></tr></table></figure>

<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Kibana is served by a back end server. This setting specifies the</span></span><br><span class="line"><span class="string">port</span> <span class="string">to</span> <span class="string">use.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">server.port:</span> <span class="number">5601</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Specifies the address to which the Kibana server will bind. IP</span></span><br><span class="line"><span class="string">addresses</span> <span class="string">and</span> <span class="string">host</span> <span class="string">names</span> <span class="string">are</span> <span class="string">both</span> <span class="string">valid</span> <span class="string">values.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The default is 'localhost', which usually means remote machines</span></span><br><span class="line"><span class="string">will</span> <span class="string">not</span> <span class="string">be</span> <span class="string">able</span> <span class="string">to</span> <span class="string">connect.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To allow connections from remote users, set this parameter to a</span></span><br><span class="line"><span class="string">non-loopback</span> <span class="string">address.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">server.host:</span> <span class="string">"192.168.9.102"</span></span><br><span class="line"></span><br><span class="line"><span class="string">...</span> <span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="string">...</span> <span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The URL of the Elasticsearch instance to use for all your queries.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">elasticsearch.url:</span> <span class="string">"http://192.168.9.102:9200"</span></span><br></pre></td></tr></table></figure>

<p>启动Kibana</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kibana]$ bin&#x2F;kibana</span><br></pre></td></tr></table></figure>

<p><img src="/bigdata/ElasticSearch%E4%BB%8B%E7%BB%8D/%E5%9B%BE%E7%89%874.png" alt="图片4"></p>
<p>修改之前的ES启动脚本为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">es_home=/opt/module/elasticsearch</span><br><span class="line"></span><br><span class="line">kibana_home=/opt/module/kibana</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line"><span class="string">"start"</span>) &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"==============<span class="variable">$i</span>=============="</span></span><br><span class="line"></span><br><span class="line">ssh <span class="variable">$i</span> <span class="string">"source /etc/profile;<span class="variable">$&#123;es_home&#125;</span>/bin/elasticsearch &gt;/dev/null</span></span><br><span class="line"><span class="string">2&gt;&amp;1 &amp;"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">nohup <span class="variable">$&#123;kibana_home&#125;</span>/bin/kibana &gt; kibana.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="string">"stop"</span>) &#123;</span><br><span class="line"></span><br><span class="line">ps -ef | grep <span class="variable">$&#123;kibana_home&#125;</span> | grep -v grep | awk <span class="string">'&#123;print $2&#125;'</span>|</span><br><span class="line">xargs <span class="built_in">kill</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"==============<span class="variable">$i</span>=============="</span></span><br><span class="line"></span><br><span class="line">ssh <span class="variable">$i</span> <span class="string">"ps -ef|grep <span class="variable">$es_home</span> |grep -v grep|awk '&#123;print</span></span><br><span class="line"><span class="string"><span class="variable">$2</span>&#125;'|xargs kill"</span> &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis系列（二）</title>
    <url>/bigdata/Redis%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Redis持久化机制"><a href="#Redis持久化机制" class="headerlink" title="Redis持久化机制"></a>Redis持久化机制</h1><p><a href="https://redis.io/topics/persistence#snapshotting" target="_blank" rel="noopener">官网描述</a></p>
<p>持久化的话是<strong>Redis</strong>高可用中比较重要的一个环节。工作时数据都存储在内存中，万一服务器断电，则所有数据都会丢失。针对这种情况，Redis采用持久化机制来增强数据安全性。</p>
<h2 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h2><p><strong>RDB</strong> 持久化机制，是对 <strong>Redis</strong> 中的数据执行<strong>周期性</strong>的持久化。</p>
<h3 id="①机制描述"><a href="#①机制描述" class="headerlink" title="①机制描述"></a>①机制描述</h3><p>每隔一定的时间把内存中的数据作为一个快照保存到硬盘上的文件中。Redis默认开启RDB机制。</p>
<h3 id="②触发时机"><a href="#②触发时机" class="headerlink" title="②触发时机"></a>②触发时机</h3><h4 id="1-基于默认配置"><a href="#1-基于默认配置" class="headerlink" title="[1]基于默认配置"></a>[1]基于默认配置</h4><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">save</span> <span class="string">900 1</span></span><br><span class="line"><span class="attr">save</span> <span class="string">300 10</span></span><br><span class="line"><span class="attr">save</span> <span class="string">60 10000</span></span><br></pre></td></tr></table></figure>

<p>含义</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>save 900 1</td>
<td>900秒内至少有一次修改则触发保存操作</td>
</tr>
<tr>
<td>save 300 10</td>
<td>300秒内至少有10次修改则触发保存操作</td>
</tr>
<tr>
<td>save 60 10000</td>
<td>60秒内至少有1万次修改则触发保存操作</td>
</tr>
</tbody></table>
<h4 id="2-使用保存命令"><a href="#2-使用保存命令" class="headerlink" title="[2]使用保存命令"></a>[2]使用保存命令</h4><p>save或bgsave</p>
<h4 id="3-使用flushall命令"><a href="#3-使用flushall命令" class="headerlink" title="[3]使用flushall命令"></a>[3]使用flushall命令</h4><p>这个命令也会产生dump.rdb文件，但里面是空的，没有意义</p>
<h4 id="4-服务器关闭"><a href="#4-服务器关闭" class="headerlink" title="[4]服务器关闭"></a>[4]服务器关闭</h4><p>如果执行SHUTDOWN命令让Redis正常退出，那么此前Redis就会执行一次持久化保存。</p>
<h3 id="③相关配置"><a href="#③相关配置" class="headerlink" title="③相关配置"></a>③相关配置</h3><table>
<thead>
<tr>
<th>配置项</th>
<th>取值</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>save</td>
<td>“”</td>
<td>禁用RDB机制</td>
</tr>
<tr>
<td>dbfilename</td>
<td>文件名，例如：dump.rdb</td>
<td>设置RDB机制下，数据存储文件的文件名</td>
</tr>
<tr>
<td>dir</td>
<td>Redis工作目录路径</td>
<td>指定存放持久化文件的目录的路径。注意：这里指定的必须是目录不能是文件名</td>
</tr>
</tbody></table>
<h3 id="④思考"><a href="#④思考" class="headerlink" title="④思考"></a>④思考</h3><p>RDB机制能够保证数据的绝对安全吗？</p>
<h2 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h2><p><strong>AOF</strong> 机制对每条写入命令作为日志，以 <strong>append-only</strong> 的模式写入一个日志文件中，因为这个模式是只追加的方式，所以没有任何磁盘寻址的开销，所以很快，有点像Mysql中的<strong>binlog</strong>。</p>
<h3 id="①机制描述-1"><a href="#①机制描述-1" class="headerlink" title="①机制描述"></a>①机制描述</h3><p>根据配置文件中指定的策略，把生成数据的命令保存到硬盘上的文件中。一个AOF文件的内容可以参照下面的例子：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">*2</span></span><br><span class="line"><span class="attr">$6</span></span><br><span class="line"><span class="attr">SELECT</span></span><br><span class="line"><span class="attr">$1</span></span><br><span class="line"><span class="attr">0</span></span><br><span class="line"><span class="attr">*3</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">set</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">num</span></span><br><span class="line"><span class="attr">$2</span></span><br><span class="line"><span class="attr">10</span></span><br><span class="line"><span class="attr">*2</span></span><br><span class="line"><span class="attr">$4</span></span><br><span class="line"><span class="attr">incr</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">num</span></span><br><span class="line"><span class="attr">*2</span></span><br><span class="line"><span class="attr">$4</span></span><br><span class="line"><span class="attr">incr</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">num</span></span><br><span class="line"><span class="attr">*2</span></span><br><span class="line"><span class="attr">$4</span></span><br><span class="line"><span class="attr">incr</span></span><br><span class="line"><span class="attr">$3</span></span><br><span class="line"><span class="attr">num</span></span><br></pre></td></tr></table></figure>

<p>生成上面文件内容的Redis命令是：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">set num 10</span><br><span class="line">incr num</span><br><span class="line">incr num</span><br><span class="line">incr num</span><br></pre></td></tr></table></figure>

<h3 id="②AOF基本配置"><a href="#②AOF基本配置" class="headerlink" title="②AOF基本配置"></a>②AOF基本配置</h3><table>
<thead>
<tr>
<th>配置项</th>
<th>取值</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>appendonly</td>
<td>yes</td>
<td>启用AOF持久化机制</td>
</tr>
<tr>
<td></td>
<td>no</td>
<td>禁用AOF持久化机制[默认值]</td>
</tr>
<tr>
<td>appendfilename</td>
<td>“文件名”</td>
<td>AOF持久化文件名</td>
</tr>
<tr>
<td>dir</td>
<td>Redis工作目录路径</td>
<td>指定存放持久化文件的目录的路径。注意：这里指定的必须是目录不能是文件名</td>
</tr>
<tr>
<td>appendfsync</td>
<td>always</td>
<td>每一次数据修改后都将执行文件写入操作，缓慢但是最安全。</td>
</tr>
<tr>
<td></td>
<td>everysec</td>
<td>每秒执行一次写入操作。折中。</td>
</tr>
<tr>
<td></td>
<td>no</td>
<td>由操作系统在适当的时候执行写入操作，最快。</td>
</tr>
</tbody></table>
<h3 id="③AOF重写"><a href="#③AOF重写" class="headerlink" title="③AOF重写"></a>③AOF重写</h3><p>对比下面两组命令：</p>
<table>
<thead>
<tr>
<th>AOF重写前</th>
<th>AOF重写后</th>
</tr>
</thead>
<tbody><tr>
<td>set count 1<br>incr count<br>incr count<br>incr count</td>
<td>set count 4</td>
</tr>
</tbody></table>
<p>两组命令执行后对于count来说最终的值是一致的，但是进行AOF重写后省略了中间过程，可以让AOF文件体积更小。而Redis会根据AOF文件的体积来决定是否进行AOF重写。参考的配置项如下：</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>auto-aof-rewrite-percentage 100</td>
<td>文件体积增大100%时执行AOF重写</td>
</tr>
<tr>
<td>auto-aof-rewrite-min-size 64mb</td>
<td>文件体积增长到64mb时执行AOF重写</td>
</tr>
</tbody></table>
<p>实际工作中不要进行频繁的AOF重写，因为CPU资源和硬盘资源二者之间肯定是CPU资源更加宝贵，所以不应该过多耗费CPU性能去节省硬盘空间。</p>
<h2 id="持久化文件损坏修复"><a href="#持久化文件损坏修复" class="headerlink" title="持久化文件损坏修复"></a>持久化文件损坏修复</h2><p>Redis服务器启动时如果读取了损坏的持久化文件会导致启动失败，此时为了让Redis服务器能够正常启动，需要对损坏的持久化文件进行修复。这里以AOF文件为例介绍修复操作的步骤。</p>
<ul>
<li><p>第一步：备份要修复的appendonly.aof文件</p>
</li>
<li><p>第二步：执行修复程序</p>
<p>/usr/local/redis/bin/redis-check-aof –fix /usr/local/redis/appendonly.aof</p>
</li>
<li><p>第三步：重启Redis</p>
</li>
</ul>
<p>注意：所谓修复持久化文件仅仅是把损坏的部分去掉，而没法把受损的数据找回。</p>
<h2 id="扩展阅读：两种持久化机制的取舍"><a href="#扩展阅读：两种持久化机制的取舍" class="headerlink" title="扩展阅读：两种持久化机制的取舍"></a>扩展阅读：两种持久化机制的取舍</h2><h3 id="两种机制对比"><a href="#两种机制对比" class="headerlink" title="两种机制对比"></a>两种机制对比</h3><table>
<thead>
<tr>
<th align="center"></th>
<th align="center">RDB</th>
<th align="center">AOF</th>
</tr>
</thead>
<tbody><tr>
<td align="center">优势</td>
<td align="center">适合大规模的数据恢复、速度较快、对<strong>Redis</strong>的性能影响非常小</td>
<td align="center">选择appendfsync always方式运行时理论上能够做到数据完整一致（一秒一次），但此时性能不好。文件内容（append-only）具备一定可读性，能够用来分析Redis工作情况。</td>
</tr>
<tr>
<td align="center">劣势</td>
<td align="center">会丢失最后一次快照后的所有修改，不能绝对保证数据的高度一致性和完整性。Fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑（在生成数据快照的时候，如果文件很大，客户端可能会暂停几毫秒甚至几秒），但上述成立有条件，Linux也有优化手段</td>
<td align="center">持久化相同的数据，文件体积比RDB大，恢复速度比RDB慢。效率在同步写入时低于RDB，不同步写入时与RDB相同。</td>
</tr>
<tr>
<td align="center">适合场景</td>
<td align="center">冷备（他会生成多个数据文件，每个数据文件分别都代表了某一时刻<strong>Redis</strong>里面的数据）</td>
<td align="center">热备（<strong>AOF</strong>的日志是通过一个叫<strong>非常可读</strong>的方式记录的，这样的特性适合做<strong>灾难性数据误删除</strong>）</td>
</tr>
</tbody></table>
<h3 id="RDB和AOF并存"><a href="#RDB和AOF并存" class="headerlink" title="RDB和AOF并存"></a>RDB和AOF并存</h3><p>Redis重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整</p>
<p>RDB的数据不实时，同时使用两者时服务器重启也只会找AOF文件。那要不要只使用AOF呢？作者建议不要，因为RDB更适合用于备份数据库(AOF在不断变化不好备份)、快速重启，而且不会有AOF可能潜在的bug，留着作为一个万一的手段。</p>
<h3 id="使用建议"><a href="#使用建议" class="headerlink" title="使用建议"></a>使用建议</h3><p>如果Redis仅仅作为缓存可以不使用任何持久化方式。</p>
<p>其他应用方式综合考虑性能和完整性、一致性要求。</p>
<p>RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够了，只保留save 900 1这条规则。如果Enalbe AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自己的AOF文件就可以了。代价一是带来了持续的IO，二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上。默认超过原大小100%大小时重写可以改到适当的数值。如果不开启AOF，仅靠Master-Slave Replication 实现高可用性能也不错。能省掉一大笔IO也减少了rewrite时带来的系统波动。代价是如果Master/Slave同时倒掉，会丢失十几分钟的数据，启动脚本也要比较两个Master/Slave中的RDB文件，载入较新的那个。新浪微博就选用了这种架构。</p>
<h1 id="Redis事务控制"><a href="#Redis事务控制" class="headerlink" title="Redis事务控制"></a>Redis事务控制</h1><h2 id="Redis事务控制的相关命令"><a href="#Redis事务控制的相关命令" class="headerlink" title="Redis事务控制的相关命令"></a>Redis事务控制的相关命令</h2><table>
<thead>
<tr>
<th>命令名</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>MULTI</td>
<td>表示开始收集命令，后面所有命令都不是马上执行，而是加入到一个队列中。</td>
</tr>
<tr>
<td>EXEC</td>
<td>执行MULTI后面命令队列中的所有命令。</td>
</tr>
<tr>
<td>DISCARD</td>
<td>放弃执行队列中的命令。</td>
</tr>
<tr>
<td>WATCH</td>
<td>“观察“、”监控“一个KEY，在当前队列外的其他命令操作这个KEY时，放弃执行自己队列的命令</td>
</tr>
<tr>
<td>UNWATCH</td>
<td>放弃监控一个KEY</td>
</tr>
</tbody></table>
<h2 id="2-命令队列执行失败的两种情况"><a href="#2-命令队列执行失败的两种情况" class="headerlink" title="2.命令队列执行失败的两种情况"></a>2.命令队列执行失败的两种情况</h2><h3 id="①加入队列时失败"><a href="#①加入队列时失败" class="headerlink" title="①加入队列时失败"></a>①加入队列时失败</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; multi</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set age 20</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; incr age</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; incr age www</span><br><span class="line">(error) ERR wrong number of arguments for &#39;incr&#39; command</span><br><span class="line">127.0.0.1:6379&gt; exec</span><br><span class="line">(error) EXECABORT Transaction discarded because of previous errors.</span><br></pre></td></tr></table></figure>

<p>遇到了入队时即可检测到的错误，整个队列都不会执行。</p>
<h3 id="②执行队列时失败"><a href="#②执行队列时失败" class="headerlink" title="②执行队列时失败"></a>②执行队列时失败</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; multi</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set age 30</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; incrby age 5</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; incrby age 5</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; incrby age ww</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; incrby age 5</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; EXEC</span><br><span class="line">1) OK</span><br><span class="line">2) (integer) 35</span><br><span class="line">3) (integer) 40</span><br><span class="line">4) (error) ERR value is not an integer or out of range</span><br><span class="line">5) (integer) 45</span><br><span class="line">127.0.0.1:6379&gt; get age</span><br><span class="line">"45"</span><br></pre></td></tr></table></figure>

<p>错误在入队时检测不出来，整个队列执行时有错的命令执行失败，但是其他命令并没有回滚。</p>
<h3 id="③Redis为什么不支持回滚"><a href="#③Redis为什么不支持回滚" class="headerlink" title="③Redis为什么不支持回滚"></a>③Redis为什么不支持回滚</h3><p>官方解释如下：</p>
<blockquote>
<pre><code>    如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。以下是这种做法的优点：
    1.Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。
    2.因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。

有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 INCR 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR ， 回滚是没有办法处理这些情况的。</code></pre></blockquote>
<h2 id="3-悲观锁和乐观锁"><a href="#3-悲观锁和乐观锁" class="headerlink" title="3.悲观锁和乐观锁"></a>3.悲观锁和乐观锁</h2><p>在使用WATCH命令监控一个KEY后，当前队列中的命令会由于外部命令的执行而放弃，这是乐观锁的体现。</p>
<ul>
<li><p>悲观锁</p>
<p>认为当前环境非常容易发生碰撞，所以执行操作前需要把数据锁定，操作完成后释放锁，其他操作才可以继续操作。</p>
</li>
<li><p>乐观锁</p>
<p>认为当前环境不容易发生碰撞，所以执行操作前不锁定数据，万一碰撞真的发生了，那么放弃自己的操作。</p>
</li>
</ul>
<p>redis锁机制：<a href="https://blog.csdn.net/shuangyueliao/article/details/89344256" target="_blank" rel="noopener">https://blog.csdn.net/shuangyueliao/article/details/89344256</a></p>
<h1 id="一致性问题"><a href="#一致性问题" class="headerlink" title="一致性问题"></a>一致性问题</h1><p>一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统<strong>不是严格要求</strong> “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：<strong>读请求和写请求串行化</strong>，串到一个<strong>内存队列</strong>里去。</p>
<p>串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。</p>
<p>把一些列的操作都放到队列里面，顺序肯定不会乱，但是并发高了，这队列很容易阻塞，反而会成为整个系统的弱点，瓶颈</p>
<p>本段摘抄自：<a href="https://github.com/AobingJava/JavaFamily/blob/master/docs/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E3%80%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E3%80%81%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.md" target="_blank" rel="noopener">https://github.com/AobingJava/JavaFamily/blob/master/docs/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E3%80%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E3%80%81%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.md</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>IDE中常用的插件</title>
    <url>/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="IDE中常用的插件"><a href="#IDE中常用的插件" class="headerlink" title="IDE中常用的插件"></a>IDE中常用的插件</h1><h2 id="第1章-在Eclipse中使用Maven"><a href="#第1章-在Eclipse中使用Maven" class="headerlink" title="第1章 在Eclipse中使用Maven"></a>第1章 在Eclipse中使用Maven</h2><h3 id="1-1-安装Maven核心程序"><a href="#1-1-安装Maven核心程序" class="headerlink" title="1.1 安装Maven核心程序"></a>1.1 安装Maven核心程序</h3><p>1) 下载地址：<a href="http://maven.apache.org/" target="_blank" rel="noopener">http://maven.apache.org/</a></p>
<p>2) 检查JAVA_HOME环境变量。Maven是使用Java开发的，所以必须知道当前系统环境中JDK的安装目录。</p>
<p>C:\Users\red&gt;echo %JAVA_HOME%D:\Java\jdk1.8.0_111</p>
<p>3) 解压Maven的核心程序。</p>
<p>将apache-maven-3.6.3-bin.zip解压到一个非中文无空格的目录下。例如：D:\apache-maven-3.6.3</p>
<p>4) 配置环境变量。</p>
<table>
<thead>
<tr>
<th>MAVEN_HOME</th>
</tr>
</thead>
<tbody><tr>
<td>D:\apache-maven-3.6.3</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>path</th>
</tr>
</thead>
<tbody><tr>
<td>%MAVEN_HOME%\bin</td>
</tr>
</tbody></table>
<p>5) ④查看Maven版本信息验证安装是否正确</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\red&gt;mvn -vApache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)Maven home: D:\apache-maven-3.6.3\bin\..Java version: 1.8.0_111, vendor: Oracle Corporation, runtime: D:\Java\jdk1.8.0_111\jreDefault locale: zh_CN, platform encoding: GBKOS name: &quot;windows 10&quot;, version: &quot;10.0&quot;, arch: &quot;amd64&quot;, family: &quot;windows&quot;</span><br></pre></td></tr></table></figure>

<h3 id="1-2-配置本地仓库和阿里云镜像"><a href="#1-2-配置本地仓库和阿里云镜像" class="headerlink" title="1.2 配置本地仓库和阿里云镜像"></a>1.2 配置本地仓库和阿里云镜像</h3><h4 id="1-2-1-配置本地仓库"><a href="#1-2-1-配置本地仓库" class="headerlink" title="1.2.1 配置本地仓库"></a>1.2.1 配置本地仓库</h4><p>1) Maven的核心程序并不包含具体功能，仅负责宏观调度。具体功能由插件来完成。Maven核心程序会到本地仓库中查找插件。如果本地仓库中没有就会从远程中央仓库下载。此时如果不能上网则无法执行Maven的具体功能。为了解决这个问题，我们可以将Maven的本地仓库指向一个在联网情况下下载好的目录。</p>
<p>2) Maven默认的本地仓库：~.m2\repository目录。</p>
<p>Tips：~表示当前用户的家目录。</p>
<p>3) 找到Maven的核心配置文件settings.xml文件：</p>
<p>解压目录D:\apache-maven-3.6.3\conf\settings.xml</p>
<p>4) 设置方式</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">localRepository</span>&gt;</span>本地仓库的路径<span class="tag">&lt;/<span class="name">localRepository</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">localRepository</span>&gt;</span>E:\LocalRepository<span class="tag">&lt;/<span class="name">localRepository</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h4 id="1-2-2-配置阿里云镜像"><a href="#1-2-2-配置阿里云镜像" class="headerlink" title="1.2.2 配置阿里云镜像"></a>1.2.2 配置阿里云镜像</h4><p>为了下载jar包方便，在Maven的核心配置文件settings.xml文件的<mirrors></mirrors>标签里面配置以下标签：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="1-3-在Eclipse中配置Maven"><a href="#1-3-在Eclipse中配置Maven" class="headerlink" title="1.3 在Eclipse中配置Maven"></a>1.3 在Eclipse中配置Maven</h3><p>Eclipse中默认自带Maven插件，但是自带的Maven插件不能修改本地仓库，所以通常我们不使用自带的Maven，而是使用自己安装的，在Eclipse中配置Maven的步骤如下：</p>
<p>1) 点击Eclipse中的Window→Preferences</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps1.jpg" alt="img"> </p>
<p>2) 点开Maven前面的箭头，选择Installations，点击Add…</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps2.jpg" alt="img"> </p>
<p>3) 点击Directory…选择我们安装的Maven核心程序的根目录，然后点击Finish</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps3.jpg" alt="img"> </p>
<p>4) 勾上添加的Maven核心程序</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps4.jpg" alt="img"> </p>
<p>5) 选择Maven下的User Settings，在全局设置哪儿点击Browse…选择Maven核心程序的配置文件settings.xml，本地仓库会自动变为我们在settings.xml文件中设置的路径</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps5.jpg" alt="img"> </p>
<h3 id="1-4-在Eclipse中创建Maven项目"><a href="#1-4-在Eclipse中创建Maven项目" class="headerlink" title="1.4 在Eclipse中创建Maven项目"></a>1.4 在Eclipse中创建Maven项目</h3><h4 id="1-4-1-创建Java工程"><a href="#1-4-1-创建Java工程" class="headerlink" title="1.4.1 创建Java工程"></a>1.4.1 创建Java工程</h4><p>1) 点击File→New→Maven Project，弹出如下窗口</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps6.jpg" alt="img"> </p>
<p>2) 点击Next，配置坐标（GAV）及打包方式，然后点击Finish</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps7.jpg" alt="img"> </p>
<p>3) 创建完工程之后发现默认的JDK的编译版本是1.5，在Maven的核心配置文件settings.xml文件中添加以下配置将编译版本改为1.8，重启Eclipse即可</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">profile</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>jdk-1.8<span class="tag">&lt;/<span class="name">id</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">activation</span>&gt;</span>	</span><br><span class="line">        <span class="tag">&lt;<span class="name">activeByDefault</span>&gt;</span>true<span class="tag">&lt;/<span class="name">activeByDefault</span>&gt;</span>	</span><br><span class="line">        <span class="tag">&lt;<span class="name">jdk</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">jdk</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">activation</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span>	 </span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span>	</span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.compilerVersion</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.compilerVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>4) 配置Maven的核心配置文件pom.xml文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">**</span>"<span class="attr">http:</span>//<span class="attr">maven.apache.org</span>/<span class="attr">POM</span>/<span class="attr">4.0.0</span>"** <span class="attr">xmlns:xsi</span>=<span class="string">**</span>"<span class="attr">http:</span>//<span class="attr">www.w3.org</span>/<span class="attr">2001</span>/<span class="attr">XMLSchema-instance</span>"** <span class="attr">xsi:schemaLocation</span>=<span class="string">**</span>"<span class="attr">http:</span>//<span class="attr">maven.apache.org</span>/<span class="attr">POM</span>/<span class="attr">4.0.0</span> <span class="attr">https:</span>//<span class="attr">maven.apache.org</span>/<span class="attr">xsd</span>/<span class="attr">maven-4.0.0.xsd</span>"**&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.red.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>Hello<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span> 	</span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span> 		</span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> 		</span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> 		</span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span> 		</span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span> 	</span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>5) 编写主代码</p>
<p>在src/main/java目录下创建包并创建Hello.java文件</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.maven; </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;	</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">sayHello</span><span class="params">(String name)</span></span>&#123;	    </span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Hello "</span>+name+<span class="string">"!"</span>;	  </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>6) 编写测试代码</p>
<p>在src/test/java目录下创建包并创建HelloTest.java文件</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.maven; </span><br><span class="line"><span class="keyword">import</span> org.junit.Test; </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloTest</span> </span>&#123; </span><br><span class="line">    <span class="meta">@Test</span>	</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testHello</span><span class="params">()</span> </span>&#123;		</span><br><span class="line">        Hello hello = <span class="keyword">new</span> Hello();		</span><br><span class="line">        String maven = hello.sayHello(<span class="string">"Maven"</span>);	</span><br><span class="line">        System.*out\**\***.println(maven);	</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>7) 使用Maven的方式运行Maven工程</p>
<p>在工程名Hello或pom.xml上右键→Run As运行Maven项目</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps8.jpg" alt="img"> </p>
<h4 id="1-4-2-创建Web工程（了解）"><a href="#1-4-2-创建Web工程（了解）" class="headerlink" title="1.4.2 创建Web工程（了解）"></a>1.4.2 创建Web工程（了解）</h4><p>1) 创建简单的Maven工程，打包方式为war包</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps9.jpg" alt="img"> </p>
<p>2) 创建完成之后因缺少web.xml文件工程出现小红叉</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps10.jpg" alt="img"> </p>
<p>3) 在工程上右键→Build Path→Configure Build Path…</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps11.jpg" alt="img"> </p>
<p>4) 点击Project Facets欺骗Eclipse当前工程不是Web工程，点击应用</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps12.jpg" alt="img"> </p>
<p>5) 再告诉Eclipse当前工程是一个Web工程，点击应用并关闭</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps13.jpg" alt="img"> </p>
<p>6) 发现MavenWeb工程小红叉消失，并出现了WebContext目录</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps14.jpg" alt="img"> </p>
<p>7) 在WebContext下创建index.jsp页面并添加Tomcat库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps15.jpg" alt="img"> </p>
<p>8) 在MavenWeb上右键→Run As→Run on Server部署到Tomcat上运行</p>
<h4 id="1-4-3-创建父工程"><a href="#1-4-3-创建父工程" class="headerlink" title="1.4.3 创建父工程"></a>1.4.3 创建父工程</h4><p>​    父工程的打包方式为pom，父工程只需要保留pom.xml文件即可</p>
<p>1) 创建简单的Maven工程，打包方式选择pom</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps16.jpg" alt="img"> </p>
<p>2) 在pom.xml文件中通过<dependencyManagement></dependencyManagement>标签进行依赖管理</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">**</span>"<span class="attr">http:</span>//<span class="attr">maven.apache.org</span>/<span class="attr">POM</span>/<span class="attr">4.0.0</span>"** <span class="attr">xmlns:xsi</span>=<span class="string">**</span>"<span class="attr">http:</span>//<span class="attr">www.w3.org</span>/<span class="attr">2001</span>/<span class="attr">XMLSchema-instance</span>"** <span class="attr">xsi:schemaLocation</span>=<span class="string">**</span>"<span class="attr">http:</span>//<span class="attr">maven.apache.org</span>/<span class="attr">POM</span>/<span class="attr">4.0.0</span> <span class="attr">https:</span>//<span class="attr">maven.apache.org</span>/<span class="attr">xsd</span>/<span class="attr">maven-4.0.0.xsd</span>"**&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.red.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>Parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span> </span><br><span class="line">    <span class="comment">&lt;!-- 依赖管理 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span> 		</span><br><span class="line">        <span class="comment">&lt;!-- 在此配置要管理的依赖 --&gt;</span> 	</span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3) 在子工程中继承父工程</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 继承 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span>     </span><br><span class="line">    <span class="comment">&lt;!-- 在此配置父工程的坐标 --&gt;</span>	</span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="1-5-在Eclipse中导入Maven项目"><a href="#1-5-在Eclipse中导入Maven项目" class="headerlink" title="1.5 在Eclipse中导入Maven项目"></a>1.5 在Eclipse中导入Maven项目</h3><p>1) 点击File→Import…</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps17.jpg" alt="img"> </p>
<p>2) 第一次导入手动创建的Maven项目时，由于项目中没有Eclipse生成的一些文件，使用方式一导入时Eclipse认为它不是一个工程</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps18.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps19.jpg" alt="img"> </p>
<p>3) 所以必须通过方式二导入到Eclipse中</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps20.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps21.jpg" alt="img"> </p>
<p>4) 导入到Eclipse中之后就会生成一些Eclipse能识别的文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps22.jpg" alt="img"> </p>
<p>5) 有了这些Eclipse能识别的文件之后以后再往Eclipse中导入的时候选择方式一和方式二都可以</p>
<h2 id="第2章-在Idea中使用Maven"><a href="#第2章-在Idea中使用Maven" class="headerlink" title="第2章 在Idea中使用Maven"></a>第2章 在Idea中使用Maven</h2><h3 id="2-1-在Idea中配置Maven"><a href="#2-1-在Idea中配置Maven" class="headerlink" title="2.1 在Idea中配置Maven"></a>2.1 在Idea中配置Maven</h3><p>​    Idea中也自带Maven插件，而且我们也可以给自带的Maven插件进行配置，所以我们可以使用自带的Maven，也可以使用我们安装的Maven核心程序</p>
<h4 id="2-1-1-配置自带的Maven插件"><a href="#2-1-1-配置自带的Maven插件" class="headerlink" title="2.1.1 配置自带的Maven插件"></a>2.1.1 配置自带的Maven插件</h4><p>1) Idea自带的Maven在Idea的安装目录的plugins目录中</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps23.jpg" alt="img"> </p>
<p>2) 在自带的Maven里配置了本地仓库之后打开Idea之后会发现本地仓库自动变成了我们设置的仓库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps24.jpg" alt="img"> </p>
<p>3) 设置Maven自动导包</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps25.jpg" alt="img"> </p>
<h4 id="2-1-2-配置我们自己安装的Maven"><a href="#2-1-2-配置我们自己安装的Maven" class="headerlink" title="2.1.2 配置我们自己安装的Maven"></a>2.1.2 配置我们自己安装的Maven</h4><p>1) 点击工具栏中的Settings</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps26.jpg" alt="img"> </p>
<p>2) 点击OK保存即可</p>
<h3 id="2-2-在Idea中创建Maven项目"><a href="#2-2-在Idea中创建Maven项目" class="headerlink" title="2.2 在Idea中创建Maven项目"></a>2.2 在Idea中创建Maven项目</h3><h4 id="2-2-1-创建Java工程"><a href="#2-2-1-创建Java工程" class="headerlink" title="2.2.1 创建Java工程"></a>2.2.1 创建Java工程</h4><p>1) 点击File→New→Module…（如果之前没有Project选Project）→Maven</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps27.jpg" alt="img"> </p>
<p>2) 点击Next，配置要继承的模块（如果直接创建的是Project不存在这一项）、坐标（GAV）、路径。不同的Idea版本可能有所差别，我使用的是2019.3.3的版本</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps28.jpg" alt="img"> </p>
<p>3) 点击Finish即可创建成功</p>
<p>4) 配置Maven的核心配置文件pom.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">**<span class="php"><span class="meta">&lt;?</span>**xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>**<span class="meta">?&gt;</span></span>**** **</span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span>      <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span>      <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.red.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>Hello<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span>     </span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span>      </span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>       </span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>       </span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span>       </span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span>    </span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>5) 编写主代码</p>
<p>在src/main/java目录下创建包并创建Hello.java文件</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.maven; </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;   </span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">sayHello</span><span class="params">(String name)</span></span>&#123;    </span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Hello "</span>+name+<span class="string">"!"</span>;  </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>6) 编写测试代码</p>
<p>​    在/src/test/java目录下创建包并创建HelloTest.java文件</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.maven;  </span><br><span class="line"><span class="keyword">import</span> org.junit.Test;  </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloTest</span> </span>&#123;    </span><br><span class="line">    <span class="meta">@Test</span>   </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testHello</span><span class="params">()</span></span>&#123;     </span><br><span class="line">        Hello hello = <span class="keyword">new</span> Hello();    </span><br><span class="line">        String maven = hello.sayHello(<span class="string">"Maven"</span>);    </span><br><span class="line">        System.*out\**\***.println(maven);  </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>7) 使用Maven的方式运行Maven工程</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps29.jpg" alt="img"> </p>
<h4 id="2-2-2-创建Web工程（了解）"><a href="#2-2-2-创建Web工程（了解）" class="headerlink" title="2.2.2 创建Web工程（了解）"></a>2.2.2 创建Web工程（了解）</h4><p>1) 创建简单的Maven工程，打包方式为war包</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.red.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>MavenWeb<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">packaging</span>&gt;</span>war<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>2) 点击Project Structure</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps30.jpg" alt="img"> </p>
<p>3) 选择对应的Module，设置Web目录</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps31.jpg" alt="img"> </p>
<p>4) 弹出提示框，选择版本后点击OK</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps32.jpg" alt="img"> </p>
<p>5) 生成web.xml文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps33.jpg" alt="img"> </p>
<p>6) 设置存放web页面文件的目录后点击OK</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps34.jpg" alt="img"> </p>
<p>7) 点击OK</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps35.jpg" alt="img"> </p>
<p>8) 发现项目中多了一个web目录，而且目录上有一个蓝点</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps36.jpg" alt="img"> </p>
<p>9) 在web目录下创建index.jsp页面</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps37.jpg" alt="img"> </p>
<p>10) 部署到Tomcat上运行</p>
<h3 id="2-3-在Idea中导入Maven项目"><a href="#2-3-在Idea中导入Maven项目" class="headerlink" title="2.3 在Idea中导入Maven项目"></a>2.3 在Idea中导入Maven项目</h3><p>1) 点击Project Structure</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps38.jpg" alt="img"> </p>
<p>2) 点击Modules→➕→Import Module</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps39.jpg" alt="img"> </p>
<p>3) 找到项目所在的位置</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps40.jpg" alt="img"> </p>
<p>4) 选择Import module from external model（从外部模型导入模块）→Maven→Finish</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps41.jpg" alt="img"> </p>
<h2 id="第3章-在Eclipse中使用Git"><a href="#第3章-在Eclipse中使用Git" class="headerlink" title="第3章 在Eclipse中使用Git"></a>第3章 在Eclipse中使用Git</h2><p>​    Eclipse中默认自带了Git插件，通过点击Help→About Eclipse IDE可以查看</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps42.jpg" alt="img"> </p>
<h3 id="3-1-全局配置"><a href="#3-1-全局配置" class="headerlink" title="3.1 全局配置"></a>3.1 全局配置</h3><h4 id="3-1-1-配置用户名和邮箱"><a href="#3-1-1-配置用户名和邮箱" class="headerlink" title="3.1.1 配置用户名和邮箱"></a>3.1.1 配置用户名和邮箱</h4><p>1) 点击Window→Preferences→Team→Git→Configuration</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps43.jpg" alt="img"> </p>
<p>2) 点击Add Entry…设置全局用户名和邮箱</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps44.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps45.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps46.jpg" alt="img"> </p>
<p>3) 点击Apply and Close之后在Windows的用户目录下会生成.gitconfig配置文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps47.jpg" alt="img"> </p>
<h4 id="3-1-2-配置忽略的文件"><a href="#3-1-2-配置忽略的文件" class="headerlink" title="3.1.2 配置忽略的文件"></a>3.1.2 配置忽略的文件</h4><p>1) 在用户目录（其他目录也可以）创建Java.gitignore文件，添加以下内容</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Compiled class file*.class </span><br><span class="line">    # Log file*.log </span><br><span class="line">    # BlueJ files*.ctxt</span><br><span class="line">    # Mobile Tools for Java (J2ME).mtj.tmp/ </span><br><span class="line">        # Package Files #*.jar*.war*.nar*.ear*.zip*.tar.gz*.rar </span><br><span class="line">        # virtual machine crash logs, see</span><br><span class="line">        http:<span class="comment">//www.java.com/en/download/help/error_hotspot.xmlhs_err_pid* .classpath.project.settingstarget</span></span><br></pre></td></tr></table></figure>

<p>2) 在全局的配置文件.gitconfig文件中添加如下内容</p>
<blockquote>
<p>[core]    excludesfile = C:/Users/red/Java.gitignore</p>
</blockquote>
<p>3) 文件所在位置图</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps48.jpg" alt="img"> </p>
<p>4) 重启Eclipse忽略文件Java.gitignore即生效</p>
<h3 id="3-2-创建本地库"><a href="#3-2-创建本地库" class="headerlink" title="3.2 创建本地库"></a>3.2 创建本地库</h3><h4 id="3-2-1-新建本地库"><a href="#3-2-1-新建本地库" class="headerlink" title="3.2.1 新建本地库"></a>3.2.1 新建本地库</h4><p>1) 创建一个普通的Maven工程</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps49.jpg" alt="img"> </p>
<p>2) 将Maven工程交给Git管理，即生成.git目录</p>
<p>在工程上右键→Team→Share Project…</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps50.jpg" alt="img"> </p>
<p>3) 勾选Use or create repository in parent folder of project 并选中工程</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps51.jpg" alt="img"> </p>
<p>4) 点击Create Repository按钮生成.git目录</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps52.jpg" alt="img"> </p>
<p>5) 点击Finish之后发现工程已被Git管理</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps53.jpg" alt="img"> </p>
<p>6) 可以配置当前工程的用户名和邮箱</p>
<p>点击Window→Preferences→Team→Git→Configuration→Repository Settings</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps54.jpg" alt="img"> </p>
<p>7) 点击Add Entry…配置当前工程的用户名和邮箱</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps55.jpg" alt="img"> </p>
<p>8) 在src/main/java目录下创建包并创建HelloGit.java文件，此时文件只存在于工作区，文件的状态如下图：</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps56.jpg" alt="img"> </p>
<p>9) 在工程上右键→Team→Add to Index将工程添加到暂存区</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps57.jpg" alt="img"> </p>
<p>10) 添加到暂存区之后文件的状态如下图：</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps58.jpg" alt="img"> </p>
<p>11) 在工程上右键→Team→Commit…将工程添加到本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps59.jpg" alt="img"> </p>
<p>12) 添加注释后点击Commit将工程添加到本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps60.jpg" alt="img"> </p>
<p>13) 也可以直接点击Commit and Push…添加到本地库后开始上传到项目托管的网站</p>
<p>14) 工程添加到本地库之后文件的状态如下图：</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps61.jpg" alt="img"> </p>
<h4 id="3-2-2-版本间切换"><a href="#3-2-2-版本间切换" class="headerlink" title="3.2.2 版本间切换"></a>3.2.2 版本间切换</h4><p>1) 查看历史版本</p>
<p>在工程上右键→Team→Show in History</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps62.jpg" alt="img"> </p>
<p>2) 当前版本</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps63.jpg" alt="img"> </p>
<p>3) 在要切换的版本上右键→Reset→Hard</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps64.jpg" alt="img"> </p>
<p>4) 切换成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps65.jpg" alt="img"> </p>
<h4 id="3-2-3-创建分支"><a href="#3-2-3-创建分支" class="headerlink" title="3.2.3 创建分支"></a>3.2.3 创建分支</h4><p>1) 在工程上右键→Team→Switch To→New Branch…</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps66.jpg" alt="img"> </p>
<p>2) 给分支命名</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps67.jpg" alt="img"> </p>
<p>3) 点击Finish之后自动切换到新的分支</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps68.jpg" alt="img"> </p>
<p>4) 在新的分支上添加新的内容，添加到暂存区，添加到本地库</p>
<h4 id="3-2-4-合并分支"><a href="#3-2-4-合并分支" class="headerlink" title="3.2.4 合并分支"></a>3.2.4 合并分支</h4><p>1) 切换到主干</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps69.jpg" alt="img"> </p>
<p>2) 将分支中的内容合并到主干</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps70.jpg" alt="img"> </p>
<p>3) 选中分支开始合并</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps71.jpg" alt="img"> </p>
<p>4) 合并成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps72.jpg" alt="img"> </p>
<h4 id="3-2-5-解决冲突"><a href="#3-2-5-解决冲突" class="headerlink" title="3.2.5 解决冲突"></a>3.2.5 解决冲突</h4><p>​    让主干和分支在同一个位置添加一行代码</p>
<p>1) 分支添加内容，并添加到暂存区和本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps73.jpg" alt="img"> </p>
<p>2) 主干添加内容，并添加到暂存区和本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps74.jpg" alt="img"> </p>
<p>3) 在主干上合并分支，出现冲突</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps75.jpg" alt="img"> </p>
<p>4) 有效沟通后选择保留的代码，重写添加到暂存区、本地库冲突即可解决</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps76.jpg" alt="img"> </p>
<h3 id="3-3-将本地库上传到GitHub"><a href="#3-3-将本地库上传到GitHub" class="headerlink" title="3.3 将本地库上传到GitHub"></a>3.3 将本地库上传到GitHub</h3><h4 id="3-3-1-注册GitHub账号"><a href="#3-3-1-注册GitHub账号" class="headerlink" title="3.3.1 注册GitHub账号"></a>3.3.1 注册GitHub账号</h4><p>1) 访问GitHub网站<a href="https://github.com/，首页即是注册页面" target="_blank" rel="noopener">https://github.com/，首页即是注册页面</a></p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps77.jpg" alt="img"> </p>
<p>2) 输入用户名、邮箱、密码点击注册之后登录邮箱激活即可</p>
<h4 id="3-3-2-上传本地库"><a href="#3-3-2-上传本地库" class="headerlink" title="3.3.2 上传本地库"></a>3.3.2 上传本地库</h4><p>1) 登录GitHub在首页点击Start a project</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps78.jpg" alt="img"> </p>
<p>2) 指定仓库的名称和类型</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps79.jpg" alt="img"> </p>
<p>3) 仓库创建成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps80.jpg" alt="img"> </p>
<p>4) 复制仓库地址</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps81.jpg" alt="img"> </p>
<p>5) 将本地库上传到GitHub上创建的仓库中</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps82.jpg" alt="img"> </p>
<p>6) 指定仓库地址、用户名和密码</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps83.jpg" alt="img"> </p>
<p>7) 点击Preview</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps84.jpg" alt="img"> </p>
<p>8) 点击Preview开始连接GitHub，然后点击Push开始上传</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps85.jpg" alt="img"> </p>
<p>9) 上传成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps86.jpg" alt="img"> </p>
<p>10) 查看GitHub仓库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps87.jpg" alt="img"> </p>
<h4 id="3-3-3-更新本地库"><a href="#3-3-3-更新本地库" class="headerlink" title="3.3.3 更新本地库"></a>3.3.3 更新本地库</h4><p>​    项目在GitHub上被合作伙伴更新之后，我们就需要将GitHub上最新的代码拉到本地库，否则会上传失败！接下来我们以在GitHub上在线添加内容演示如何更新本地库。</p>
<p>1) 在GitHub上在线修改文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps88.jpg" alt="img"> </p>
<p>2) 在Eclipse如果不更新本地库直接上传会由于不是最新的版本而被拒绝</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps89.jpg" alt="img"> </p>
<p>3) 将GitHub上最新的内容Pull下来</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps90.jpg" alt="img"> </p>
<p>4) 更新本地库成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps91.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps92.jpg" alt="img"> </p>
<h3 id="3-4-从GitHub上克隆项目到本地"><a href="#3-4-从GitHub上克隆项目到本地" class="headerlink" title="3.4 从GitHub上克隆项目到本地"></a>3.4 从GitHub上克隆项目到本地</h3><h4 id="3-4-1-克隆项目"><a href="#3-4-1-克隆项目" class="headerlink" title="3.4.1 克隆项目"></a>3.4.1 克隆项目</h4><p>1) 在Eclipse中点击File→Import…→Git</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps93.jpg" alt="img"> </p>
<p>2) 选中Clone URI</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps94.jpg" alt="img"> </p>
<p>3) 输入克隆的项目在GitHub上仓库的地址</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps95.jpg" alt="img"> </p>
<p>4) 选择要克隆的分支</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps96.jpg" alt="img"> </p>
<p>5) 选择项目存放的路径</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps97.jpg" alt="img"> </p>
<p>6) 选择作为一个普通工程导入（通过方式二导入没有这一步）</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps98.jpg" alt="img"> </p>
<p>7) 点击完成</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps99.jpg" alt="img"> </p>
<p>8) 导入之后并不是一个Maven工程（如果通过方式二导入会自动识别为Maven工程）</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps100.jpg" alt="img"> </p>
<p>9) 转换为Maven工程</p>
<p>右键→Configure→Convert to Maven Project</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps101.jpg" alt="img"> </p>
<p>10) 转换之后</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps102.jpg" alt="img"> </p>
<h4 id="3-4-2-添加合作伙伴"><a href="#3-4-2-添加合作伙伴" class="headerlink" title="3.4.2 添加合作伙伴"></a>3.4.2 添加合作伙伴</h4><p>在项目的协同开发过程中，如果GitHub上的仓库不是你创建的，你克隆下来的项目完成代码的编辑之后上传会失败，如下图：</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps103.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps104.jpg" alt="img"> </p>
<p>此时如果想要上传成功，必须让GitHub上仓库的拥有者添加你为合作伙伴，</p>
<p>添加合作伙伴的步骤：</p>
<p>1) 让仓库拥有者在仓库上点击settings</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps105.jpg" alt="img"> </p>
<p>2) 点击Manage Access</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps106.jpg" alt="img"> </p>
<p>3) 搜索合作伙伴，即搜索你的GitHub账户</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps107.jpg" alt="img"> </p>
<p>4) 点击邀请</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps108.jpg" alt="img"> </p>
<p>5) 等待你回复</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps109.jpg" alt="img"> </p>
<p>6) 仓库拥有者可以将链接发送给你让你确认，当然你的邮箱也会收到等待确认的邮件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps110.jpg" alt="img"> </p>
<p>7) 等你接收之后就与仓库拥有者成为了合作伙伴，就可以向仓库上传项目了</p>
<h4 id="3-4-3-非合作伙伴如何共同开发项目"><a href="#3-4-3-非合作伙伴如何共同开发项目" class="headerlink" title="3.4.3 非合作伙伴如何共同开发项目"></a>3.4.3 非合作伙伴如何共同开发项目</h4><p>GitHub上有好多开源的好的项目，我们可以下载下来查看、借鉴别人的代码。但是如果我们修改了，由于不是对方的合作伙伴，我们无法将代码上传到别人的仓库，此时我们可以选择使用fork和pullrequest操作</p>
<p>1) 看到喜欢的项目点击fork操作将别人的项目复制一份作为自己的仓库，同时仓库下面会显示当前项目来自于哪里</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps111.jpg" alt="img"> </p>
<p>2) 修改代码之后如果想合并到作者那里，需要让作者审核，点击Pull requests→New pull request</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps112.jpg" alt="img"> </p>
<p>3) 点击Create pull request</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps113.jpg" alt="img"> </p>
<p>4) 填入标题、描述后点击Create pull request</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps114.jpg" alt="img"> </p>
<p>5) 你创建了pull request之后作者会收到Pull requests信息，作者可以选择拒绝和接受你的请求</p>
<h2 id="第4章-在Idea中使用Git"><a href="#第4章-在Idea中使用Git" class="headerlink" title="第4章 在Idea中使用Git"></a>第4章 在Idea中使用Git</h2><h3 id="4-1-安装Git核心程序"><a href="#4-1-安装Git核心程序" class="headerlink" title="4.1 安装Git核心程序"></a>4.1 安装Git核心程序</h3><p>根据自己的电脑操作系统从Git官网<a href="https://git-scm.com/下载对应的Git核心程序。" target="_blank" rel="noopener">https://git-scm.com/下载对应的Git核心程序。</a></p>
<p>以git-2.21.0为例说明安装步骤：</p>
<p>1) 双击Git-2.21.0-64-bit.exe</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps115.jpg" alt="img"> </p>
<p>2) 点击Next设置安装路径</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps116.jpg" alt="img"> </p>
<p>3) 点击Next</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps117.jpg" alt="img"> </p>
<p>4) 点击Next</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps118.jpg" alt="img"> </p>
<p>5) 点击Next，选择默认的编辑器</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps119.jpg" alt="img"> </p>
<p>6) 点击Next，选择第一项</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps120.jpg" alt="img"> </p>
<p>7) 点击Next</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps121.jpg" alt="img"> </p>
<p>8) 点击Next</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps122.jpg" alt="img"> </p>
<p>9) 点击Next</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps123.jpg" alt="img"> </p>
<p>10) 点击Next</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps124.jpg" alt="img"> </p>
<p>11) 点击Install开始安装</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps125.jpg" alt="img"> </p>
<p>12) 点击Finish安装完成</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps126.jpg" alt="img"> </p>
<h3 id="4-2-全局配置"><a href="#4-2-全局配置" class="headerlink" title="4.2 全局配置"></a>4.2 全局配置</h3><h4 id="4-2-1-配置Git核心程序"><a href="#4-2-1-配置Git核心程序" class="headerlink" title="4.2.1 配置Git核心程序"></a>4.2.1 配置Git核心程序</h4><p>1) 配置git.exe执行文件</p>
<p>点击工具栏中的settings→Version Control→Git</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps127.jpg" alt="img"> </p>
<p>2) 点击Test测试</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps128.jpg" alt="img"> </p>
<h4 id="4-2-2-配置GitHub账户"><a href="#4-2-2-配置GitHub账户" class="headerlink" title="4.2.2 配置GitHub账户"></a>4.2.2 配置GitHub账户</h4><p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps129.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps130.jpg" alt="img"> </p>
<p>3) 点击OK完成配置</p>
<h3 id="4-3-创建本地库"><a href="#4-3-创建本地库" class="headerlink" title="4.3 创建本地库"></a>4.3 创建本地库</h3><h4 id="4-3-1-新建本地库"><a href="#4-3-1-新建本地库" class="headerlink" title="4.3.1 新建本地库"></a>4.3.1 新建本地库</h4><p>1) 创建一个Empty Project</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps131.jpg" alt="img"> </p>
<p>2) 在空工程（Empty Project）中添加模块（Modules）</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps132.jpg" alt="img"> </p>
<p>3) 添加一个Maven模块</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps133.jpg" alt="img"> </p>
<p>4) 选择我无论如何都想编辑这个文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps134.jpg" alt="img"> </p>
<p>5) 创建本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps135.jpg" alt="img"> </p>
<p>6) 选择当前模块的上一级目录</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps136.jpg" alt="img"> </p>
<p>7) 点击OK本地库创建成功 </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps137.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps138.jpg" alt="img"> </p>
<p>8) 同时工具栏会出现Git相关操作</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps139.jpg" alt="img"> </p>
<p>9) 在src/main/java和src/main/resources目录下创建文件，创建了新文件之后会提示是否添加到暂存区</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps140.jpg" alt="img"> </p>
<p>10) 如果点击了Cancel，此时文件只存在于工作区，文件的状态如下图：</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps141.jpg" alt="img"> </p>
<p>11) 设置忽略文件</p>
<p>在模块上右键，选择一种方式设置忽略的文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps142.jpg" alt="img"> </p>
<p>Tips：方式二只需要修改.git/info目录下的exclude文件即可，不需要创建新的文件，所以建议大家选择这种方式。</p>
<p>12) 弹出提示框，提示是否在当前工作区创建.gitignore文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps143.jpg" alt="img"> </p>
<p>13) 点击Create，添加如下内容</p>
<p>.idea *.iml</p>
<p>14) 创建.gitignore文件之后发现被忽略的文件变成了灰色（有时候可能需要刷新模块或重启Idea才能看到）</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps144.jpg" alt="img"> </p>
<p>15) 在模块上右键将文件添加到暂存区</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps145.jpg" alt="img"> </p>
<p>16) 添加到暂存区之后文件的状态如下图：</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps146.jpg" alt="img"> </p>
<p>17) 在模块上右键或点击工具栏将文件添加到本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps147.jpg" alt="img"> </p>
<p>工具栏</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps148.jpg" alt="img"> </p>
<p>18) 添加注释内容后提交</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps149.jpg" alt="img"> </p>
<p>19) 提交到本地库之后文件的状态如下图：</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps150.jpg" alt="img"> </p>
<h4 id="4-3-2-版本间切换"><a href="#4-3-2-版本间切换" class="headerlink" title="4.3.2 版本间切换"></a>4.3.2 版本间切换</h4><p>1) 在模块上右键或者点击工具栏查看历史</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps151.jpg" alt="img"> </p>
<p>2) 选择要切换的版本</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps152.jpg" alt="img"> </p>
<p>3) 右键→Copy Revision Number</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps153.jpg" alt="img"> </p>
<p>4) 在模块上右键</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps154.jpg" alt="img"> </p>
<p>5) 选择Hard并粘贴版本号</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps155.jpg" alt="img"> </p>
<p>6) 版本切换成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps156.jpg" alt="img"> </p>
<h4 id="4-3-3-创建分支"><a href="#4-3-3-创建分支" class="headerlink" title="4.3.3 创建分支"></a>4.3.3 创建分支</h4><p>1) 在模块上右键</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps157.jpg" alt="img"> </p>
<p>2) 点击New Branch</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps158.jpg" alt="img"> </p>
<p>3) 给新分支命名</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps159.jpg" alt="img"> </p>
<p>4) 点击Create后自动切换到新分支</p>
<p>5) 在新分支添加新的代码并提交</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps160.jpg" alt="img"> </p>
<h4 id="4-3-4-合并分支"><a href="#4-3-4-合并分支" class="headerlink" title="4.3.4 合并分支"></a>4.3.4 合并分支</h4><p>1) 在模块上右键切换到主干</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps161.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps162.jpg" alt="img"> </p>
<p>2) 在模块上右键选择合并改变</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps163.jpg" alt="img"> </p>
<p>3) 选择要合并的分支</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps164.jpg" alt="img"> </p>
<p>4) 合并成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps165.jpg" alt="img"> </p>
<h4 id="4-3-5-解决冲突"><a href="#4-3-5-解决冲突" class="headerlink" title="4.3.5 解决冲突"></a>4.3.5 解决冲突</h4><p>​    让主干和分支在同一个位置添加一行代码</p>
<p>1) 分支添加内容，并添加到暂存区和本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps166.jpg" alt="img"> </p>
<p>2) 主干添加内容，并添加到暂存区和本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps167.jpg" alt="img"> </p>
<p>3) 在主干合并分支内容，出现冲突</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps168.jpg" alt="img"> </p>
<p>4) 选择合并，出现处理窗口</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps169.jpg" alt="img"> </p>
<p>5) 处理之后</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps170.jpg" alt="img"> </p>
<p>6) 点击Apply应用</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps171.jpg" alt="img"> </p>
<h3 id="4-4-将本地库上传到GitHub"><a href="#4-4-将本地库上传到GitHub" class="headerlink" title="4.4 将本地库上传到GitHub"></a>4.4 将本地库上传到GitHub</h3><h4 id="4-4-1-上传本地库"><a href="#4-4-1-上传本地库" class="headerlink" title="4.4.1 上传本地库"></a>4.4.1 上传本地库</h4><p>1) 在GitHub网站上创建仓库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps172.jpg" alt="img"> </p>
<p>2) 复制仓库地址</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps173.jpg" alt="img"> </p>
<p>3) 在Idea中的模块上右键</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps174.jpg" alt="img"> </p>
<p>4) 设置远程地址别名</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps175.jpg" alt="img"> </p>
<p>5) 点击Push推送到GitHub仓库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps176.jpg" alt="img"> </p>
<p>6) 上传成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps177.jpg" alt="img"> </p>
<h4 id="4-4-2-更新本地库"><a href="#4-4-2-更新本地库" class="headerlink" title="4.4.2 更新本地库"></a>4.4.2 更新本地库</h4><p>​    正常情况下是合作伙伴上传新的代码到GitHub，如果此时本地库不更新将无法上传，为了简单起见，我们直接在GitHub上在线修改文件。</p>
<p>1) 在GitHub上在线添加一个文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps178.jpg" alt="img"> </p>
<p>2) Idea中的本地库也修改文件、添加到暂存库、添加到本地库，然后上传，发现上传被拒绝</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps179.jpg" alt="img"> </p>
<p>3) 此时点击Merge或Rebase都可以实现本地库与远程GitHub的同步</p>
<p>4) 也可以点击Cancel之后通过以下方式更新本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps180.jpg" alt="img"> </p>
<p>5) 点击Pull将GitHub上最新的代码合并都本地库</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps181.jpg" alt="img"> </p>
<p>6) 点击Pull之后更新本地库成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps182.jpg" alt="img"> </p>
<h3 id="4-5-从GitHub上克隆项目到本地"><a href="#4-5-从GitHub上克隆项目到本地" class="headerlink" title="4.5 从GitHub上克隆项目到本地"></a>4.5 从GitHub上克隆项目到本地</h3><p>1) 点击Idea中的CVS选项</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps183.jpg" alt="img"> </p>
<p>2) 输入GitHub中的仓库地址并指定项目的存放路径</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps184.jpg" alt="img"> </p>
<p>3) 提示是否为克隆的项目创建一个新工程</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps185.jpg" alt="img"> </p>
<p>4) 点击Yes弹出导入工程的提示框</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps186.jpg" alt="img"> </p>
<p>5) 点击Finish之后在Idea中显示的是一个空工程</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps187.jpg" alt="img"> </p>
<p>6) 需要为新工程配置一下JDK、导入Module</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps188.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps189.jpg" alt="img"> </p>
<p>7) 克隆成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps190.jpg" alt="img"> </p>
<h2 id="第5章-国内的项目托管网站-码云"><a href="#第5章-国内的项目托管网站-码云" class="headerlink" title="第5章 国内的项目托管网站-码云"></a>第5章 国内的项目托管网站-码云</h2><h3 id="5-1-简介"><a href="#5-1-简介" class="headerlink" title="5.1 简介"></a>5.1 简介</h3><p>使用GitHub作为项目托管网站如果网速不好很影响效率，大家也可以使用国内的项目托管网站-码云。网址是<a href="https://gitee.com/" target="_blank" rel="noopener">https://gitee.com/</a> ，使用方式跟GitHub一样，而且它还是一个中文网站，如果你英文不是很好它是最好的选择。</p>
<h3 id="5-2-配置SSH免密登录"><a href="#5-2-配置SSH免密登录" class="headerlink" title="5.2 配置SSH免密登录"></a>5.2 配置SSH免密登录</h3><p>在码云上通过HTTPS的模式上传项目跟在GitHub上一样，但是在码云上上传项目总是输入用户名和密码，比较麻烦，所以给大家演示一下通过SSH模式免密登录上传项目，使用SSH模式的好处是每次上传项目不需要输入用户名和密码，SSH免密登录同样适用于GitHub。</p>
<p>使用SSH模式前提是你必须是这个项目的拥有者或者合作者，且配好了SSH Key，配置SSH Key的步骤如下：</p>
<p>1) 进入电脑的用户目录，在用户目录右键打开Git命令行窗口</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps191.jpg" alt="img"> </p>
<p>2) 创建SSH Key</p>
<p>在命令行窗口输入以下命令</p>
<p>ssh-keygen -t rsa -C 任意内容</p>
<p>命令参数说明：</p>
<p>​    -t = The type of the key to generate</p>
<p>​    密钥的类型</p>
<p>​    -C = comment to identify the key</p>
<p>​    用于识别这个密钥的注释</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps192.jpg" alt="img"> </p>
<p>输入命令后回车，然后再回车、回车、回车</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps193.jpg" alt="img"> </p>
<p>3) SSH Key创建成功会在用户目录生成.ssh文件夹</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps194.jpg" alt="img"> </p>
<p>4) 进入.ssh文件夹，查看id_rsa.pub文件，复制全部内容</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps195.jpg" alt="img"> </p>
<p>5) 找到码云账户的设置</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps196.jpg" alt="img"> </p>
<p>6) 点击SSH公钥，设置标题，粘贴公钥，点击确定</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps197.jpg" alt="img"> </p>
<p>Tips：码云账户中可以添加多个SSH公钥，但是一台电脑只能授权一个用户免密登录</p>
<p>7) 复制码云账户中仓库的SSH地址</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps198.jpg" alt="img"> </p>
<h3 id="5-3-在Eclipse中通过SSH模式上传项目"><a href="#5-3-在Eclipse中通过SSH模式上传项目" class="headerlink" title="5.3 在Eclipse中通过SSH模式上传项目"></a>5.3 在Eclipse中通过SSH模式上传项目</h3><p>创建了SSH Key之后Eclipse可以自动识别对应的公钥和私钥文件</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps199.jpg" alt="img"> </p>
<p>通过SSH模式上传项目的步骤：</p>
<p>1) 在要上传的项目上右键</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps200.jpg" alt="img"> </p>
<p>2) 点击New Remote…</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps201.jpg" alt="img"> </p>
<p>3) 复制SSH模式的地址并给远程地址起一个别名</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps202.jpg" alt="img"> </p>
<p>4) 因为是第一次使用SSH模式，点击Finish之后需要勾选保存key，勾选创建known_hosts文件，以后就不需要这样了</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps203.jpg" alt="img"> </p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps204.jpg" alt="img"> </p>
<p>5) 点击Preview进入确定上传窗口</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps205.jpg" alt="img"> </p>
<p>6) 点击Push开始上传，不再需要输入用户名和密码</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps206.jpg" alt="img"> </p>
<p>7) 上传成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps207.jpg" alt="img"> </p>
<h3 id="5-4-在Idea中通过SSH模式上传项目"><a href="#5-4-在Idea中通过SSH模式上传项目" class="headerlink" title="5.4 在Idea中通过SSH模式上传项目"></a>5.4 在Idea中通过SSH模式上传项目</h3><p>1) 在码云账户中创建一个新的仓库，复制SSH模式的地址</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps208.jpg" alt="img"> </p>
<p>2) 在Idea中要上传的项目上右键</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps209.jpg" alt="img"> </p>
<p>3) 在弹出的窗口点击origin→Define Remote</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps210.jpg" alt="img"> </p>
<p>4) 设置SSH模式远程地址与别名</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps211.jpg" alt="img"> </p>
<p>5) 选择定义的sshorigin开始上传项目</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps212.jpg" alt="img"> </p>
<p>6) 点击Push直接上传成功</p>
<p><img src="/tools/IDE%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6/wps213.jpg" alt="img"> </p>
]]></content>
      <categories>
        <category>工具集</category>
      </categories>
      <tags>
        <tag>IDE中常用的插件 git</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis系列（一）</title>
    <url>/bigdata/Redis%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Redis简介"><a href="#Redis简介" class="headerlink" title="Redis简介"></a>Redis简介</h1><p>Redis英文官网介绍：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs and geospatial indexes with radius queries. Redis has built-in replication, Lua scripting, LRU eviction, transactions and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.</span><br></pre></td></tr></table></figure>

<p>Redis中文官网介绍：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。</span><br></pre></td></tr></table></figure>

<p>Redis命令参考文档网址：<a href="http://redisdoc.com" target="_blank" rel="noopener">http://redisdoc.com</a></p>
<p>Redis的典型应用场景：</p>
<blockquote>
<p>1、缓存</p>
<p>使用Redis可以建立性能非常出色的缓存服务器，查询请求先在Redis中查找所需要的数据，如果能够查询到（命中）则直接返回，大大减轻关系型数据库的压力。</p>
<p>2、数据临时存储位置</p>
<p>使用token（令牌）作为用户登录系统时的身份标识，这个token就可以在Redis中临时存储。</p>
<p>3、分布式环境下解决Session不一致问题时的Session库</p>
<p>Spring提供了一种技术解决分布式环境下Session不一致问题，叫SpringSession。而Redis就可以为SpringSession提供一个数据存储空间。</p>
<p>4、流式数据去重</p>
<p>在Redis中有一种数据类型是set，和Java中的Set集合很像，不允许存储重复数据。借助这个特性我们可以在Redis中使用set类型存储流式数据达到去重的目的。</p>
</blockquote>
<h1 id="Redis和Memcached"><a href="#Redis和Memcached" class="headerlink" title="Redis和Memcached"></a>Redis和Memcached</h1><p><strong>Redis</strong> 支持复杂的数据结构：</p>
<p><strong>Redis</strong> 相比 <strong>Memcached</strong> 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， <strong>Redis</strong> 会是不错的选择。</p>
<p><strong>Redis</strong> 原生支持集群模式：</p>
<p>在 redis3.x 版本中，便能支持 <strong>Cluster</strong> 模式，而 <strong>Memcached</strong> 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。</p>
<p>性能对比：</p>
<p>由于 <strong>Redis</strong> 只使用单核，而 <strong>Memcached</strong> 可以使用多核，所以平均每一个核上 <strong>Redis</strong> 在存储小数据时比 <strong>Memcached</strong> 性能更高。而在 100k 以上的数据中，<strong>Memcached</strong> 性能要高于 <strong>Redis</strong>，虽然 <strong>Redis</strong> 最近也在存储大数据的性能上进行优化，但是比起 <strong>Remcached</strong>，还是稍有逊色。</p>
<p>本段摘抄自：<a href="https://github.com/AobingJava/JavaFamily/blob/master/docs/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E3%80%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E3%80%81%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.md" target="_blank" rel="noopener">https://github.com/AobingJava/JavaFamily/blob/master/docs/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E3%80%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E3%80%81%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.md</a></p>
<p>更多关于<strong>Memcached</strong>：<a href="https://blog.csdn.net/qq_35190492/article/details/103041932" target="_blank" rel="noopener">https://blog.csdn.net/qq_35190492/article/details/103041932</a></p>
<h1 id="Redis安装"><a href="#Redis安装" class="headerlink" title="Redis安装"></a>Redis安装</h1><h2 id="上传并解压"><a href="#上传并解压" class="headerlink" title="上传并解压"></a>上传并解压</h2><p>redis-4.0.2.tar.gz</p>
<h2 id="安装C语言编译环境"><a href="#安装C语言编译环境" class="headerlink" title="安装C语言编译环境"></a>安装C语言编译环境</h2><p><strong>[建议先拍快照]</strong>  yum install -y gcc-c++</p>
<h2 id="修改安装位置"><a href="#修改安装位置" class="headerlink" title="修改安装位置"></a>修改安装位置</h2><p>vim redis解压目录/src/Makefile</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">PREFIX?=/usr/local/redis</span><br></pre></td></tr></table></figure>

<p>就Redis自身而言是不需要修改的，这里修改的目的是让Redis的运行程序不要和其他文件混杂在一起。</p>
<h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><p>编译：进入Redis解压目录执行make命令</p>
<p><strong>[建议先拍快照]</strong>  安装：make install</p>
<h2 id="启动Redis服务器"><a href="#启动Redis服务器" class="headerlink" title="启动Redis服务器"></a>启动Redis服务器</h2><h3 id="①默认启动"><a href="#①默认启动" class="headerlink" title="①默认启动"></a>①默认启动</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">[root@rich ~]# /usr/local/redis/bin/redis-server</span><br><span class="line">7239:C 07 Oct 18:59:12.144 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">7239:C 07 Oct 18:59:12.144 # Redis version=4.0.2, bits=64, commit=00000000, modified=0, pid=7239, just started</span><br><span class="line">7239:C 07 Oct 18:59:12.144 # Warning: no config file specified, using the default config. In order to specify a config file use /usr/local/redis/bin/redis-server /path/to/redis.conf</span><br><span class="line">7239:M 07 Oct 18:59:12.145 * Increased maximum number of open files to 10032 (it was originally set to 1024).</span><br><span class="line">                _._                                                  </span><br><span class="line">           _.-``__ ''-._                                             </span><br><span class="line">      _.-``    `.  `_.  ''-._           Redis 4.0.2 (00000000/0) 64 bit</span><br><span class="line">  .-`` .-```.  ```\/    _.,_ ''-._                                   </span><br><span class="line"> (    '      ,       .-`  | `,    )     Running in standalone mode</span><br><span class="line"> |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379</span><br><span class="line"> |    `-._   `._    /     _.-'    |     PID: 7239</span><br><span class="line">  `-._    `-._  `-./  _.-'    _.-'                                   </span><br><span class="line"> |`-._`-._    `-.__.-'    _.-'_.-'|                                  </span><br><span class="line"> |    `-._`-._        _.-'_.-'    |           http://redis.io        </span><br><span class="line">  `-._    `-._`-.__.-'_.-'    _.-'                                   </span><br><span class="line"> |`-._`-._    `-.__.-'    _.-'_.-'|                                  </span><br><span class="line"> |    `-._`-._        _.-'_.-'    |                                  </span><br><span class="line">  `-._    `-._`-.__.-'_.-'    _.-'                                   </span><br><span class="line">      `-._    `-.__.-'    _.-'                                       </span><br><span class="line">          `-._        _.-'                                           </span><br><span class="line">              `-.__.-'                                               </span><br><span class="line"></span><br><span class="line">7239:M 07 Oct 18:59:12.148 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.</span><br><span class="line">7239:M 07 Oct 18:59:12.148 # Server initialized</span><br><span class="line">7239:M 07 Oct 18:59:12.148 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.</span><br><span class="line">7239:M 07 Oct 18:59:12.148 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.</span><br><span class="line">7239:M 07 Oct 18:59:12.148 * Ready to accept connections</span><br></pre></td></tr></table></figure>

<p>停止Redis服务器</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">/usr/local/redis/bin/redis-cli shutdown</span><br></pre></td></tr></table></figure>

<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">7239:M 07 Oct 19:00:53.208 # User requested shutdown...</span><br><span class="line">7239:M 07 Oct 19:00:53.208 * Saving the final RDB snapshot before exiting.</span><br><span class="line">7239:M 07 Oct 19:00:53.214 * DB saved on disk</span><br><span class="line">7239:M 07 Oct 19:00:53.214 # Redis is now ready to exit, bye bye...</span><br></pre></td></tr></table></figure>



<h3 id="②定制配置项启动"><a href="#②定制配置项启动" class="headerlink" title="②定制配置项启动"></a>②定制配置项启动</h3><h4 id="1-准备配置文件"><a href="#1-准备配置文件" class="headerlink" title="[1]准备配置文件"></a>[1]准备配置文件</h4><p>cp /opt/redis-4.0.2/redis.conf /usr/local/redis/</p>
<h4 id="2-修改配置项"><a href="#2-修改配置项" class="headerlink" title="[2]修改配置项"></a>[2]修改配置项</h4><table>
<thead>
<tr>
<th>配置项名称</th>
<th>作用</th>
<th>取值</th>
</tr>
</thead>
<tbody><tr>
<td>daemonize</td>
<td>控制是否以守护进程形式运行Redis服务器</td>
<td>yes</td>
</tr>
<tr>
<td>logfile</td>
<td>指定日志文件位置</td>
<td>“/var/logs/redis.log”</td>
</tr>
<tr>
<td>dir</td>
<td>Redis工作目录</td>
<td>/usr/local/redis</td>
</tr>
</tbody></table>
<p>注意：/var/logs目录需要我们提前创建好</p>
<h4 id="3-让Redis根据指定的配置文件启动"><a href="#3-让Redis根据指定的配置文件启动" class="headerlink" title="[3]让Redis根据指定的配置文件启动"></a>[3]让Redis根据指定的配置文件启动</h4><p>格式</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">redis-server文件路径 redis.conf文件路径</span><br></pre></td></tr></table></figure>



<p>举例</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">/usr/local/redis/bin/redis-server /usr/local/redis/redis.conf</span><br></pre></td></tr></table></figure>

<h2 id="客户端登录"><a href="#客户端登录" class="headerlink" title="客户端登录"></a>客户端登录</h2><p>/usr/local/redis/bin/redis-cli</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; ping</span><br><span class="line">PONG</span><br><span class="line">127.0.0.1:6379&gt; exit</span><br></pre></td></tr></table></figure>

<h1 id="Redis五种常用数据结构"><a href="#Redis五种常用数据结构" class="headerlink" title="Redis五种常用数据结构"></a>Redis五种常用数据结构</h1><h2 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h2><table>
    <tr>
        <td rowspan="6">KEY</td>
        <td>VALUE</td>
    </tr>
    <tr>
        <td>string</td>
    </tr>
    <tr>
        <td>list</td>
    </tr>
    <tr>
        <td>set</td>
    </tr>
    <tr>
        <td>hash</td>
    </tr>
    <tr>
        <td>zset</td>
    </tr>
</table>


<p>Redis中的数据，总体上是键值对，不同数据类型指的是键值对中值的类型。</p>
<h2 id="字符串String类型"><a href="#字符串String类型" class="headerlink" title="字符串String类型"></a>字符串String类型</h2><p>Redis中最基本的类型，它是key对应的一个单一值。二进制安全，不必担心由于编码等问题导致二进制数据变化。所以redis的string可以包含任何数据，比如jpg图片或者序列化的对象。Redis中一个字符串值的最大容量是512M。</p>
<p><font color="red">注意：redis中的Key和Value时区分大小写的，命令不区分大小写</font></p>
<p>​           <font color="red"> redis是单线程 </font></p>
<p>​          <font color="red"> 不适合存储大容量的数据</font></p>
<p>​          <font color="red"> redis中自增的value是可以转成数字的</font></p>
<p>应用场景：1.计数器。许多系统都会使用Redis作为系统的实时计数器，可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库或者其它存储介质当中进行永久保存。</p>
<p>​                   2.共享用户Session。用户重新刷新一次界面，可能需要访问一下数据进行重新登录，或者访问页面缓存Cookie，但是可以利用Redis将用户的Session集中管理，在这种模式只需要保证Redis的高可用，每次用户Session的更新和获取都可以快速完成。大大提高效率。</p>
<p>Hash：</p>
<h2 id="列表List类型"><a href="#列表List类型" class="headerlink" title="列表List类型"></a>列表List类型</h2><p>Redis 列表是简单的字符串列表，按照插入顺序排序（有顺序、可重复）。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。说明它的底层是基于链表实现的，所以它操作时头尾效率高，中间效率低。</p>
<p>应用场景：1.最新消息排行榜。</p>
<p>​                   2.消息队列，以完成多程序之间的消息交换。可以用push操作将任务存在list中（生产者），然后线程在用pop操作将任务取出进行执行。（消费者）</p>
<h2 id="集合Set类型"><a href="#集合Set类型" class="headerlink" title="集合Set类型"></a>集合Set类型</h2><p>Redis的set是string类型的<strong>无序不可重复</strong>集合，在执行插入和删除以及判断是否存在某元素时，效率高。它是基于哈希表实现的。集合最大的优势在于可以进行交集并集差集操作。Set可包含的最大元素数量是4294967295。</p>
<p>应用场景：1.利用交集求共同好友。</p>
<p>​                   2.利用唯一性，可以统计访问网站的所有独立IP。</p>
<p>​                   3.好友推荐的时候根据tag求交集，大于某个threshold（临界值的）就可以推荐。</p>
<h2 id="字典Hash类型"><a href="#字典Hash类型" class="headerlink" title="字典Hash类型"></a>字典Hash类型</h2><p>本身就是一个键值对集合。可以当做Java中的Map&lt;String,String&gt;对待。</p>
<h2 id="有序集合SortedSet-zSet-类型"><a href="#有序集合SortedSet-zSet-类型" class="headerlink" title="有序集合SortedSet(zSet)类型"></a>有序集合SortedSet(zSet)类型</h2><p>Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员（此处的不能重复是索引为唯一的，数据却可以重复）。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序（有顺序）。</p>
<p>应用场景：1.排行榜。可以用于一个大型在线游戏的积分排行榜，每当玩家的分数发生变化时，可以执行zadd更新玩家分数(score)，此后在通过zrange获取几分topN的用户信息。</p>
<p>​                  2.做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行（微博热搜榜）。</p>
<h1 id="Redis命令行操作"><a href="#Redis命令行操作" class="headerlink" title="Redis命令行操作"></a>Redis命令行操作</h1><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="①切换数据库"><a href="#①切换数据库" class="headerlink" title="①切换数据库"></a>①切换数据库</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">Redis默认有16个数据库。</span><br><span class="line">115 # Set the number of databases. The default database is DB 0, you can select</span><br><span class="line">116 # a different one on a per-connection basis using SELECT <span class="tag">&lt;<span class="name">dbid</span>&gt;</span> where</span><br><span class="line">117 # dbid is a number between 0 and 'databases'-1</span><br><span class="line">118 databases 16</span><br><span class="line">使用select进行切换，数据库索引从0开始</span><br><span class="line">127.0.0.1:6379&gt; select 2</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379[2]&gt; select 0</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure>

<h3 id="②查看数据库长度"><a href="#②查看数据库长度" class="headerlink" title="②查看数据库长度"></a>②查看数据库长度</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; dbsize</span><br><span class="line">(integer) 3</span><br></pre></td></tr></table></figure>

<h2 id="KEY操作"><a href="#KEY操作" class="headerlink" title="KEY操作"></a>KEY操作</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">●KEYS PATTERN</span><br><span class="line">●TYPE KEY</span><br><span class="line">	返回KEY对应的值的类型</span><br><span class="line">●MOVE KEY DB</span><br><span class="line">	把一组键值对数据移动到另一个数据库中</span><br><span class="line">●DEL KEY [KEY ...]</span><br><span class="line">	根据KEY进行删除，至少要指定一个KEY</span><br><span class="line">●EXISTS KEY</span><br><span class="line">	检查指定的KEY是否存在。指定一个KEY时，存在返回1，不存在返回0。可以指定多个，返回存在的KEY的数量。</span><br><span class="line">●RANDOMKEY</span><br><span class="line">	在现有的KEY中随机返回一个</span><br><span class="line">●RENAME KEY NEWKEY</span><br><span class="line">	重命名一个KEY，NEWKEY不管是否是已经存在的都会执行，如果NEWKEY已经存在则会被覆盖。</span><br><span class="line">●RENAMENX KEY NEWKEY</span><br><span class="line">	只有在NEWKEY不存在时能够执行成功，否则失败</span><br><span class="line">●TTL KEY</span><br><span class="line">	以秒为单位查看KEY还能存在多长时间</span><br><span class="line">●EXPIRE KEY SECONDS</span><br><span class="line">	给一个KEY设置在SECONDS秒后过期，过期会被Redis移除。</span><br><span class="line">●PERSIST KEY</span><br><span class="line">	移除过期时间，变成永久key</span><br></pre></td></tr></table></figure>

<h2 id="string操作"><a href="#string操作" class="headerlink" title="string操作"></a>string操作</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">●SET KEY VALUE [EX SECONDS] [PX MILLISECONDS] [NX|XX]</span><br><span class="line">	给KEY设置一个string类型的值。</span><br><span class="line">	EX参数用于设置存活的秒数。</span><br><span class="line">	PX参数用于设置存活的毫秒数。</span><br><span class="line">	NX参数表示当前命令中指定的KEY不存在才行。</span><br><span class="line">	XX参数表示当前命令中指定的KEY存在才行。</span><br><span class="line">●GET KEY</span><br><span class="line">	根据key得到值，只能用于string类型。</span><br><span class="line">●APPEND KEY VALUE</span><br><span class="line">	把指定的value追加到KEY对应的原来的值后面，返回值是追加后字符串长度</span><br><span class="line">●STRLEN KEY</span><br><span class="line">	直接返回字符串长度</span><br><span class="line">●INCR KEY</span><br><span class="line">	自增1</span><br><span class="line">●DECR KEY</span><br><span class="line">	自减1</span><br><span class="line">●INCRBY KEY INCREMENT</span><br><span class="line">	原值+INCREMENT</span><br><span class="line">●DECRBY KEY DECREMENT</span><br><span class="line">	原值-DECREMENT</span><br><span class="line">●GETRANGE KEY START END</span><br><span class="line">	从字符串中取指定的一段</span><br><span class="line">●SETRANGE KEY OFFSET VALUE</span><br><span class="line">	从offset开始使用VALUE进行替换</span><br><span class="line">●SETEX KEY SECONDS VALUE</span><br><span class="line">	设置KEY,VALUE时指定存在秒数</span><br><span class="line">●SETNX KEY VALUE</span><br><span class="line">	新建字符串类型的键值对</span><br><span class="line">●MSET KEY VALUE [KEY VALUE ...]</span><br><span class="line">	一次性设置一组多个键值对</span><br><span class="line">●MGET KEY [KEY ...]</span><br><span class="line">	一次性指定多个KEY，返回它们对应的值，没有值的KEY返回值是(nil)</span><br><span class="line">●MSETNX KEY VALUE [KEY VALUE ...]</span><br><span class="line">	一次性新建多个值</span><br><span class="line">●GETSET KEY VALUE</span><br><span class="line">	设置新值，同时能够将旧值返回</span><br></pre></td></tr></table></figure>

<h2 id="list操作"><a href="#list操作" class="headerlink" title="list操作"></a>list操作</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">●LPUSH key value [value ...]</span><br><span class="line">●RPUSH key value [value ...]</span><br><span class="line">●LRANGE key start stop</span><br><span class="line">	根据list集合的索引打印元素数据</span><br><span class="line">	正着数：0,1,2,3,...</span><br><span class="line">	倒着数：-1,-2,-3,...</span><br><span class="line">●LLEN key</span><br><span class="line">●LPOP key</span><br><span class="line">	从左边弹出一个元素。</span><br><span class="line">	弹出=返回+删除。</span><br><span class="line">●RPOP key</span><br><span class="line">	从右边弹出一个元素。</span><br><span class="line">●RPOPLPUSH source destination</span><br><span class="line">	从source中RPOP一个元素，LPUSH到destination中</span><br><span class="line">●LINDEX key index</span><br><span class="line">	根据索引从集合中取值</span><br><span class="line">●LINSERT key BEFORE|AFTER pivot value</span><br><span class="line">	在pivot指定的值前面或后面插入value</span><br><span class="line">●LPUSHX key value</span><br><span class="line">	只能针对存在的list执行LPUSH</span><br><span class="line">●LREM key count value</span><br><span class="line">	根据count指定的数量从key对应的list中删除value</span><br><span class="line">●LSET key index value</span><br><span class="line">	把指定索引位置的元素替换为另一个值</span><br><span class="line">●LTRIM key start stop</span><br><span class="line">	仅保留指定区间的数据，两边的数据被删除</span><br></pre></td></tr></table></figure>

<h2 id="set操作"><a href="#set操作" class="headerlink" title="set操作"></a>set操作</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">●SADD key member [member ...]</span><br><span class="line">●SMEMBERS key</span><br><span class="line">●SCARD key</span><br><span class="line">	返回集合中元素的数量</span><br><span class="line">●SISMEMBER key member</span><br><span class="line">	检查当前指定member是否是集合中的元素</span><br><span class="line">●SREM key member [member ...]</span><br><span class="line">	从集合中删除元素</span><br><span class="line">●SINTER key [key ...]</span><br><span class="line">	将指定的集合进行“交集”操作</span><br><span class="line">	集合A：a,b,c</span><br><span class="line">	集合B：b,c,d</span><br><span class="line">	交集：b,c</span><br><span class="line">●SINTERSTORE destination key [key ...]</span><br><span class="line">	取交集后存入destination</span><br><span class="line">●SDIFF key [key ...]</span><br><span class="line">	将指定的集合执行“差集”操作</span><br><span class="line">	集合A：a,b,c</span><br><span class="line">	集合B：b,c,d</span><br><span class="line">	A对B执行diff：a</span><br><span class="line">	相当于：A-交集部分</span><br><span class="line">●SDIFFSTORE destination key [key ...]</span><br><span class="line">●SUNION key [key ...]</span><br><span class="line">	将指定的集合执行“并集”操作</span><br><span class="line">	集合A：a,b,c</span><br><span class="line">	集合B：b,c,d</span><br><span class="line">	并集：a,b,c,d</span><br><span class="line">●SUNIONSTORE destination key [key ...]</span><br><span class="line">●SMOVE source destination member</span><br><span class="line">	把member从source移动到destination</span><br><span class="line">●SPOP key [count]</span><br><span class="line">	从集合中随机弹出count个数量的元素，count不指定就弹出1个</span><br><span class="line">●SRANDMEMBER key [count]</span><br><span class="line">	从集合中随机返回count个数量的元素，count不指定就返回1个</span><br><span class="line">●SSCAN key cursor [MATCH pattern] [COUNT count]</span><br><span class="line">	基于游标的遍历</span><br></pre></td></tr></table></figure>

<h2 id="hash操作"><a href="#hash操作" class="headerlink" title="hash操作"></a>hash操作</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">●HSET key field value</span><br><span class="line">●HGETALL key</span><br><span class="line">●HGET key field</span><br><span class="line">●HLEN key</span><br><span class="line">●HKEYS key</span><br><span class="line">●HVALS key</span><br><span class="line">●HEXISTS key field</span><br><span class="line">●HDEL key field [field ...]</span><br><span class="line">●HINCRBY key field increment</span><br><span class="line">●HMGET key field [field ...]</span><br><span class="line">●HMSET key field value [field value ...]</span><br><span class="line">●HSETNX key field value</span><br></pre></td></tr></table></figure>

<h2 id="zset操作"><a href="#zset操作" class="headerlink" title="zset操作"></a>zset操作</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">●ZADD key [NX|XX] [CH] [INCR] score member [score member ...]</span><br><span class="line">●ZRANGE key start stop [WITHSCORES]</span><br><span class="line">●ZCARD key</span><br><span class="line">●ZSCORE key member</span><br><span class="line">●ZINCRBY key increment member</span><br><span class="line">●ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]</span><br><span class="line">	在分数的指定区间内返回数据</span><br><span class="line">●ZRANK key member</span><br><span class="line">	先对分数进行升序排序，返回member的排名</span><br><span class="line">●ZREM key member [member ...]</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis系列（三）</title>
    <url>/bigdata/Redis%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Redis高可用"><a href="#Redis高可用" class="headerlink" title="Redis高可用"></a>Redis高可用</h1><p>Redis 支持主从同步，提供 Cluster 集群部署模式，通过 Sentinel哨兵来监控 Redis主服务器的状态。当主挂掉时，在从节点中根据一定策略选出新主，并调整其他从 slaveof 到新主。</p>
<p>选主的策略简单来说有三个：slave 的 priority 设置的越低，优先级越高；同等情况下，slave 复制的数据越多优先级越高；相同的条件下 runid 越小越容易被选中。</p>
<p>在 Redis 集群中，sentinel 也会进行多实例部署，sentinel 之间通过 Raft 协议来保证自身的高可用。</p>
<p>Redis Cluster 使用分片机制，在内部分为 16384 个 slot 插槽，分布在所有 master 节点上，每个 master 节点负责一部分 slot。数据操作时按 key 做 CRC16 来计算在哪个 slot，由哪个 master 进行处理。数据的冗余是通过 slave 节点来保障。</p>
<p>本段摘抄自：<a href="https://blog.csdn.net/qq_35190492/article/details/103041932" target="_blank" rel="noopener">https://blog.csdn.net/qq_35190492/article/details/103041932</a></p>
<h1 id="Redis的同步机制"><a href="#Redis的同步机制" class="headerlink" title="Redis的同步机制"></a>Redis的同步机制</h1><p>Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将RDB文件全量同步到复制节点，复制节点接受完成后将RDB镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。后续的增量数据通过AOF日志同步即可，有点类似数据库的binlog。</p>
<p>当启动一台slave 的时候，他会发送一个<strong>psync</strong>命令给master ，如果是这个slave第一次连接到master，他会触发一个全量复制。master就会启动一个线程，生成<strong>RDB</strong>快照，还会把新的写请求都缓存在内存中，<strong>RDB</strong>文件生成后，master会将这个<strong>RDB</strong>发送给slave的，slave拿到之后做的第一件事情就是写进本地的磁盘，然后加载进内存，然后master会把内存里面缓存的那些新命名都发给slave。</p>
<p>本段摘抄自：<a href="https://blog.csdn.net/qq_35190492/article/details/103041932" target="_blank" rel="noopener">https://blog.csdn.net/qq_35190492/article/details/103041932</a></p>
<h1 id="Redis主从复制机制"><a href="#Redis主从复制机制" class="headerlink" title="Redis主从复制机制"></a>Redis主从复制机制</h1><h2 id="1-读写分离的好处："><a href="#1-读写分离的好处：" class="headerlink" title="1.读写分离的好处："></a>1.读写分离的好处：</h2><ul>
<li>性能优化：主服务器专注于写操作，可以用更适合写入数据的模式工作；同样，从服务器专注于读操作，可以用更适合读取数据的模式工作。</li>
<li>强化数据安全，避免单点故障：由于数据同步机制的存在，各个服务器之间数据保持一致，所以其中某个服务器宕机不会导致数据丢失或无法访问。从这个角度说参与主从复制的Redis服务器构成了一个<b><font color="blue">集群</font></b>。</li>
</ul>
<h2 id="2-搭建步骤"><a href="#2-搭建步骤" class="headerlink" title="2.搭建步骤"></a>2.搭建步骤</h2><h3 id="①思路"><a href="#①思路" class="headerlink" title="①思路"></a>①思路</h3><p>Redis集群在运行时使用的是同一个可执行文件，只是对应的配置文件不同。</p>
<p>每个配置文件中相同的参数是：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">daemonize yes</span><br><span class="line">dir /usr/local/cluster-redis</span><br></pre></td></tr></table></figure>

<p>不同的参数有：</p>
<table>
<thead>
<tr>
<th>配置项名称</th>
<th>作用</th>
<th>取值</th>
</tr>
</thead>
<tbody><tr>
<td>port</td>
<td>Redis服务器启动后监听的端口号</td>
<td>6000<br>7000<br>8000</td>
</tr>
<tr>
<td>dbfilename</td>
<td>RDB文件存储位置</td>
<td>dump6000.rdb<br>dump7000.rdb<br>dump8000.rdb</td>
</tr>
<tr>
<td>logfile</td>
<td>日志文件位置</td>
<td>/var/logs/redis6000.log<br>/var/logs/redis7000.log<br>/var/logs/redis8000.log</td>
</tr>
<tr>
<td>pidfile</td>
<td>pid文件位置</td>
<td>/var/run/redis6000.pid<br>/var/run/redis7000.pid<br>/var/run/redis8000.pid</td>
</tr>
</tbody></table>
<h3 id="②步骤"><a href="#②步骤" class="headerlink" title="②步骤"></a>②步骤</h3><ul>
<li>第一步：创建/usr/local/cluster-redis目录</li>
<li>第二步：把原始未经修改的redis.conf复制到/usr/local/cluster-redis目录</li>
<li>第三步：把/usr/local/cluster-redis目录下的redis.conf复制为redis6000.conf</li>
<li>第四步：按照既定计划修改redis6000.conf中的相关配置项<ul>
<li>daemonize yes</li>
<li>dir</li>
<li>port</li>
<li>dbfilename</li>
<li>logfile</li>
<li>pidfile</li>
</ul>
</li>
<li>第五步：复制redis6000.conf为redis7000.conf</li>
<li>第六步：修改redis7000.conf中的相关配置项<ul>
<li>port</li>
<li>dbfilename</li>
<li>logfile</li>
<li>pidfile</li>
</ul>
</li>
<li>第七步：复制redis6000.conf为redis8000.conf</li>
<li>第八步：修改redis8000.conf中的相关配置项<ul>
<li>port</li>
<li>dbfilename</li>
<li>logfile</li>
<li>pidfile</li>
</ul>
</li>
</ul>
<h3 id="③启动Redis主从复制集群"><a href="#③启动Redis主从复制集群" class="headerlink" title="③启动Redis主从复制集群"></a>③启动Redis主从复制集群</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">/usr/local/redis/bin/redis-server /usr/local/cluster-redis/redis6000.conf</span><br><span class="line">/usr/local/redis/bin/redis-server /usr/local/cluster-redis/redis7000.conf</span><br><span class="line">/usr/local/redis/bin/redis-server /usr/local/cluster-redis/redis8000.conf</span><br></pre></td></tr></table></figure>

<p>使用redis-cli停止指定服务器的命令格式如下：<br><br>/usr/local/bin/redis-cli -h IP地址 -p 端口号 shutdown</p>
<h2 id="3-主从关系"><a href="#3-主从关系" class="headerlink" title="3.主从关系"></a>3.主从关系</h2><h3 id="①查看主从关系"><a href="#①查看主从关系" class="headerlink" title="①查看主从关系"></a>①查看主从关系</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6000&gt; info replication</span><br><span class="line"># Replication</span><br><span class="line">role:master</span><br><span class="line">connected_slaves:0</span><br></pre></td></tr></table></figure>

<p>刚刚启动的集群服务器中每一个节点服务器都认为自己是主服务器。需要建立主从关系。</p>
<h3 id="②设定主从关系"><a href="#②设定主从关系" class="headerlink" title="②设定主从关系"></a>②设定主从关系</h3><p>在从机上指定主机位置即可</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">SLAVEOF 127.0.0.1 6000</span><br></pre></td></tr></table></figure>

<h3 id="③取消主从关系"><a href="#③取消主从关系" class="headerlink" title="③取消主从关系"></a>③取消主从关系</h3><p>在从机上执行命令</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">SLAVEOF NO ONE</span><br></pre></td></tr></table></figure>

<h2 id="4-初步测试"><a href="#4-初步测试" class="headerlink" title="4.初步测试"></a>4.初步测试</h2><ul>
<li>测试1：在主机写入数据，在从机查看</li>
<li>测试2：在从机写入数据报错。配置文件中的依据是：slave-read-only yes</li>
<li>测试3：主机执行SHUTDOWN看从机状态</li>
<li>测试4：主机恢复启动，看从机状态</li>
<li>测试5：从机SHUTDOWN，此时主机写入数据，从机恢复启动查看状态。重新设定主从关系后看新写入的数据是否同步。</li>
</ul>
<h2 id="5-哨兵模式"><a href="#5-哨兵模式" class="headerlink" title="5.哨兵模式"></a>5.哨兵模式</h2><h3 id="①作用"><a href="#①作用" class="headerlink" title="①作用"></a>①作用</h3><p>通过哨兵服务器监控master/slave实现主从复制集群的自动管理。</p>
<h3 id="②相关概念"><a href="#②相关概念" class="headerlink" title="②相关概念"></a>②相关概念</h3><h4 id="1-主观下线"><a href="#1-主观下线" class="headerlink" title="[1]主观下线"></a>[1]主观下线</h4><p>1台哨兵检测到某节点服务器下线。</p>
<h4 id="2-客观下线"><a href="#2-客观下线" class="headerlink" title="[2]客观下线"></a>[2]客观下线</h4><p>认为某个节点服务器下线的哨兵服务器达到指定数量。这个数量后面在哨兵的启动配置文件中指定。</p>
<h3 id="③配置方式"><a href="#③配置方式" class="headerlink" title="③配置方式"></a>③配置方式</h3><p>简单起见我们只配置一台哨兵。我们所需要做的就是创建一个哨兵服务器运行所需要的配置文件。</p>
<p>vim /usr/local/cluster-redis/sentinel.conf</p>
<table>
<thead>
<tr>
<th>格式</th>
<th>sentinel monitor 为主机命名 主机IP 主机端口号 将主机判定为下线时需要Sentinel同意的数量</th>
</tr>
</thead>
<tbody><tr>
<td>例子</td>
<td>sentinel monitor mymaster 127.0.0.1 6000 1</td>
</tr>
</tbody></table>
<h3 id="④启动哨兵"><a href="#④启动哨兵" class="headerlink" title="④启动哨兵"></a>④启动哨兵</h3><p>/usr/local/redis/bin/redis-server /usr/local/cluster-redis/sentinel.conf –sentinel</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">+sdown master mymaster 127.0.0.1 6379 【主观下线】</span><br><span class="line">+odown master mymaster 127.0.0.1 6379 #quorum 1/1【客观下线】</span><br><span class="line">……</span><br><span class="line">+vote-for-leader 17818eb9240c8a625d2c8a13ae9d99ae3a70f9d2 1【选举leader】</span><br><span class="line">……</span><br><span class="line">+failover-state-send-slaveof-noone slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6379【把一个从机设置为主机】</span><br><span class="line"></span><br><span class="line">-------------挂掉的主机又重新启动---------------------</span><br><span class="line">-sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ mymaster 127.0.0.1 6381【离开主观下线状态】</span><br><span class="line">+convert-to-slave slave 127.0.0.1:6379 127.0.0.1 6379 @ mymaster 127.0.0.1 6381【转换为从机】</span><br></pre></td></tr></table></figure>

<h1 id="Redis-的线程模型"><a href="#Redis-的线程模型" class="headerlink" title="Redis 的线程模型"></a>Redis 的线程模型</h1><p><strong>Redis</strong> 内部使用文件事件处理器 <code>file event handler</code>，这个文件事件处理器是单线程的，所以 <strong>Redis</strong> 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 <strong>Socket</strong>，根据 <strong>Socket</strong> 上的事件来选择对应的事件处理器进行处理。</p>
<p>文件事件处理器的结构包含 4 个部分：</p>
<ul>
<li>多个 <strong>Socket</strong></li>
<li>IO 多路复用程序</li>
<li>文件事件分派器</li>
<li>事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）</li>
</ul>
<p>多个 <strong>Socket</strong> 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 <strong>Socket</strong>，会将 <strong>Socket</strong> 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。</p>
<p>本段摘抄自：<a href="https://github.com/AobingJava/JavaFamily/blob/master/docs/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E3%80%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E3%80%81%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.md" target="_blank" rel="noopener">https://github.com/AobingJava/JavaFamily/blob/master/docs/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E3%80%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E3%80%81%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.md</a></p>
<h1 id="发布订阅"><a href="#发布订阅" class="headerlink" title="发布订阅"></a>发布订阅</h1><h2 id="1-订阅一个频道"><a href="#1-订阅一个频道" class="headerlink" title="1.订阅一个频道"></a>1.订阅一个频道</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; SUBSCRIBE cctv</span><br><span class="line">Reading messages... (press Ctrl-C to quit)</span><br><span class="line">1) "subscribe"</span><br><span class="line">2) "cctv"</span><br><span class="line">3) (integer) 1</span><br></pre></td></tr></table></figure>

<h2 id="2-在一个频道上发布信息"><a href="#2-在一个频道上发布信息" class="headerlink" title="2.在一个频道上发布信息"></a>2.在一个频道上发布信息</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; PUBLISH cctv hai</span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure>



<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; SUBSCRIBE cctv</span><br><span class="line">Reading messages... (press Ctrl-C to quit)</span><br><span class="line">1) "subscribe"</span><br><span class="line">2) "cctv"</span><br><span class="line">3) (integer) 1</span><br><span class="line">1) "message"</span><br><span class="line">2) "cctv"</span><br><span class="line">3) "hai"</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis系列（四）</title>
    <url>/bigdata/Redis%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Jedis"><a href="#Jedis" class="headerlink" title="Jedis"></a>Jedis</h1><h2 id="一个对比"><a href="#一个对比" class="headerlink" title="一个对比"></a>一个对比</h2><table>
<thead>
<tr>
<th></th>
<th>MySQL</th>
<th>Redis</th>
</tr>
</thead>
<tbody><tr>
<td>连接</td>
<td>Connection</td>
<td>Jedis</td>
</tr>
<tr>
<td>连接池</td>
<td>C3P0等等</td>
<td>JedisPool</td>
</tr>
<tr>
<td>操作完成</td>
<td>关闭连接</td>
<td>关闭连接</td>
</tr>
</tbody></table>
<h2 id="Redis准备"><a href="#Redis准备" class="headerlink" title="Redis准备"></a>Redis准备</h2><h3 id="①理解Redis配置文件中bind配置项含义"><a href="#①理解Redis配置文件中bind配置项含义" class="headerlink" title="①理解Redis配置文件中bind配置项含义"></a>①理解Redis配置文件中bind配置项含义</h3><p>bind后面跟的ip地址是客户端访问Redis时使用的IP地址。看下面例子：</p>
<table>
<thead>
<tr>
<th>bind值</th>
<th>访问方式</th>
</tr>
</thead>
<tbody><tr>
<td>127.0.0.1</td>
<td>./redis-cli -h 127.0.0.1</td>
</tr>
<tr>
<td>192.168.200.100</td>
<td>./redis-cli -h 192.168.200.100</td>
</tr>
</tbody></table>
<h3 id="②查看Linux系统本机IP"><a href="#②查看Linux系统本机IP" class="headerlink" title="②查看Linux系统本机IP"></a>②查看Linux系统本机IP</h3><p>远程客户端访问Linux服务器时不能使用127.0.0.1，要使用网络上的实际IP。可以用ifconfig命令查看。</p>
<h3 id="③将Redis配置文件中的bind配置项设置为本机IP。"><a href="#③将Redis配置文件中的bind配置项设置为本机IP。" class="headerlink" title="③将Redis配置文件中的bind配置项设置为本机IP。"></a>③将Redis配置文件中的bind配置项设置为本机IP。</h3><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">bind [你的实际IP]</span><br><span class="line">bind 192.168.200.100</span><br></pre></td></tr></table></figure>

<h2 id="Jedis-1"><a href="#Jedis-1" class="headerlink" title="Jedis"></a>Jedis</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//指定Redis服务器的IP地址和端口号</span></span><br><span class="line">Jedis jedis = <span class="keyword">new</span> Jedis(<span class="string">"192.168.200.100"</span>, <span class="number">6379</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//执行ping命令</span></span><br><span class="line">String ping = jedis.ping();</span><br><span class="line"></span><br><span class="line">System.out.println(ping);</span><br><span class="line"></span><br><span class="line"><span class="comment">//关闭连接</span></span><br><span class="line">jedis.close();</span><br></pre></td></tr></table></figure>

<h2 id="JedisPool"><a href="#JedisPool" class="headerlink" title="JedisPool"></a>JedisPool</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//声明Linux服务器IP地址</span></span><br><span class="line">String host = <span class="string">"192.168.200.100"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//声明Redis端口号</span></span><br><span class="line"><span class="keyword">int</span> port = Protocol.DEFAULT_PORT;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建连接池对象</span></span><br><span class="line">JedisPool jedisPool = <span class="keyword">new</span> JedisPool(host, port);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取Jedis对象连接Redis</span></span><br><span class="line">Jedis jedis = jedisPool.getResource();</span><br><span class="line"></span><br><span class="line"><span class="comment">//执行具体操作</span></span><br><span class="line">String ping = jedis.ping();</span><br><span class="line"></span><br><span class="line">System.out.println(ping);</span><br><span class="line"></span><br><span class="line"><span class="comment">//关闭连接</span></span><br><span class="line">jedisPool.close();</span><br></pre></td></tr></table></figure>

<h1 id="SpringBoot整合Redis"><a href="#SpringBoot整合Redis" class="headerlink" title="SpringBoot整合Redis"></a>SpringBoot整合Redis</h1><h2 id="pom-xml文件配置"><a href="#pom-xml文件配置" class="headerlink" title="pom.xml文件配置"></a>pom.xml文件配置</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 导入SpringBoot需要使用的依赖信息 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-dependencies<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.6.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>import<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 引入整合Redis所需的场景启动器 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-data-redis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 引入SpringBoot测试的场景启动器 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="创建application-yml配置文件"><a href="#创建application-yml配置文件" class="headerlink" title="创建application.yml配置文件"></a>创建application.yml配置文件</h2><p>在main/resources目录下创建application.yml，在这个配置文件中配置Redis连接信息</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">redis:</span></span><br><span class="line">    <span class="attr">host:</span> <span class="number">192.168</span><span class="number">.19</span><span class="number">.88</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">6379</span></span><br></pre></td></tr></table></figure>



<h2 id="创建主启动类"><a href="#创建主启动类" class="headerlink" title="创建主启动类"></a>创建主启动类</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SpringBootMainClass</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(SpringBootMainClass<span class="class">.<span class="keyword">class</span>, <span class="title">args</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="创建测试类"><a href="#创建测试类" class="headerlink" title="创建测试类"></a>创建测试类</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RunWith</span>(SpringRunner<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">@<span class="title">SpringBootTest</span></span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">SpringBootRedisTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="SpringBoot提供的模板类"><a href="#SpringBoot提供的模板类" class="headerlink" title="SpringBoot提供的模板类"></a>SpringBoot提供的模板类</h2><h3 id="①带泛型的模板类"><a href="#①带泛型的模板类" class="headerlink" title="①带泛型的模板类"></a>①带泛型的模板类</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisTemplate</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>虽然使用泛型可以指定各种具体数据类型，但是用着不方便。Redis中最基本的数据类型就是字符串。</p>
<h3 id="②不带泛型的模板类"><a href="#②不带泛型的模板类" class="headerlink" title="②不带泛型的模板类"></a>②不带泛型的模板类</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringRedisTemplate</span> <span class="keyword">extends</span> <span class="title">RedisTemplate</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>StringRedisTemplate在继承RedisTemplate时直接指定泛型为String类型，所有操作都按照字符串处理，使用非常方便。</p>
<h3 id="③基本用法"><a href="#③基本用法" class="headerlink" title="③基本用法"></a>③基本用法</h3><h4 id="1-Key相关操作"><a href="#1-Key相关操作" class="headerlink" title="[1]Key相关操作"></a>[1]Key相关操作</h4><table>
<thead>
<tr>
<th>Redis命令</th>
<th>StringRedisTemplate的对应方法</th>
</tr>
</thead>
<tbody><tr>
<td>KEYS PATTERN</td>
<td>public Set<K> keys(K pattern)</K></td>
</tr>
<tr>
<td>TYPE KEY</td>
<td>public DataType type(K key)</td>
</tr>
<tr>
<td>MOVE KEY DB</td>
<td>public Boolean move(K key, final int dbIndex)</td>
</tr>
<tr>
<td>EXISTS KEY</td>
<td>public Long countExistingKeys(Collection<K> keys)</K></td>
</tr>
<tr>
<td>RANDOMKEY</td>
<td>public K randomKey()</td>
</tr>
<tr>
<td>RENAME KEY NEWKEY</td>
<td>public void rename(K oldKey, K newKey)</td>
</tr>
<tr>
<td>RENAMENX KEY NEWKEY</td>
<td>public Boolean renameIfAbsent(K oldKey, K newKey)</td>
</tr>
<tr>
<td>TTL KEY</td>
<td>public Long getExpire(K key)</td>
</tr>
<tr>
<td>EXPIRE KEY SECONDS</td>
<td>public Boolean expire(K key, final long timeout, final TimeUnit unit)</td>
</tr>
<tr>
<td>PERSIST KEY</td>
<td>public Boolean persist(K key)</td>
</tr>
</tbody></table>
<h4 id="2-string类型操作"><a href="#2-string类型操作" class="headerlink" title="[2]string类型操作"></a>[2]string类型操作</h4><p>先获取“操作对象”，然后调用“操作对象”的相关方法即可</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ValueOperations&lt;String, String&gt; operations = stringRedisTemplate.opsForValue();</span><br><span class="line">operations.set(<span class="string">"hello"</span>, <span class="string">"hello-value"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="3-list类型操作"><a href="#3-list类型操作" class="headerlink" title="[3]list类型操作"></a>[3]list类型操作</h4><p>先获取“操作对象”，然后调用“操作对象”的相关方法即可</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ListOperations&lt;String, String&gt; operations = stringRedisTemplate.opsForList();</span><br><span class="line">List&lt;String&gt; fruitList = operations.range(<span class="string">"fruit"</span>, <span class="number">0</span>, -<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> (String fruit : fruitList) &#123;</span><br><span class="line">    System.out.println(fruit);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-set类型操作"><a href="#4-set类型操作" class="headerlink" title="[4]set类型操作"></a>[4]set类型操作</h4><p>先获取“操作对象”，然后调用“操作对象”的相关方法即可</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SetOperations&lt;String, String&gt; operations = stringRedisTemplate.opsForSet();</span><br><span class="line">Set&lt;String&gt; animalSet = operations.members(<span class="string">"animal"</span>);</span><br><span class="line"><span class="keyword">for</span> (String animal : animalSet) &#123;</span><br><span class="line">    System.out.println(<span class="string">"animal="</span>+animal);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-hash类型操作"><a href="#5-hash类型操作" class="headerlink" title="[5]hash类型操作"></a>[5]hash类型操作</h4><p>先获取“操作对象”，然后调用“操作对象”的相关方法即可</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">HashOperations&lt;String, Object, Object&gt; operations = stringRedisTemplate.opsForHash();</span><br><span class="line">Map&lt;Object, Object&gt; studentMap = operations.entries(<span class="string">"student"</span>);</span><br><span class="line">Set&lt;Map.Entry&lt;Object, Object&gt;&gt; entries = studentMap.entrySet();</span><br><span class="line"><span class="keyword">for</span> (Map.Entry&lt;Object, Object&gt; entry : entries) &#123;</span><br><span class="line">    Object key = entry.getKey();</span><br><span class="line">    Object value = entry.getValue();</span><br><span class="line">    System.out.println(key+<span class="string">"="</span>+value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="6-zset类型操作"><a href="#6-zset类型操作" class="headerlink" title="[6]zset类型操作"></a>[6]zset类型操作</h4><p>先获取“操作对象”，然后调用“操作对象”的相关方法即可</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ZSetOperations&lt;String, String&gt; operations = stringRedisTemplate.opsForZSet();</span><br><span class="line">Set&lt;ZSetOperations.TypedTuple&lt;String&gt;&gt; chengjiSet = operations.rangeWithScores(<span class="string">"chengji"</span>, <span class="number">0</span>, -<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> (ZSetOperations.TypedTuple&lt;String&gt; stringTypedTuple : chengjiSet) &#123;</span><br><span class="line">    String value = stringTypedTuple.getValue();</span><br><span class="line">    Double score = stringTypedTuple.getScore();</span><br><span class="line">    System.out.println(value+<span class="string">"="</span>+score);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><h3 id="①SpringBoot的自动化配置包"><a href="#①SpringBoot的自动化配置包" class="headerlink" title="①SpringBoot的自动化配置包"></a>①SpringBoot的自动化配置包</h3><p>spring-boot-autoconfigure-2.1.6.RELEASE.jar</p>
<h3 id="②自动化配置包中的重要配置文件"><a href="#②自动化配置包中的重要配置文件" class="headerlink" title="②自动化配置包中的重要配置文件"></a>②自动化配置包中的重要配置文件</h3><p>在META-INF目录下有一个spring.factories文件</p>
<h3 id="③重要配置文件中配置了大量自动化配置类"><a href="#③重要配置文件中配置了大量自动化配置类" class="headerlink" title="③重要配置文件中配置了大量自动化配置类"></a>③重要配置文件中配置了大量自动化配置类</h3><p>属性名是：org.springframework.boot.autoconfigure.EnableAutoConfiguration</p>
<h3 id="④Redis的自动化配置类"><a href="#④Redis的自动化配置类" class="headerlink" title="④Redis的自动化配置类"></a>④Redis的自动化配置类</h3><p>org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@ConditionalOnClass</span>(RedisOperations<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">@<span class="title">EnableConfigurationProperties</span>(<span class="title">RedisProperties</span>.<span class="title">class</span>)</span></span><br><span class="line"><span class="class">@<span class="title">Import</span>(</span>&#123; LettuceConnectionConfiguration<span class="class">.<span class="keyword">class</span>, <span class="title">JedisConnectionConfiguration</span>.<span class="title">class</span> &#125;)</span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">RedisAutoConfiguration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Bean</span></span><br><span class="line">	<span class="meta">@ConditionalOnMissingBean</span>(name = <span class="string">"redisTemplate"</span>)</span><br><span class="line">	<span class="function"><span class="keyword">public</span> RedisTemplate&lt;Object, Object&gt; <span class="title">redisTemplate</span><span class="params">(RedisConnectionFactory redisConnectionFactory)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> UnknownHostException </span>&#123;</span><br><span class="line">		RedisTemplate&lt;Object, Object&gt; template = <span class="keyword">new</span> RedisTemplate&lt;&gt;();</span><br><span class="line">		template.setConnectionFactory(redisConnectionFactory);</span><br><span class="line">		<span class="keyword">return</span> template;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Bean</span></span><br><span class="line">	<span class="meta">@ConditionalOnMissingBean</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> StringRedisTemplate <span class="title">stringRedisTemplate</span><span class="params">(RedisConnectionFactory redisConnectionFactory)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> UnknownHostException </span>&#123;</span><br><span class="line">		StringRedisTemplate template = <span class="keyword">new</span> StringRedisTemplate();</span><br><span class="line">		template.setConnectionFactory(redisConnectionFactory);</span><br><span class="line">		<span class="keyword">return</span> template;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>SpringBoot启动时会参考@ConditionalXxx注解决定是否加载这个类到IOC容器中。</p>
<h3 id="⑤配置项的默认值"><a href="#⑤配置项的默认值" class="headerlink" title="⑤配置项的默认值"></a>⑤配置项的默认值</h3><p>RedisProperties类中定义了Redis配置的默认值。</p>
<p>GitHub骚操作</p>
<p>seckill in:name,readme,description</p>
<p>sckill stars:&gt;=500</p>
<p>seckill forks:&gt;=1000</p>
<p>seckill stars:1000..2000 forks:300..500</p>
<p>awesome spring</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>【转载】2&gt;/dev/null和&gt;/dev/null 2&gt;&amp;1和2&gt;&amp;1&gt;/dev/null的区别</title>
    <url>/Linux%E5%9F%BA%E7%A1%80/bash/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="一、区别"><a href="#一、区别" class="headerlink" title="一、区别"></a>一、区别</h1><h2 id="2-gt-dev-null"><a href="#2-gt-dev-null" class="headerlink" title="2&gt;/dev/null"></a>2&gt;/dev/null</h2><p>意思就是把错误输出到“黑洞”</p>
<blockquote>
<p>/dev/null 2&gt;&amp;1<br>默认情况是1，也就是等同于1&gt;/dev/null 2&gt;&amp;1。意思就是把标准输出重定向到“黑洞”，还把错误输出2重定向到标准输出1，也就是标准输出和错误输出都进了“黑洞”</p>
</blockquote>
<h2 id="2-gt-amp-1-gt-dev-null"><a href="#2-gt-amp-1-gt-dev-null" class="headerlink" title="2&gt;&amp;1 &gt;/dev/null"></a>2&gt;&amp;1 &gt;/dev/null</h2><p>意思就是把错误输出2重定向到标准出书1，也就是屏幕，标准输出进了“黑洞”，也就是标准输出进了黑洞，错误输出打印到屏幕</p>
<h1 id="二、解释"><a href="#二、解释" class="headerlink" title="二、解释"></a>二、解释</h1><h2 id="1、文件描述符"><a href="#1、文件描述符" class="headerlink" title="1、文件描述符"></a>1、文件描述符</h2><p>Linux系统预留可三个文件描述符：0、1和2，他们的意义如下所示：</p>
<p>0——标准输入（stdin）</p>
<p>1——标准输出（stdout）</p>
<p>2——标准错误（stderr）</p>
<p>标准输出——stdout</p>
<p>假设:在当前目录下，有且只有一个文件名称为ljl.txt的文件，这时我们运行这个命令【ls ljl.txt】,就会获得一个标准输出stdout的输出结果：ljl.txt<br><img src="/Linux%E5%9F%BA%E7%A1%80/bash/20190524165041606.png" alt="在这里插入图片描述"></p>
<p>错误输出——stderr</p>
<p>按照上面的假设，我们运行另一条命令【ls gss.txt】，这样我们就会获得一个标准错误stderr的输出结果“ls：无法访问gss.txt：没有那个文件或目录”。<br><img src="/Linux%E5%9F%BA%E7%A1%80/bash/20190524165513576.png" alt="在这里插入图片描述"></p>
<h2 id="2、重定向"><a href="#2、重定向" class="headerlink" title="2、重定向"></a>2、重定向</h2><p>重定向的符号有两个：&gt;或&gt;&gt;，两者的区别是：前者会先清空文件，然后再写入内容，后者会将重定向的内容追加到现有文件的尾部。举个例子：</p>
<p>（1）、重定向标准输出stdout<br><img src="/Linux%E5%9F%BA%E7%A1%80/bash/20190524165513576.png" alt="在这里插入图片描述"></p>
<p>如上图所示，对比没有添加重定向的操作，这条命令在使用之后并没有将123.txt打印到屏幕。在紧接的cat操作后，可以发现本来应该被输出的内容被记录到stdout.txt中。</p>
<p>（2）、重定向标准错误stderr<br><img src="/Linux%E5%9F%BA%E7%A1%80/bash/20190524165600561.png" alt="在这里插入图片描述"></p>
<p>如上图所示，文件描述符2，标准错误的重定向也是同样的原理被记录在了文件stderr.txt这个文件里面了。</p>
<p>（3）、可以将stderr单独定向到一个文件，stdout重定向到另一个文件<br>cmd 2&gt; stderr.txt 1&gt;stdout.txt</p>
<p>（4）、也可以将stderr和stdout重定向到同一个文件<br>cmd &gt; output.txt 2&gt;&amp;1</p>
<p>或采用下面的方法，可以少写几个字，能达到同样的效果</p>
<p>cmd &amp;&gt; output.txt</p>
<p>cmd &gt;&amp; output.txt #两个表达式效果一样的</p>
<h2 id="3、Linux特殊文件"><a href="#3、Linux特殊文件" class="headerlink" title="3、Linux特殊文件"></a>3、Linux特殊文件</h2><p>/dev/null是一个特殊的设备文件，这个文件接收到任何数据都会被丢弃。因此，null这个设备通常也被称为位桶（bit bucket）或黑洞。<br>所以，2&gt;/dev/null的意思就是将标准错误stderr删掉。</p>
<p>附：linux中单进程的文件数据结构图</p>
<p><img src="/Linux%E5%9F%BA%E7%A1%80/bash/2019052417015153.png" alt="在这里插入图片描述"></p>
<p>上图是linux中单进程的文件数据结构图，最左边使我们熟悉的fd标志，也就是文件描述符，一个进程内所有的文件描述符按照顺序排列构成一张文件描述符表，其中包括fd0，fd1，fd2。（注意：这里并没有说标准输入，标准输出，错误输出，原因后面讲）<br>　　那么，问题来了，假如我们想fd1写入数据时，最终数据会到哪儿呢？事实上fd1作为文件描述符，它本身并不是文件的真正的“入口”，文件真正的“入口”在文件描述符表的第二列：记录了每个文件描述符所对应文件位置的文件指针。换言之，如果我们更换fd1所对应的文件指针，就改变了fd1指向文件的”真正位置”。<br>　　fd0，fd1，fd2指向的文件默认情况下分别是/dev/stdin、/dev/stdout和/dev/stderr，这才是真正的标准输入，标准输出，错误输出，如果将数据写入到/dev/stdout中，就会在屏幕上显示数据，fd0，fd1，fd2只是标志而已，真正起作用的是他们对应的文件指针！<br>　　所以重定向命令’&gt;’所做的工作就是就是改变了fd所对应的文件指针！</p>
<p>转载自：</p>
<p><a href="https://blog.csdn.net/longgeaisisi/article/details/90519690" target="_blank" rel="noopener">https://blog.csdn.net/longgeaisisi/article/details/90519690</a></p>
]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title>常用端口号一览（一）</title>
    <url>/tools/%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7%E4%B8%80%E8%A7%88%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><table>
<thead>
<tr>
<th>端口类型</th>
<th>端口号</th>
<th>注释</th>
</tr>
</thead>
<tbody><tr>
<td>TCP</td>
<td>0</td>
<td>Reserved</td>
</tr>
<tr>
<td>TCP</td>
<td>1</td>
<td>TCP Port Service Multiplexer</td>
</tr>
<tr>
<td>TCP</td>
<td>2</td>
<td>Death</td>
</tr>
<tr>
<td>TCP</td>
<td>5</td>
<td>Remote Job Entry,yoyo</td>
</tr>
<tr>
<td>TCP</td>
<td>7</td>
<td>Echo</td>
</tr>
<tr>
<td>TCP</td>
<td>11</td>
<td>Skun</td>
</tr>
<tr>
<td>TCP</td>
<td>12</td>
<td>Bomber</td>
</tr>
<tr>
<td>TCP</td>
<td>16</td>
<td>Skun</td>
</tr>
<tr>
<td>TCP</td>
<td>17</td>
<td>Skun</td>
</tr>
<tr>
<td>TCP</td>
<td>18</td>
<td>消息传输协议，skun</td>
</tr>
<tr>
<td>TCP</td>
<td>19</td>
<td>Skun</td>
</tr>
<tr>
<td><font color="blue"><strong>TCP</strong></font></td>
<td><strong><font color="blue">20</font></strong></td>
<td><strong><font color="blue">FTP Data,Amanda</font></strong></td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">21</font></strong></td>
<td><font color="blue"><strong>文件传输,Back Construction,Blade Runner,Doly Trojan,Fore,FTP trojan,Invisible FTP,Larva,WebEx,WinCrash</strong></font></td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">22</font></strong></td>
<td><strong><font color="blue">远程登录协议</font></strong></td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">23</font></strong></td>
<td><strong><font color="blue">远程登录（Telnet),Tiny Telnet Server (</font></strong></td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">25</font></strong></td>
<td><strong><font color="blue">电子邮件(SMTP),Ajan,Antigen,Email Password Sender,Happy 99,Kuang2,ProMail trojan,Shtrilitz,Stealth,Tapiras,Terminator,WinPC,WinSpy,Haebu Coceda</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>27</td>
<td>Assasin</td>
</tr>
<tr>
<td>TCP</td>
<td>28</td>
<td>Amanda</td>
</tr>
<tr>
<td>TCP</td>
<td>29</td>
<td>MSG ICP</td>
</tr>
<tr>
<td>TCP</td>
<td>30</td>
<td>Agent 40421</td>
</tr>
<tr>
<td>TCP</td>
<td>31</td>
<td>Agent 31,Hackers Paradise,Masters Paradise,Agent 40421</td>
</tr>
<tr>
<td>TCP</td>
<td>37</td>
<td>Time,ADM worm</td>
</tr>
<tr>
<td>TCP</td>
<td>39</td>
<td>SubSARI</td>
</tr>
<tr>
<td>TCP</td>
<td>41</td>
<td>DeepThroat,Foreplay</td>
</tr>
<tr>
<td>TCP</td>
<td>42</td>
<td>Host Name Server</td>
</tr>
<tr>
<td>TCP</td>
<td>43</td>
<td>WHOIS</td>
</tr>
<tr>
<td>TCP</td>
<td>44</td>
<td>Arctic</td>
</tr>
<tr>
<td>TCP</td>
<td>48</td>
<td>DRAT</td>
</tr>
<tr>
<td>TCP</td>
<td>49</td>
<td>主机登录协议</td>
</tr>
<tr>
<td>TCP</td>
<td>50</td>
<td>DRAT</td>
</tr>
<tr>
<td>TCP</td>
<td>51</td>
<td>IMP Logical Address Maintenance,Fuck Lamers Backdoor</td>
</tr>
<tr>
<td>TCP</td>
<td>52</td>
<td>MuSka52,Skun</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">53</font></strong></td>
<td><strong><font color="blue">DNS,Bonk (DOS Exploit)</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>54</td>
<td>MuSka52</td>
</tr>
<tr>
<td>TCP</td>
<td>58</td>
<td>DMSetup</td>
</tr>
<tr>
<td>TCP</td>
<td>59</td>
<td>DMSetup</td>
</tr>
<tr>
<td>TCP</td>
<td>63</td>
<td>whois++</td>
</tr>
<tr>
<td>TCP</td>
<td>64</td>
<td>Communications Integrator</td>
</tr>
<tr>
<td>TCP</td>
<td>65</td>
<td>TACACS-Database Service</td>
</tr>
<tr>
<td>TCP</td>
<td>66</td>
<td>Oracle SQL*NET,AL-Bareki</td>
</tr>
<tr>
<td>TCP</td>
<td>67</td>
<td>Bootstrap Protocol Server</td>
</tr>
<tr>
<td>TCP</td>
<td>68</td>
<td>Bootstrap Protocol Client</td>
</tr>
<tr>
<td>TCP</td>
<td>69</td>
<td>TFTP,W32.Evala.Worm,BackGate Kit,Nimda,Pasana,Storm,Storm worm,Theef,Worm.Cycle.a</td>
</tr>
<tr>
<td>TCP</td>
<td>70</td>
<td>Gopher服务，ADM worm</td>
</tr>
<tr>
<td>TCP</td>
<td>79</td>
<td>用户查询（Finger),Firehotcker,ADM worm</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">80</font></strong></td>
<td><strong><font color="blue">超文本服务器（Http),Executor,RingZero</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>81</td>
<td>Chubo,Worm.Bbeagle.q</td>
</tr>
<tr>
<td>TCP</td>
<td>82</td>
<td>Netsky-Z</td>
</tr>
<tr>
<td>TCP</td>
<td>88</td>
<td>Kerberos krb5服务</td>
</tr>
<tr>
<td>TCP</td>
<td>99</td>
<td>Hidden Port</td>
</tr>
<tr>
<td>TCP</td>
<td>102</td>
<td>消息传输代理</td>
</tr>
<tr>
<td>TCP</td>
<td>108</td>
<td>SNA网关访问服务器</td>
</tr>
<tr>
<td>TCP</td>
<td>109</td>
<td>Pop2</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">110</font></strong></td>
<td><strong><font color="blue">电子邮件（Pop3),ProMail</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>113</td>
<td>Kazimas,Auther Idnet</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">115</font></strong></td>
<td><strong><font color="blue">简单文件传输协议</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>118</td>
<td>SQL Services,Infector 1.4.2</td>
</tr>
<tr>
<td>TCP</td>
<td>119</td>
<td>新闻组传输协议（Newsgroup(Nntp)),Happy 99</td>
</tr>
<tr>
<td>TCP</td>
<td>121</td>
<td>JammerKiller,Bo jammerkillah</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">123</font></strong></td>
<td><strong><font color="blue">网络时间协议(NTP),Net Controller</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>129</td>
<td>Password Generator Protocol</td>
</tr>
<tr>
<td>TCP</td>
<td>133</td>
<td>Infector 1.x</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">135</font></strong></td>
<td><strong><font color="blue">微软DCE RPC end-point mapper服务</font></strong></td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">137</font></strong></td>
<td><strong><font color="blue">微软Netbios Name服务（网上邻居传输文件使用）</font></strong></td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">138</font></strong></td>
<td><strong><font color="blue">微软Netbios Name服务（网上邻居传输文件使用）</font></strong></td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">139</font></strong></td>
<td><strong><font color="blue">微软Netbios Name服务（用于文件及打印机共享）</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>142</td>
<td>NetTaxi</td>
</tr>
<tr>
<td>TCP</td>
<td>143</td>
<td>Internet 邮件访问协议版本 4（IMAP4)</td>
</tr>
<tr>
<td>TCP</td>
<td>146</td>
<td>FC Infector,Infector</td>
</tr>
<tr>
<td>TCP</td>
<td>150</td>
<td>NetBIOS Session Service</td>
</tr>
<tr>
<td>TCP</td>
<td>156</td>
<td>SQL服务器</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">161</font></strong></td>
<td><strong><font color="blue">Snmp</font></strong></td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">162</font></strong></td>
<td><strong><font color="blue">Snmp-Trap</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>170</td>
<td>A-Trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>177</td>
<td>X Display管理控制协议</td>
</tr>
<tr>
<td>TCP</td>
<td>179</td>
<td>Border网关协议（BGP)</td>
</tr>
<tr>
<td>TCP</td>
<td>190</td>
<td>网关访问控制协议（GACP)</td>
</tr>
<tr>
<td>TCP</td>
<td>194</td>
<td>Irc</td>
</tr>
<tr>
<td>TCP</td>
<td>197</td>
<td>目录定位服务（DLS)</td>
</tr>
<tr>
<td>TCP</td>
<td>220</td>
<td>Internet 邮件访问协议版本 3（IMAP3)</td>
</tr>
<tr>
<td>TCP</td>
<td>256</td>
<td>Nirvana</td>
</tr>
<tr>
<td>TCP</td>
<td>315</td>
<td>The Invasor</td>
</tr>
<tr>
<td>TCP</td>
<td>371</td>
<td>ClearCase版本管理软件</td>
</tr>
<tr>
<td>TCP</td>
<td>389</td>
<td>Lightweight Directory Access Protocol (LDAP)</td>
</tr>
<tr>
<td>TCP</td>
<td>396</td>
<td>Novell Netware over IP</td>
</tr>
<tr>
<td>TCP</td>
<td>420</td>
<td>Breach</td>
</tr>
<tr>
<td>TCP</td>
<td>421</td>
<td>TCP Wrappers</td>
</tr>
<tr>
<td><strong><font color="blue">TCP</font></strong></td>
<td><strong><font color="blue">443</font></strong></td>
<td><strong><font color="blue">安全服务（HTTPS）</font></strong></td>
</tr>
<tr>
<td>TCP</td>
<td>444</td>
<td>Simple Network Paging Protocol(SNPP)</td>
</tr>
<tr>
<td>TCP</td>
<td>445</td>
<td>Microsoft-DS</td>
</tr>
<tr>
<td>TCP</td>
<td>455</td>
<td>Fatal Connections</td>
</tr>
<tr>
<td>TCP</td>
<td>456</td>
<td>Hackers paradise,FuseSpark</td>
</tr>
<tr>
<td>TCP</td>
<td>458</td>
<td>苹果公司QuickTime</td>
</tr>
<tr>
<td>TCP</td>
<td>513</td>
<td>Grlogin</td>
</tr>
<tr>
<td>TCP</td>
<td>514</td>
<td>RPC Backdoor</td>
</tr>
<tr>
<td>TCP</td>
<td>531</td>
<td>Rasmin,Net666</td>
</tr>
<tr>
<td>TCP</td>
<td>544</td>
<td>kerberos kshell</td>
</tr>
<tr>
<td>TCP</td>
<td>546</td>
<td>DHCP Client</td>
</tr>
<tr>
<td>TCP</td>
<td>547</td>
<td>DHCP Server</td>
</tr>
<tr>
<td>TCP</td>
<td>548</td>
<td>Macintosh文件服务</td>
</tr>
<tr>
<td>TCP</td>
<td>555</td>
<td>Ini-Killer,Phase Zero,Stealth Spy</td>
</tr>
<tr>
<td>TCP</td>
<td>569</td>
<td>MSN</td>
</tr>
<tr>
<td>TCP</td>
<td>605</td>
<td>SecretService</td>
</tr>
<tr>
<td>TCP</td>
<td>606</td>
<td>Noknok8</td>
</tr>
<tr>
<td>TCP</td>
<td>660</td>
<td>DeepThroat</td>
</tr>
<tr>
<td>TCP</td>
<td>661</td>
<td>Noknok8</td>
</tr>
<tr>
<td>TCP</td>
<td>666</td>
<td>Attack FTP,Satanz Backdoor,Back Construction,Dark Connection Inside 1.2</td>
</tr>
<tr>
<td>TCP</td>
<td>667</td>
<td>Noknok7.2</td>
</tr>
<tr>
<td>TCP</td>
<td>668</td>
<td>Noknok6</td>
</tr>
<tr>
<td>TCP</td>
<td>669</td>
<td>DP trojan</td>
</tr>
<tr>
<td>TCP</td>
<td>692</td>
<td>GayOL</td>
</tr>
<tr>
<td>TCP</td>
<td>707</td>
<td>Welchia,nachi</td>
</tr>
<tr>
<td>TCP</td>
<td>777</td>
<td>AIM Spy</td>
</tr>
<tr>
<td>TCP</td>
<td>808</td>
<td>RemoteControl,WinHole</td>
</tr>
<tr>
<td>TCP</td>
<td>815</td>
<td>Everyone Darling</td>
</tr>
<tr>
<td>TCP</td>
<td>901</td>
<td>Backdoor.Devil</td>
</tr>
<tr>
<td>TCP</td>
<td>911</td>
<td>Dark Shadow</td>
</tr>
<tr>
<td>TCP</td>
<td>990</td>
<td>ssl加密</td>
</tr>
<tr>
<td>TCP</td>
<td>993</td>
<td>IMAP</td>
</tr>
<tr>
<td>TCP</td>
<td>999</td>
<td>DeepThroat</td>
</tr>
<tr>
<td>TCP</td>
<td>1000</td>
<td>Der Spaeher</td>
</tr>
<tr>
<td>TCP</td>
<td>1001</td>
<td>Silencer,WebEx,Der Spaeher</td>
</tr>
<tr>
<td>TCP</td>
<td>1003</td>
<td>BackDoor</td>
</tr>
<tr>
<td>TCP</td>
<td>1010</td>
<td>Doly</td>
</tr>
<tr>
<td>TCP</td>
<td>1011</td>
<td>Doly</td>
</tr>
<tr>
<td>TCP</td>
<td>1012</td>
<td>Doly</td>
</tr>
<tr>
<td>TCP</td>
<td>1015</td>
<td>Doly</td>
</tr>
<tr>
<td>TCP</td>
<td>1016</td>
<td>Doly</td>
</tr>
<tr>
<td>TCP</td>
<td>1020</td>
<td>Vampire</td>
</tr>
<tr>
<td>TCP</td>
<td>1023</td>
<td>Worm.Sasser.e</td>
</tr>
</tbody></table>
<p>整理来自于：今天超市开门了吗</p>
<p>（非本人整理）</p>
]]></content>
      <categories>
        <category>工具集</category>
      </categories>
      <tags>
        <tag>常用端口</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP和UDP的区别</title>
    <url>/computernetwork/TCP%E5%92%8CUDP%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="TCP与UDP基本区别"><a href="#TCP与UDP基本区别" class="headerlink" title="TCP与UDP基本区别"></a>TCP与UDP基本区别</h1><p>1.基于连接与无连接 </p>
<p>2.TCP要求系统资源较多，UDP较少</p>
<p>3.UDP程序结构较简单 </p>
<p>4.流模式（TCP）与数据报模式(UDP)</p>
<p>5.TCP保证数据正确性，UDP可能丢包 </p>
<p>6.TCP保证数据顺序，UDP不保证 
　　</p>
<p>UDP应用场景</p>
<p> 1.面向数据报方式</p>
<p> 2.网络数据大多为短消息 </p>
<p> 3.拥有大量Client</p>
<p>4.对数据安全性无特殊要求</p>
<p>5.网络负担非常重，但对响应速度要求高</p>
<h1 id="具体编程时的区别"><a href="#具体编程时的区别" class="headerlink" title="具体编程时的区别"></a>具体编程时的区别</h1><p>1.socket()的参数不同 </p>
<p>2.UDP Server不需要调用listen和accept </p>
<p>3.UDP收发数据用sendto/recvfrom函数 </p>
<p>4.TCP：地址信息在connect/accept时确定 </p>
<p>5.UDP：在sendto/recvfrom函数中每次均 需指定地址信息 </p>
<p>6.UDP：shutdown函数无效</p>
<h1 id="编程区别"><a href="#编程区别" class="headerlink" title="编程区别"></a>编程区别</h1><p>  <font color="red">通常我们在说到网络编程时默认是指TCP编程，即用前面提到的socket函数创建一个socket用于TCP通讯，函数参数我们通常填为SOCK_STREAM。即socket(PF_INET, SOCK_STREAM, 0)，这表示建立一个socket用于流式网络通讯。<br>　  SOCK_STREAM这种的特点是面向连接的，即每次收发数据之前必须通过connect建立连接，也是双向的，即任何一方都可以收发数据，协议本身提供了一些保障机制保证它是可靠的、有序的，即每个包按照发送的顺序到达接收方。 </font></p>
<p><font color="red">而SOCK_DGRAM这种是User Datagram Protocol协议的网络通讯，它是无连接的，不可靠的，因为通讯双方发送数据后不知道对方是否已经收到数据，是否正常收到数据。任何一方建立一个socket以后就可以用sendto发送数据，也可以用recvfrom接收数据。根本不关心对方是否存在，是否发送了数据。它的特点是通讯速度比较快。大家都知道TCP是要经过三次握手的，而UDP没有。  </font></p>
<p>基于上述不同，UDP和TCP编程步骤也有些不同。</p>
<h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><h3 id="TCP编程的服务器端一般步骤"><a href="#TCP编程的服务器端一般步骤" class="headerlink" title="TCP编程的服务器端一般步骤"></a>TCP编程的服务器端一般步骤</h3><p>1、创建一个socket，用函数socket()；<br>　　2、设置socket属性，用函数setsockopt(); * 可选<br>　　3、绑定IP地址、端口等信息到socket上，用函数bind();<br>　　4、开启监听，用函数listen()；<br>　　5、接收客户端上来的连接，用函数accept()；<br>　　6、收发数据，用函数send()和recv()，或者read()和write();<br>　　7、关闭网络连接；<br>　　8、关闭监听； </p>
<h3 id="TCP编程的客户端一般步骤"><a href="#TCP编程的客户端一般步骤" class="headerlink" title="TCP编程的客户端一般步骤"></a>TCP编程的客户端一般步骤</h3><p>1、创建一个socket，用函数socket()；<br>　　2、设置socket属性，用函数setsockopt();* 可选<br>　　3、绑定IP地址、端口等信息到socket上，用函数bind();* 可选<br>　　4、设置要连接的对方的IP地址和端口等属性；<br>　　5、连接服务器，用函数connect()；<br>　　6、收发数据，用函数send()和recv()，或者read()和write();<br>　　7、关闭网络连接；</p>
<h2 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h2><p>与之对应的UDP编程步骤要简单许多。</p>
<h3 id="UDP编程的服务器端一般步骤"><a href="#UDP编程的服务器端一般步骤" class="headerlink" title="UDP编程的服务器端一般步骤"></a>UDP编程的服务器端一般步骤</h3><p>　　1、创建一个socket，用函数socket()；<br>　　2、设置socket属性，用函数setsockopt();* 可选<br>　　3、绑定IP地址、端口等信息到socket上，用函数bind();<br>　　4、循环接收数据，用函数recvfrom();<br>　　5、关闭网络连接； </p>
<h3 id="UDP编程的客户端一般步骤"><a href="#UDP编程的客户端一般步骤" class="headerlink" title="UDP编程的客户端一般步骤"></a>UDP编程的客户端一般步骤</h3><p>　　1、创建一个socket，用函数socket()；<br>　　2、设置socket属性，用函数setsockopt();* 可选<br>　　3、绑定IP地址、端口等信息到socket上，用函数bind();* 可选<br>　　4、设置对方的IP地址和端口等属性;<br>　　5、发送数据，用函数sendto();<br>　　6、关闭网络连接；</p>
<p>TCP和UDP是OSI模型中的运输层中的协议。TCP提供可靠的通信传输，而UDP则常被用于让广播和细节控制交给应用的通信传输。</p>
<h1 id="区别补充"><a href="#区别补充" class="headerlink" title="区别补充"></a>区别补充</h1><h2 id="UDP-1"><a href="#UDP-1" class="headerlink" title="UDP"></a>UDP</h2><p>​    UDP不提供复杂的控制机制，利用IP提供面向无连接的通信服务。并且它是将应用程序发来的数据在收到的那一刻，立刻按照原样发送到网络上的一种机制。即使是出现网络拥堵的情况下，UDP也无法进行流量控制等避免网络拥塞的行为。此外，传输途中如果出现了丢包，UDO也不负责重发。甚至当出现包的到达顺序乱掉时也没有纠正的功能。如果需要这些细节控制，那么不得不交给由采用UDO的应用程序去处理。换句话说，UDP将部分控制转移到应用程序去处理，自己却只提供作为传输层协议的最基本功能。UDP有点类似于用户说什么听什么的机制，但是需要用户充分考虑好上层协议类型并制作相应的应用程序。</p>
<h2 id="TCP-1"><a href="#TCP-1" class="headerlink" title="TCP"></a>TCP</h2><p>​    TCP充分实现了数据传输时各种控制功能，可以进行丢包的重发控制，还可以对次序乱掉的分包进行顺序控制。而这些在UDP中都没有。此外，TCP作为一种面向有连接的协议，只有在确认通信对端存在时才会发送数据，从而可以控制通信流量的浪费。TCP通过检验和、序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输。</p>
<h1 id="TCP与UDP区别总结"><a href="#TCP与UDP区别总结" class="headerlink" title="TCP与UDP区别总结"></a>TCP与UDP区别总结</h1><p>1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接</p>
<p>2、TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保  证可靠交付</p>
<p>3、TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;UDP是面向报文的<br> UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）</p>
<p>4、每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信</p>
<p>5、TCP首部开销20字节;UDP的首部开销小，只有8个字节</p>
<p>6、TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP高效编程</title>
    <url>/computernetwork/TCP%E9%AB%98%E6%95%88%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="技巧1-理解基于连接和无连接协议之间的差异"><a href="#技巧1-理解基于连接和无连接协议之间的差异" class="headerlink" title="技巧1 理解基于连接和无连接协议之间的差异"></a>技巧1 理解基于连接和无连接协议之间的差异</h1><p>（1）对于无连接协议，每一个数据包和另外的数据包都是<font color="blue">独立地处理</font>，而对于面向连接的协议，状态信息是<font color="blue">被协议实现在</font><font color="red">连续的数据包</font>中维护的。</p>
<p>（2）对于一个无连接协议来说，每个数据包称作数据报，它都是独立地被应用程序发送出去，从协议这个意义上来说，每一个数据都是独立的实体，它和任何其它的在两个相同的对等方之间发送的数据报都无关。</p>
<p>（3）面向连接的协议却<font color="blue">在</font><font color="red">数据包之间</font><font color="blue">维持着状态信息</font>，应用程序使用这些状态信息来进行额外的会话，这些记住的状态信息<font color="blue">使协议能够提供可靠的递交</font>。</p>
<p>（4）无连接协议可以轻松支持<font color="blue">一对多和多对一</font>的通信，而面向连接的协议通常却需要为此建立独立的连接。</p>
<p>（5）TCP在基本的IP服务里增加了三个服务：</p>
<p>A）首先，它<font color="blue">为TCP段提供了</font><font color="red">校验位</font>，这就能保证到达目的地的数据不会在网络上传输时被破坏。</p>
<p>B）第二，它<font color="blue">为每个字节分配一个</font><font color="red">序列号</font>，如果数据不按顺序到达目的地，那么接收者也可以重新把它们组合。</p>
<p>C）TCP提供了一个<font color="blue">确认和重传的</font><font color="red">机制</font>来保证每一段最终都会被递交到目的地。</p>
<p>（6）UDP增加的服务：</p>
<p>A）UDP校验和。尽管IP也有校验和，但是它仅仅计算IP数据包的报头部分。</p>
<p>B）端口的概念。端口提供了一个<font color="blue">多路</font><font color="red">输出数据</font><font color="blue">到适当的</font><font color="red">应用程序</font><font color="blue">的方法</font>。应用程序可以调用bind来设置这个端口，它也可以让操作系统给它指定一个端口。当数据包到达时，<font color="blue">操作系统内核在</font><font color="red">套接字列表</font>中搜索一个<font color="blue">关联到数据包中</font><font color="red">协议、地址和端口的套接字</font>，如果匹配，数据就由指定的协议来处理，并使该套接字可以被任何打开了相应套接字的应用程序获得。</p>
<h1 id="技巧2-理解子网和CIDR"><a href="#技巧2-理解子网和CIDR" class="headerlink" title="技巧2 理解子网和CIDR"></a>技巧2 理解子网和CIDR</h1><p>（1）大型网络通过路由器分成相互连接的几个小段。</p>
<h2 id="子网划分"><a href="#子网划分" class="headerlink" title="子网划分"></a>子网划分</h2><p>（1）我们希望有一种解决方法，它包含<font color="blue">少量的</font><font color="red">路由表</font><font color="blue">和由</font><font color="red">单一网络ID</font><font color="blue">连接起来</font>的简易并有效的<font color="blue">IP地址空间</font>，通过为每一个段分配独立的网络ID来达到目的。我们希望对于外部主机来说这是一个单一的网络，但是对于内部主机来说有很多的网络。</p>
<p>（2）外部主机仅使用IP地址的网络ID来决定路由，内部主机则根据IP地址的{网络ID，子网ID}来决定路由。</p>
<p>（3）<font color="red">子网</font><font color="blue">是和</font><font color="red">接口</font>相关联的，因此它也是和路由表中的条目相关联的，这意味着<font color="blue">不同的子网具有不同的子网掩码</font>是可能的。</p>
<h2 id="CIDR"><a href="#CIDR" class="headerlink" title="CIDR"></a>CIDR</h2><p>（1）另一个分类地址的严重问题是B类网络ID的消耗，因此各个组织不得不获取C类网络ID块，但是这将再次导致要引入子网划分来解决的问题：Internet上路由表的增长。</p>
<p>（2）无分类域间路由通过<font color="red">“反向操作”子网划分</font>来解决这个问题。在子网划分中，IP网络地址的网络ID部分变长了，而CIDR使它变短。</p>
<p>（3）从长远来看，地址消耗和路由表大小问题都会被IPv6解决。IPv6有一个很大的地址空间以及显示的层次结构。</p>
<h1 id="技巧3-理解私有地址和NAT"><a href="#技巧3-理解私有地址和NAT" class="headerlink" title="技巧3 理解私有地址和NAT"></a>技巧3 理解私有地址和NAT</h1><h1 id="技巧4-开发和使用应用程序框架"><a href="#技巧4-开发和使用应用程序框架" class="headerlink" title="技巧4 开发和使用应用程序框架"></a>技巧4 开发和使用应用程序框架</h1><h1 id="技巧5-记住TCP只是一个流协议"><a href="#技巧5-记住TCP只是一个流协议" class="headerlink" title="技巧5 记住TCP只是一个流协议"></a>技巧5 记住TCP只是一个流协议</h1><p>（1）TCP是一个流协议，这意味着数据是作为字节流递交给接收者的，没有内在的“消息”或“消息边界”的概念。从这方面来考虑，读TCP数据就像从一个串行端口读数据一样，读数据时永远也不知道一个给定的读调用将会返回多少字节。</p>
<p>（2）虽然数据时在IP数据包中传输的，但是<font color="blue">一个<font color="red">数据包中的数据</font><font color="blue">跟</font><font color="red">调用send函数</font><font color="blue">传递多少数据给TCP没有直接的关系</font>。而且，接收数据应用程序也没有可靠的方法来决定数据是如何分组打包的，这是因为<font color="blue">几个包可能在</font><font color="red">recv调用之间</font>到达。</font></p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM原理以及垃圾回收简单介绍</title>
    <url>/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="JVM的作用-1"><a href="#JVM的作用-1" class="headerlink" title="JVM的作用[1]"></a>JVM的作用[1]</h1><p>Java 作为一门高级程序语言，它的语法非常复杂，抽象程度也很高。因此，直接在硬件上运行这种复杂的程序并不现实。所以呢，在运行 Java 程序之前，我们需要对其进行一番转换。</p>
<p>转换的过程为<strong>通过编译器[2]</strong>将 Java 程序转换成<font color="red"><strong>该虚拟机所能识别的指令序列，也称 Java 字节码。JVM会将字节码，即class文件加载到JVM中，由JVM进行解释和执行</strong></font>[3]。除了 Java 外，Scala、Clojure、Groovy，以及时下热门的 Kotlin，这些语言都可以运行在 Java 虚拟机之上。</p>
<blockquote>
<p>[1]所有Java语言既不是纯粹的编译型语言，也不是纯粹的解释型语言。Java程序的执行过程必须经过先编译，后解释两个步骤。</p>
<p>[2]由Java语言编写的程序需要进过编译步骤，但这个编译步骤并不会生成特定平台的机器码，而是生成一种与平台无关的字节码（*.class文件）。</p>
<p>[3]这种字节码不是可执行的，必须使用Java解释器来解释执行。</p>
</blockquote>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps1.png" alt="img"> </p>
<p>​                                                             图：JAVA转换字节码</p>
<p><strong>JVM是运行在操作系统之上的，它与硬件没有直接的交互。</strong></p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps2.jpg" alt="img"> </p>
<p>​                                                             图：JVM运行图</p>
<p>解释执行器: 将编译好的字节码文件转换成系统平台对应的机器码，<strong><font color="red">每解释一行，执行一行。[4]</font></strong>    </p>
<p>即时编译器: 将运行时的<font color="red">热点代码</font>即时转换成系统平台对应的机器码。</p>
<blockquote>
<p>[4]所以如果有一句话错了，JVM就不会执行下面代码了</p>
</blockquote>
<h1 id="常见的JVM"><a href="#常见的JVM" class="headerlink" title="常见的JVM"></a>常见的JVM</h1><h2 id="Sun-Classis-VM"><a href="#Sun-Classis-VM" class="headerlink" title="Sun Classis VM"></a>Sun Classis VM</h2><p>世界上第一款商用的虚拟机，已经完全淘汰。</p>
<h2 id="HotSpot-VM"><a href="#HotSpot-VM" class="headerlink" title="HotSpot VM"></a>HotSpot VM</h2><p> Sun JDK和OpenJDK中所带的虚拟机，也是目前使用范围最广的Java虚拟机。最初由一家名为“Longview Technologies”的小公司设计，后被Sun公司收购。</p>
<p>HotSpot VM，是<strong>Sun JDK和OpenJDK中所带的虚拟机</strong>，也是<strong>目前使用范围最广的Java虚拟机</strong>。最初它并非是为Java语言而开发的，它来源于Strongtalk VM，而这款虚拟机中相当多的技术又是来源于一款支持Self语言实现“达到C语言50%以上的执行效率”的目标而设计的虚拟机。Sun公司注意到了这款虚拟机在JIT编译上有许多优秀的理念和实际效果，在1997年收购了Longview Technologies公司，从而获得了HotSpot VM。HotSpot VM既继承了Sun之前两款商用虚拟机的优点（如前面提到的准确式内存管理），也有许多自己新的技术优势，如<strong>它名称中的HotSpot指的就是它的热点代码探测技术。这种技术可以通过执行计数器找出最具有编译价值的代码，然后通知JIT编译器以方法为单位进行编译。如果一个方法被频繁调用，或方法中有效循环次数很多，将会分别触发标准编译和OSR（栈上替换）编译动作。通过编译器与解释器恰当地协同工作，可以在最优化的程序响应时间与最佳执行性能中取得平衡，而且无须等待本地代码输出才能执行程序，即时编译的时间压力也相对减小，这样有助于引入更多的代码优化技术，输出质量更高的本地代码</strong>。</p>
<p>在2006年的JavaOne大会上，Sun公司宣布最终会把Java开源，并在随后的一年，陆续将JDK的各个部分（其中当然也包括了HotSpot VM）在GPL协议下公开了源码，并在此基础上建立了OpenJDK。这样，HotSpot VM便成为了Sun JDK和OpenJDK两个实现极度接近的JDK项目的共同虚拟机。</p>
<p>在2008年和2009年，Oracle公司分别收购了BEA公司和Sun公司，这样Oracle就同时拥有了两款优秀的Java虚拟机：JRockit VM和HotSpot VM。Oracle公司宣布在不久的将来（大约应在发布JDK 8的时候）会完成这两款虚拟机的整合工作，使之优势互补。整合的方式大致上是在HotSpot的基础上，移植JRockit的优秀特性，譬如使用JRockit的垃圾回收器与MissionControl服务，使用HotSpot的JIT编译器与混合的运行时系统。</p>
<h2 id="Jrockit"><a href="#Jrockit" class="headerlink" title="Jrockit"></a>Jrockit</h2><p> 由BEA公司开发的<strong>专注于服务器端应用的虚拟机</strong>。<strong>号称世界上最快的虚拟机</strong>。<strong>优势在于其垃圾收集器和MissionControl服务套件</strong>。BEA Jrockit Mission Control在2005年12月推出，它是一组以极低的开销来监控、管理和分析生产环境中的应用程序的工具。它包括三个独立的应用程序：内存泄露检测器（Memory Leak Detector），JVM运行时分析器（Runtime Analyzer）和管理控制台(Management Console)。</p>
<h2 id="J9"><a href="#J9" class="headerlink" title="J9"></a>J9</h2><p>J9由IBM公司开发，曾广泛应用于IBM公司系统内部及IBM小型机上。现已经捐献给Eclipse基金会。</p>
<h1 id="JVM体系概述"><a href="#JVM体系概述" class="headerlink" title="JVM体系概述"></a>JVM体系概述</h1><p>JVM虚拟机位于操作系统的堆中。</p>
<p><strong>Java 虚拟机将运行时内存区域划分为五个部分，分别为方法区、堆、程序计数器、Java 方法栈和本地方法栈。</strong></p>
<blockquote>
<h2 id="方法区：主要是存储类信息，常量池（static-常量和-static-变量），编译后的代码（字节码）等数据"><a href="#方法区：主要是存储类信息，常量池（static-常量和-static-变量），编译后的代码（字节码）等数据" class="headerlink" title="方法区：主要是存储类信息，常量池（static 常量和 static 变量），编译后的代码（字节码）等数据"></a>方法区：主要是存储类信息，常量池（static 常量和 static 变量），编译后的代码（字节码）等数据</h2><p>堆：初始化的对象，成员变量 （那种非 static 的变量），所有的对象实例和数组都要在堆上分配</p>
<p>栈：栈的结构是栈帧组成的，调用一个方法就压入一帧，帧上面存储局部变量表，操作数栈，方法出口等信息，局部变量表存放的是 8 大基础类型加上一个应用类型，所以还是一个指向地址的指针</p>
<p>本地方法栈：主要为 Native 方法服务</p>
<p>程序计数器：记录当前线程执行的行号</p>
</blockquote>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps3.jpg" alt="img"> </p>
<p>​                                                             图：JVM体系</p>
<p>执行Java代码首先需要使用类加载器将它编译而成的class文件加载到Java虚拟机中。加载后的Java类会被存放于方法区（Method Area）中。<font color="red">实际运行时，虚拟机会执行方法区内的代码。</font></p>
<p>在虚拟机中，<strong>方法区和堆为线程共享[5]</strong>，也是<font color="red"><strong>垃圾回收的重点照顾区域</strong>。</font><strong><u>栈空间为线程私有，基本不会出现垃圾回收。</u></strong></p>
<p>Java虚拟机将栈细分为面向Java方法的Java方法栈，面向本地方法（用C++写的native方法）的本地方法栈，以及存放各个线程执行位置的PC寄存器(程序计数器)。</p>
<p>在运行过程中，每当调用进入一个Java方法，<u>Java虚拟机会在当前线程的Java方法栈中生成一个栈帧(栈的一片区域)，用以存放局部变量以及字节码的操作数。</u>这个栈帧的大小是提前计算好的，而且Java虚拟机<strong>不要求栈帧在内存空间里连续分布</strong>。<u>当退出当前执行的方法时，不管是正常返回还是异常返回，Java虚拟机均会弹出当前线程的当前栈帧，并将之舍弃。</u></p>
<blockquote>
<p>[5]方法区、堆、执行引擎和本地库本地方法接口是线程共享的数据区.。虚拟机栈、本地方法栈和程序计数器是线程隔离的数据区</p>
</blockquote>
<h2 id="JVM运行过程"><a href="#JVM运行过程" class="headerlink" title="JVM运行过程"></a>JVM运行过程</h2><p>jvm虚拟机位于操作系统的堆中，并且，程序员写好的类加载到虚拟机执行的过程是：当一个classLoder启动的时候，classLoader的生存地点在jvm中的堆，然后它会去主机硬盘上将A.class装载到jvm的方法区，方法区中的这个字节文件会被虚拟机拿来new A字节码()，然后在堆内存生成了一个A字节码的对象，然后A字节码这个内存文件有两个引用一个指向A的class对象，一个指向加载自己的classLoader，</p>
<h2 id="类加载器ClassLoader"><a href="#类加载器ClassLoader" class="headerlink" title="类加载器ClassLoader"></a>类加载器ClassLoader</h2><h3 id="类加载器简介"><a href="#类加载器简介" class="headerlink" title="类加载器简介"></a>类加载器简介</h3><p>类加载器，即ClassLoader,它负责加载class文件，class文件在文件开头有特定的文件标示，并且<strong>ClassLoader只负责class文件的加载</strong>，至于它是否可以<strong>运行，则由Execution Engine（执行引擎）决定</strong>。</p>
<h3 id="类加载器分类"><a href="#类加载器分类" class="headerlink" title="类加载器分类"></a>类加载器分类</h3><h4 id="虚拟机自带的类加载器"><a href="#虚拟机自带的类加载器" class="headerlink" title="虚拟机自带的类加载器"></a>虚拟机自带的类加载器</h4><p>l启动类加载器（Bootstrap）：主要<font color="red">负责加载jre中的最为基础、最为重要的类</font>。如$JAVA_HOME/jre/lib/rt.jar等，以及由虚拟机参数 -Xbootclasspath 指定的类。由于它由C++代码实现，没有对应的java对象，因此在java中，尝试获取此类时，只能使用null来指代。</p>
<p>l扩展类加载器（Extension）：由Java代码实现，用于<font color="red">加载相对次要、但又通用的类</font>，比如存放在 JRE 的 lib/ext 目录下 jar 包中的类，以及由系统变量 java.ext.dirs 指定的类。如$JAVA_HOME/jre/lib/ext/*.jar。</p>
<p>应用程序类加载器（AppClassLoader）：由Java代码实现， 它负责<font color="red">加载应用程序路径下的类</font>。（这里的应用程序路径，便是指虚拟机参数 -cp/-classpath、系统变量 java.class.path 或环境变量 CLASSPATH 所指定的路径。）默认情况下，应用程序中包含的类便是由应用类加载器加载的。</p>
<h4 id="用户自定义的加载器"><a href="#用户自定义的加载器" class="headerlink" title="用户自定义的加载器"></a>用户自定义的加载器</h4><p>Java.lang.ClassLoader的子类，用户可以定制类的加载方式。例如可以对 class 文件进行加密，加载时再利用自定义的类加载器对其解密。</p>
<blockquote>
<p>注意：除了BootStrap Class Loader，其他的类加载器，都是Java.lang.ClassLoader的子类。其他的类加载器都由加载sum.misc.Launcher类后得到。</p>
</blockquote>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps4.jpg" alt="img"> </p>
<p>​                                                             图：类加载器</p>
<h1 id="双亲委派机制"><a href="#双亲委派机制" class="headerlink" title="双亲委派机制"></a>双亲委派机制</h1><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>每当一个类加载器接收到加载请求时，它会先将请求转发给父类加载器。<u>在父类加载器没有找到所请求的类的情况下，该类加载器才会尝试去加载。</u></p>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><p>①采用双亲委派模式的是好处是<strong>Java类随着它的类加载器一起具备了一种带有优先级的层次关系</strong>[6]，通过这种层级关可以<strong><font color="red">避免类的重复加载</font></strong>，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。</p>
<p>②其次是考虑到安全因素，防止java核心api中定义类型不会被用户恶意替换和篡改，从而引发错误。</p>
<p>恶意篡改代码示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> java.lang;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">String</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">String</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		System.out.println(<span class="string">"自己伪造的String"</span>);</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>[6]由这句话可是JAVA中类的层级关系是由于各类的ClassLoader中的双亲委派机制导致的</p>
</blockquote>
<h1 id="JVM内存模型"><a href="#JVM内存模型" class="headerlink" title="JVM内存模型"></a>JVM内存模型</h1><p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps5.jpg" alt="img"> </p>
<p>​                                                             图：JVM内存模型图</p>
<h3 id="执行引擎Execution-Engine"><a href="#执行引擎Execution-Engine" class="headerlink" title="执行引擎Execution Engine"></a>执行引擎Execution Engine</h3><p>Execution Engine执行引擎负责解释命令，提交操作系统执行。</p>
<p>在 HotSpot 里面，将字节码翻译为机器码的翻译过程有两种形式：</p>
<p>第一种是<font color="purple">解释执行</font>，即逐条将字节码翻译成机器码并执行。<font color="red">其优势在于无需等待编译</font>。</p>
<p>第二种是<font color="purple">即时编译（Just-In-Time compilation，JIT）</font>，即将一个方法中包含的所有字节码编译成机器码后再执行。<font color="red">其优势在于实际运行速度更快。</font></p>
<p>HotSpot 默认采用混合模式，综合了解释执行和即时编译两者的优点。<font color="red">它会先</font><font color="purple">解释执行</font><font color="red">字节码，而后将其中反复执行的<strong>热点代码</strong>，以<u>方法为单位</u>进行</font><font color="purple">即时编译。</font></p>
<h3 id="本地方法接口和本地方法栈Native-Interface-amp-Native-Method-Stack"><a href="#本地方法接口和本地方法栈Native-Interface-amp-Native-Method-Stack" class="headerlink" title="本地方法接口和本地方法栈Native Interface&amp;Native Method Stack"></a>本地方法接口和本地方法栈Native Interface&amp;Native Method Stack</h3><p>在每个操作系统内部，都定义了很多本地方法库，例如windows中以.dll文件为主，Linux总以.so文件为主。</p>
<p>这些本地方法库中，定义了很多调用本地操作系统的方法，也称之为本地方法接口(Native Interface)。</p>
<p>本地方法接口的作用是融合不同的编程语言为 Java 所用，它的初衷是融合 C/C++程序，Java 诞生的时候是 C/C++横行的时候，要想立足，必须要调用 C/C++程序，于是就在<font color="red">内存中专门开辟了一块区域处理标记为native的代码，它的具体做法是 Native Method Stack中登记 native方法，在Execution Engine 执行时加载native libraie（本地方法库）。</font></p>
<p>目前该方法使用的越来越少了，除非是与硬件有关的应用，比如通过Java程序驱动打印机或者Java系统管理生产设备，或者是使用Java语言开发安卓操作系统的硬件驱动等。</p>
<p>例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Object</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">registerNatives</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">static</span> &#123;</span><br><span class="line"></span><br><span class="line">    registerNatives();</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">native</span> Class&lt;?&gt; getClass();</span><br></pre></td></tr></table></figure>

<p>​    </p>
<h3 id="PC寄存器（程序计数器）Program-Counter-Register"><a href="#PC寄存器（程序计数器）Program-Counter-Register" class="headerlink" title="PC寄存器（程序计数器）Program Counter Register"></a>PC寄存器（程序计数器）Program Counter Register</h3><p><font color="red">每个线程都有一个程序计数器，是线程私有的</font>，就是一个指针，指向方法区中的方法字节码（用来存储指向下一条指令的地址,就是<font color="red">即将要执行的指令代码</font>），由执行引擎读取下一条指令，是一个非常小的内存空间，几乎可以忽略不记。</p>
<p><font color="red">PC寄存器主要负责计数和调度。它可以看作是当前线程所执行的字节码的行号指示器[7]。</font>由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，一个处理器都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都有一个独立的程序计数器，各个线程之间计数器互不影响，独立存储。<font color="red">程序计数器内存区域是虚拟机中唯一没有规定OutOfMemoryError情况的区域。</font></p>
<blockquote>
<p>[7]用来记录当前线程运行到哪里了，以免被在该线程被其他线程抢占资源的时候忘了自己运行到哪里了，还要重新走（就像微机原理中在中断的时候记录中断位置一样，只不过那个是有中断申请才使用，这个是一直都有，然后一直在默默记录地址，以免突然中断）</p>
</blockquote>
<h3 id="方法区Method-Area"><a href="#方法区Method-Area" class="headerlink" title="方法区Method Area"></a>方法区Method Area</h3><p>方法区是<strong>被所有线程共享</strong>。所有字段和方法字节码，以及<strong>一些特殊方法如构造函数，接口代码也在此定义。</strong>简单说，<strong>所有定义的方法的信息都保存在该区域，此区属于共享区间。</strong>静态变量+常量+类信息(构造方法/接口定义)+运行时常量池存在方法区中。</p>
<blockquote>
<p>注：实例变量存在堆内存中,和方法区无关。</p>
</blockquote>
<p>方法区是《Java虚拟机规范》中规定的一个概念，在JDK1.7之前，HotSpot使用永久区实现方法区。1.8之后，由元空间实现。</p>
<h1 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h1><p>栈也叫栈内存，主管Java程序的运行，是<u>在线程创建时创建</u>。栈是由数组或者链表实现。</p>
<p>它的<strong>生命周期是跟随线程的生命期</strong>，线程结束栈内存也就释放，对于<strong>栈来说不存在垃圾回收问题</strong>，只要线程一结束该栈就结束，生命周期和线程一致，是线程私有的。</p>
<p>一个线程中的每个方法在执行的同时都会创建一个栈帧（Stack Frame），用于存储局部变量表、操作数栈、动态链接、方法出口等信息。</p>
<p><strong>8种基本类型的变量+对象的引用变量+实例方法都是在函数的栈内存中分配。</strong></p>
<h3 id="栈区域两种异常状态"><a href="#栈区域两种异常状态" class="headerlink" title="栈区域两种异常状态"></a>栈区域两种异常状态</h3><p>如果线程请求的栈深度大于虚拟机所允许的深度，则抛出StackOverflowError异常</p>
<p>如果虚拟机栈可以动态扩展，在扩展是无法申请到足够的内存，就会抛出OutOfMemoryError异常。</p>
<h3 id="栈帧"><a href="#栈帧" class="headerlink" title="栈帧"></a>栈帧</h3><p>一个线程的每个方法在调用时都会在栈上划分一块区域，用于存储方法所需要的变量等信息，这块区域称之为栈帧（stack frame）。<strong>栈由多个栈帧构成。</strong></p>
<p>栈帧中主要保存3 类数据：</p>
<p>本地变量（Local Variables）:输入参数和输出参数以及方法内的变量</p>
<p>栈操作（Operand Stack）:记录出栈、入栈的操作</p>
<p>栈帧数据（Frame Data）:包括类文件、方法等等。</p>
<h3 id="栈运行原理"><a href="#栈运行原理" class="headerlink" title="栈运行原理"></a>栈运行原理</h3><p>栈中的数据都是以栈帧（Stack Frame）为载体存在。在栈中，方法的调用顺序遵循“先进后出”/“后进先出”原则。</p>
<p>当一个方法A被调用时就产生了一个栈帧 F1，并被压入到栈中，A方法又调用了 B方法，于是产生栈帧 F2 也被压入栈，B方法又调用了 C方法，于是产生栈帧 F3 也被压入栈，……</p>
<p>执行完毕后，先弹出F3栈帧，再弹出F2栈帧，再弹出F1栈帧……</p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps6.jpg" alt="img"> </p>
<p>​                                                             图：栈出入示意图</p>
<p>图示在一个栈中有两个栈帧：</p>
<p>栈帧 2是最先被调用的方法，先入栈，然后方法 2 又调用了方法1，栈帧 1处于栈顶的位置，栈帧 2 处于栈底，执行完毕后，依次弹出栈帧 1和栈帧 2，线程结束，栈释放。</p>
<p>每执行一个方法都会产生一个栈帧，保存到栈(后进先出)的顶部，顶部栈就是当前的方法，该方法执行完毕后会自动将此栈帧出栈。</p>
<h1 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h1><h3 id="逻辑设计"><a href="#逻辑设计" class="headerlink" title="逻辑设计"></a>逻辑设计</h3><p>堆是java虚拟机所管理的内存中<strong>最大</strong>的一块，是<strong>被所有线程共享的一块内存区域，在虚拟机启动时创建。</strong></p>
<p><strong>堆内存的大小是可以调节的，通过 -Xmx 和 -Xms 控制。</strong></p>
<p>VM初始分配的堆内存由-XMS指定，默认是物理内存的1/64；JVM最大分配的堆内存是有-XMX指定，默认是物理内存的1/4。当堆内存过大或者过小，都会自动按照对法分配内存做改变，所以在服务器端最好会将-Xms和-Xmx调到一样大，以免堆内存经常改变空间。</p>
<p>所有的对象实例以及数组都要在堆上分配。</p>
<p>如果堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出O<strong>utOfMemoryError</strong>异常。（OOM）</p>
<p>java堆是垃圾收集器管理的主要区域，因此也被成为“GC堆”（Garbage Collected Heap）。</p>
<p>堆内存逻辑上分为三部分：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>中文名</th>
<th>简称</th>
</tr>
</thead>
<tbody><tr>
<td>Young Generation Space</td>
<td>新生区(新生代)</td>
<td>Young/New</td>
</tr>
<tr>
<td>Tenure generation space</td>
<td>养老区(养老代)</td>
<td>Old/ Tenure</td>
</tr>
<tr>
<td>Permanent Space</td>
<td>永久区(永久代)（1.8后为元空间）</td>
<td>Perm</td>
</tr>
</tbody></table>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps7.jpg" alt="img"> </p>
<p>​                                                             图：逻辑划分堆</p>
<h3 id="物理设计"><a href="#物理设计" class="headerlink" title="物理设计"></a>物理设计</h3><p>在 Java 中，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old )。</p>
<h4 id="新生区"><a href="#新生区" class="headerlink" title="新生区"></a>新生区</h4><p>其中新生区默认状态下占整个堆的1/3（轻度GC）在理论上大部分伊甸区的对象都抗不过伊甸区的垃圾回收，剩下的会进入from区或者to区。</p>
<p>新生代 ( Young ) 又被划分为三个区域：Eden（伊甸区）、From Survivor（From区）、To Survivor（To区）[8]。From区和To区是两块大小相等并且可以互换角色的空间。三个区比例为8:1:1。</p>
<p>这样划分的<font color="red">目的是为了使 JVM 能够更好的管理堆内存中的对象，包括内存的分配以及回收。</font></p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps8.jpg" alt="img"> </p>
<p>​                                                             图：物理设计</p>
<p>绝大多数情况下，对象首先分配在eden区，在新生代回收后，如果对象还存活，则进入s0或s1区，之后每经过一次新生代回收，如果对象存活则它的年龄就加1，对象达到一定的年龄（默认15）后，则进入老年代[9]。</p>
<blockquote>
<p>[8]从伊甸区活下来的对象会去to区</p>
<p>[9]当每次垃圾回收之后，幸存下来的都去to区，to区存放幸存的所有对象之后变为from区。from区的对象和伊甸区新生的对象在经过垃圾回收之后将对象放到to区，to区继续变成from区，to区一直是空着的</p>
</blockquote>
<h4 id="老年代"><a href="#老年代" class="headerlink" title="老年代"></a>老年代</h4><p>老年代中的对象是一直再被使用没有被回收的对象。</p>
<p>在默认状态下占全栈比例的2/3（重度GC），因为在理论上大部分新生区的对象都抗不过新生区的垃圾回收（15次）。而老年代的对象其实是很活跃的热点代码，经常被调用才会很多次都没有被GC。但是相对而言老年代中的代码更容易被回收。</p>
<h4 id="永久区"><a href="#永久区" class="headerlink" title="永久区"></a>永久区</h4><p>永久存储区是一个常驻内存区域，用于存放JDK自身所携带的 Class,Interface 的元数据，也就是说它存储的是运行环境必须的类信息，被装载进此区域的数据是不会轻易被垃圾回收器回收掉的，关闭 JVM 才会释放此区域所占用的内存。</p>
<p>如果出现java.lang.OutOfMemoryError: PermGen space，说明是Java虚拟机对永久代Perm内存设置不够。一般出现这种情况，都是程序启动需要加载大量的第三方jar包。例如：在一个Tomcat下部署了太多的应用。或者大量动态反射生成的类不断被加载，最终导致Perm区被占满。 </p>
<p>Jdk1.6及之前：有永久代, 常量池1.6在方法区</p>
<p>Jdk1.7：有永久代，但已经逐步“去永久代”，常量池1.7在堆</p>
<p>Jdk1.8及之后：无永久代，采用了 Metaspace。常量池1.8在元空间</p>
<p>实际而言，方法区（Method Area）和堆一样，是各个线程共享的内存区域，<font color="red">它用于存储虚拟机加载的：类信息+普通常量+静态常量+编译器编译后的代码等等</font>，虽然JVM规范将方法区描述为堆的一个逻辑部分，但<font color="red">它却还有一个别名叫做Non-Heap(非堆)，目的就是要和堆分开。</font></p>
<p>对于HotSpot虚拟机，很多开发者习惯将方法区称之为“永久代(Parmanent Gen)” ，但<font color="red">严格本质上说两者不同</font>，或者说<font color="red">使用永久代来实现方法区而已，永久代是方法区(相当于是一个接口interface)的一个实现，jdk1.7的版本中，已经将原本放在永久代的字符串常量池移走。</font></p>
<p>永久存储区在物理上并不属于堆中（逻辑中在）。</p>
<p>永久区的东西一般不会有垃圾回收的。</p>
<h1 id="JVM参数设置"><a href="#JVM参数设置" class="headerlink" title="JVM参数设置"></a>JVM参数设置</h1><h2 id="常见参数设置"><a href="#常见参数设置" class="headerlink" title="常见参数设置"></a>常见参数设置</h2><p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps9.jpg" alt="img"> </p>
<p>​                                                             图：堆结构</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>-XX:+PrintGC</td>
<td>每次触发GC的时候打印相关日志</td>
</tr>
<tr>
<td>-XX:+UseSerialGC</td>
<td>串行回收</td>
</tr>
<tr>
<td>-XX:+PrintGCDetails</td>
<td>更详细的GC日志</td>
</tr>
<tr>
<td>-Xms</td>
<td>堆初始值(默认为物理内存的1/64)</td>
</tr>
<tr>
<td>-Xmx</td>
<td>堆最大可用值(默认为物理内存的1/4)</td>
</tr>
<tr>
<td>-Xmn</td>
<td>新生代堆初始值</td>
</tr>
<tr>
<td>-XX:SurvivorRatio</td>
<td>用来设置新生代中eden空间和from/to空间的比例，默认为8</td>
</tr>
<tr>
<td>-XX:NewRatio</td>
<td>配置新生代与老年代占比，默认1:2</td>
</tr>
<tr>
<td>-Xss</td>
<td>每个线程的堆栈大小，默认为1M，此值不能设置过大，否则会减少线程并发数。</td>
</tr>
</tbody></table>
<p>VM初始分配的堆内存由-Xms指定，默认是物理内存的1/64；JVM最大分配的堆内存由-Xmx指定，默认是物理内存的1/4。默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；</p>
<p>空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。因此服务器一般设置-Xms、-Xmx 相等以避免在每次GC 后调整堆的大小。</p>
<p>查看当前JVM代码如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		System.out.print(<span class="string">"最大内存"</span>);</span><br><span class="line"></span><br><span class="line">	System.out.println(Runtime.getRuntime().maxMemory() / <span class="number">1024.0</span> / <span class="number">1024</span> + <span class="string">"M"</span>);</span><br><span class="line"></span><br><span class="line">		System.out.print(<span class="string">"当前可用内存"</span>);</span><br><span class="line"></span><br><span class="line">	System.out.println(Runtime.getRuntime().freeMemory() / <span class="number">1024.0</span> / <span class="number">1024</span> + <span class="string">"M"</span>);</span><br><span class="line"></span><br><span class="line">		System.out.print(<span class="string">"当前申请内存"</span>);</span><br><span class="line"></span><br><span class="line">	System.out.println(Runtime.getRuntime().totalMemory() / <span class="number">1024.0</span> / <span class="number">1024</span> + <span class="string">"M"</span>);</span><br><span class="line"></span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>设置示例：</p>
<p>-Xms20m：堆内存初始化值20m</p>
<p>-Xmx20m：堆内存最大值20m </p>
<p>-Xmn1m -XX：新生代最大值可用1m</p>
<p>SurvivorRatio=2：eden空间和from/to空间的比例为2/1</p>
<p>-XX:+PrintGCDetails：打印详细的GC信息</p>
<p>-XX:+UseSerialGC：使用串行GC回收器</p>
<h2 id="常见异常"><a href="#常见异常" class="headerlink" title="常见异常"></a>常见异常</h2><h3 id="OutOfMemoryError"><a href="#OutOfMemoryError" class="headerlink" title="OutOfMemoryError"></a>OutOfMemoryError</h3><p>错误原因: java.lang.OutOfMemoryError: Java heap space 堆内存溢出</p>
<p>解决办法:调大堆内存大小 </p>
<p>示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// -Xms1m -Xmx10m -XX:+PrintGCDetails 	</span></span><br><span class="line">List&lt;Object&gt; listObject = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">			System.out.println(<span class="string">"i:"</span> + i);</span><br><span class="line"></span><br><span class="line">			Byte[] bytes = <span class="keyword">new</span> Byte[<span class="number">1</span> * <span class="number">1024</span> * <span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line">			listObject.add(bytes);</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"添加成功..."</span>);</span><br></pre></td></tr></table></figure>

<h3 id="StackOverflowError"><a href="#StackOverflowError" class="headerlink" title="StackOverflowError"></a>StackOverflowError</h3><p>错误原因: java.lang.StackOverflowError表示为栈内存溢出，一般产生于递归调用。</p>
<p>解决办法:设置线程最大调用深度，默认是1M（-Xss5m 设置最大调用深度</p>
<p>）</p>
<p>示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StackTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">count</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">			count++;</span><br><span class="line"></span><br><span class="line">			count(); </span><br><span class="line"></span><br><span class="line">		&#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line"></span><br><span class="line">			System.out.println(<span class="string">"最大深度:"</span>+count);</span><br><span class="line"></span><br><span class="line">			e.printStackTrace();</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">			 count();</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="GC垃圾回收机制"><a href="#GC垃圾回收机制" class="headerlink" title="GC垃圾回收机制"></a>GC垃圾回收机制</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>JVM中的Garbage Collection，简称GC，<em>它会不定时去堆内存中清理不可达对象。</em></p>
<p>如果一个对象变成了不可达对象，这并不代表它会立刻被回收。在GC时，不可达的对象并不会立刻回收。</p>
<p>垃圾回收在一个Java程序中的执行是自动执行的，不受外界干预的。程序员唯一能做的就是通过调用System.gc() 方法来”建议”执行垃圾收集器，但其是否可以执行，什么时候执行却都是不可知的。这也是垃圾收集器的最主要的缺点。当然相对于它给程序员带来的巨大方便性而言，这个缺点是瑕不掩瑜的。</p>
<h2 id="finalize"><a href="#finalize" class="headerlink" title="finalize()"></a>finalize()</h2><p>在垃圾收集器将对象从内存中清除出去前，Java技术使用finalize()方法做必要的<strong>清理</strong>工作。</p>
<p>这个方法是由垃圾收集器在确定这个对象没有被引用时对这个对象调用的。它是在Object类中定义的，因此所有的类都继承了它。子类覆盖finalize()方法以整理系统资源或者执行其他清理工作。</p>
<h2 id="GC的工作特点"><a href="#GC的工作特点" class="headerlink" title="GC的工作特点"></a>GC的工作特点</h2><p>在GC工作中，通过某种算法来对JVM中的内存区域进行检测，对检测到的不可达对象，进行垃圾回收。</p>
<p><font color="red">理论上GC过程中会频繁收集Young区，很少收集Old区，基本不动Perm区（元空间/方法区）。</font></p>
<h2 id="GC的分类"><a href="#GC的分类" class="headerlink" title="GC的分类"></a>GC的分类</h2><p>JVM在进行GC时，并非每次都对上面三个内存区域一起回收的，大部分时候回收的都是指新生代。</p>
<p><font color="red">因此GC按照回收的区域又分了两种类型：普通GC（minor GC）和全局GC（major GC or Full GC）。</font></p>
<p>新生代GC（minor GC）：只针对新生代区域的GC。</p>
<p>老年代GC（major GC or Full GC）：针对老年代的GC，偶尔伴随对新生代的GC以及对永久代的GC。 一般情况下，当出现了 Major GC，经常会伴随至少一次的 Minor GC（但非绝对的，在 ParallelScavenge 收集器的收集策略里就有直接进行 Major GC 的策略选择过程）。</p>
<p>MajorGC 的速度一般会比 Minor GC 慢 10倍以上。</p>
<p>Minor GC触发机制：当年轻代满时就会触发Minor GC，这里的年轻代满指的是Eden区满，Survivor满不会引发GC。</p>
<p>Full GC触发机制：当年老代满时会引发Full GC，Full GC将会同时回收年轻代、年老代，当永久代满时也会引发Full GC，会导致Class、Method元信息的卸载。</p>
<h3 id="标记不可达对象"><a href="#标记不可达对象" class="headerlink" title="标记不可达对象"></a>标记不可达对象</h3><h4 id="引用计数法"><a href="#引用计数法" class="headerlink" title="引用计数法"></a>引用计数法</h4><p>引用计数算法：给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值加１；当引用失效时，计数器值减１.任何时刻计数器值为0的对象就是不可能再被使用的。</p>
<p>就是如果一个对象没有被任何引用指向，则可视之为垃圾。<strong><font color="red">这种方法的缺点就是不能检测到循环指向的存在。</font></strong></p>
<p>首先需要声明，至少主流的Java虚拟机里面都没有选用引用计数算法来管理内存。主流的java虚拟机中没有使用引用计数法的最主要的原因是<font color="red"><strong>它很难解决对象之间相互循环引用的问题</strong></font>。</p>
<p>例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyObject</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> Object ref;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> String name;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		MyObject myObject1 = <span class="keyword">new</span> MyObject();</span><br><span class="line"></span><br><span class="line">		MyObject myObject2 = <span class="keyword">new</span> MyObject();</span><br><span class="line"></span><br><span class="line">		myObject1.ref=myObject2;</span><br><span class="line"></span><br><span class="line">		myObject2.ref=myObject1;</span><br><span class="line"></span><br><span class="line">		myObject1=<span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">		myObject2=<span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>将myObject1和myObject2赋值为null后，虚拟机依然无法回收，因为他们还相互指向和依赖。</p>
<h4 id="GC-ROOTS算法"><a href="#GC-ROOTS算法" class="headerlink" title="GC ROOTS算法"></a>GC ROOTS算法</h4><p>根搜索算法的基本思路就是通过一系列名为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，<strong>搜索所走过的路径称为引用链(Reference Chain)。</strong>当一个对象到GC Roots<strong>没有任何引用链相连</strong>时，则证明此对象是不可用的。<strong><u>真正标记对象为可回收状态至少要标记两次。</u></strong></p>
<p>简单理解，可以理解为<u>堆外指向堆内</u>的引用。</p>
<p>以下对象可以选取GC ROOTS节点：</p>
<p>(1). 虚拟机栈（栈帧中的局部变量区，也叫做局部变量表）中引用的对象。</p>
<p>(2). 方法区中的类静态属性引用的对象。</p>
<p>(3). 方法区中常量引用的对象。</p>
<p>(4). 本地方法栈中JNI(Native方法)引用的对象。</p>
<p>示例：</p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps10.jpg" alt="img"> </p>
<p>​                                                             图：引用链与对象</p>
<h3 id="垃圾回收的三种方式"><a href="#垃圾回收的三种方式" class="headerlink" title="垃圾回收的三种方式"></a>垃圾回收的三种方式</h3><p>当标记完所有的存活对象时，我们便可以进行死亡对象的回收工作了。主流的基础回收方式可分为三种。</p>
<h4 id="清除"><a href="#清除" class="headerlink" title="清除"></a>清除</h4><p>第一种是清除（sweep），即把死亡对象所占据的内存标记为空闲内存，并记录在一个空闲列表（free list）之中。当需要新建对象时，内存管理模块便会从该空闲列表中寻找空闲内存，并划分给新建的对象。</p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps11.jpg" alt="img"> </p>
<p>​                                                             图：清除</p>
<p>清除这种回收方式的原理及其简单，但是有两个缺点。</p>
<p>一是<font color="red">会造成内存碎片。</font>由于 Java 虚拟机的堆中对象必须是连续分布的，因此可能出现总空闲内存足够，但是无法分配的极端情况。</p>
<p>二是<font color="red">分配效率较低。</font>如果是一块连续的内存空间，那么我们可以通过指针加法（pointer bumping）来做分配。而对于空闲列表，Java 虚拟机则需要逐个访问列表中的项，来查找能够放入新建对象的空闲内存。</p>
<h4 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h4><p>第二种是压缩（compact），<strong>即把存活的对象聚集到内存区域的起始位置，从而留下一段连续的内存空间。</strong>这种做法能够解决内存碎片化的问题，但代价是压缩算法的性能开销。</p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps12.jpg" alt="img"> </p>
<p>​                                                             图：清除</p>
<h4 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h4><p>第三种则是复制（copy），即把内存区域分为两等分，分别用两个指针 <strong>from 和 to</strong> 来维护，并且<strong><u>只是用 from 指针指向的内存区域</u></strong>来分配内存。当发生垃圾回收时，便把存活的对象复制到 to 指针指向的内存区域中，并且交换 from 指针和 to 指针的内容。复制这种回收方式同样能够解决内存碎片化的问题，但是它的缺点也极其明显，即堆空间的使用效率极其低下。</p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps13.jpg" alt="img"> </p>
<p>​                                                             图：复制</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>回收死亡对象的内存共有三种方式，分别为：会造成内存碎片的清除、性能开销较大的压缩、以及堆使用效率较低的复制。当然，现代的垃圾回收器往往会综合上述几种回收方式，综合它们优点的同时规避它们的缺点。</p>
<h3 id="垃圾回收算法"><a href="#垃圾回收算法" class="headerlink" title="垃圾回收算法"></a>垃圾回收算法</h3><h4 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h4><p>在Java的大部分应用场景下，对象的存活复合这样一个规律，即大部分的 Java 对象只存活一小段时间，而存活下来的小部分 Java 对象则会存活很长一段时间。</p>
<p>基于这样一个规律，在JVM中，使用<font color="red">分代回收思想</font>来回收垃圾。简单来说，就是将堆空间划分为两代，分别叫做<font color="red">新生代和老年代。</font>新生代用来存储新建的对象。当对象存活时间够长时，则将其移动到老年代。</p>
<p>Java 虚拟机可以给不同代使用不同的回收算法。对于新生代，我们猜测大部分的 Java 对象只存活一小段时间，那么便可以频繁地采用耗时较短的垃圾回收算法，让大部分的垃圾都能够在新生代被回收掉。</p>
<p>对于老年代，由于大部分的垃圾已经在新生代中被回收了，而在老年代中的对象有大概率会继续存活。<font color="red">一般在堆空间即将或者已经耗尽时，才会触发触发针对老年代的回收。</font>这时候，Java 虚拟机往往需要做一次全堆扫描，耗时也将不计成本。（当然，现代的垃圾回收器都在并发收集的道路上发展，来避免这种全堆扫描的情况。）</p>
<h4 id="堆的划分"><a href="#堆的划分" class="headerlink" title="堆的划分"></a>堆的划分</h4><p>Java 虚拟机将堆划分为新生代和老年代。其中，新生代又被划分为 Eden 区，以及两个大小相同的 Survivor 区。</p>
<p>默认情况下，Java 虚拟机采取的是一种动态分配的策略（对应 Java 虚拟机参数 -XX:+UsePSAdaptiveSurvivorSizePolicy），根据生成对象的速率，以及 Survivor 区的使用情况<strong>动态调整 Eden 区和 Survivor 区的比例。</strong></p>
<p>当然，你也可以通过参数 -XX:SurvivorRatio 来固定这个比例。但是需要注意的是，其中一个 Survivor 区会一直为空，<font color="red">因此比例越低浪费的堆空间将越高。</font></p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps14.jpg" alt="img"> </p>
<p>​                                                             图：堆内区域划分</p>
<h4 id="新生代算法：标记复制-Mark-Copying-算法"><a href="#新生代算法：标记复制-Mark-Copying-算法" class="headerlink" title="新生代算法：标记复制(Mark-Copying)算法"></a>新生代算法：标记复制(Mark-Copying)算法</h4><p>当我们调用 new 指令时，它会在 Eden 区中划出一块作为存储对象的内存。当 Eden 区的空间耗尽了怎么办？这个时候 Java 虚拟机便会触发一次 Minor GC，来收集新生代的垃圾。存活下来的对象，则会被送到 Survivor 区。</p>
<p>新生代共有两个 Survivor 区，我们分别用 from 和 to 来指代。其中 to 指向的 Survivior 区是空的。当发生 Minor GC 时，Eden 区和 from 指向的 Survivor 区中的存活对象会被复制到 to 指向的 Survivor 区中，然后交换 from 和 to 指针，以保证下一次 Minor GC 时，to 指向的 Survivor 区还是空的。</p>
<p>Java 虚拟机会记录 Survivor 区中的对象一共被来回复制了几次。<font color="red">如果一个对象被复制的次数为 15（对应虚拟机参数 -XX:+MaxTenuringThreshold），那么该对象将被晋升（promote）至老年代。另外，如果单个 Survivor 区已经被占用了 50%（对应虚拟机参数 -XX:TargetSurvivorRatio），那么较高复制次数的对象也会被晋升至老年代。</font></p>
<p><u>万一存活对象数量比较多，那么To域的内存可能不够存放，这个时候会借助老年代的空间。</u></p>
<p>因此Minor GC使用的则是标记-复制算法。将 Survivor 区中的老存活对象晋升到老年代，然后将剩下的存活对象和 Eden 区的存活对象复制到另一个 Survivor 区中。理想情况下，Eden 区中的对象基本都死亡了，那么需要复制的数据将非常少，因此采用这种标记 - 复制算法的效果极好。</p>
<p>口诀：复制必交换，谁空谁为to</p>
<p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps15.jpg" alt="img"> </p>
<p>​                                                             图：from和to区的交换</p>
<h4 id="老年代算法：标记清除-Mark-Sweep-算法"><a href="#老年代算法：标记清除-Mark-Sweep-算法" class="headerlink" title="老年代算法：标记清除(Mark-Sweep)算法"></a>老年代算法：标记清除(Mark-Sweep)算法</h4><p>老年代一般是由标记清除或者是标记清除与标记整理的<font color="red">混合实现。</font></p>
<p>标记清除算法一般应用于老年代,因为老年代的对象生命周期比较长。该算法先对所有可访问的对象，做个标记再遍历堆，把未被标记的对象回收（标记活的）。</p>
<p>缺点：</p>
<p>①回收时，<font color="red">应用需要挂起</font>，也就是stop the world，导致用户体验非常差劲</p>
<p>②由于需要<font color="red">遍历全堆对象，效率比较低（递归与全堆对象遍历）。</font></p>
<p>③造成内存碎片化</p>
<h4 id="标记压缩-Mark–Compact-算法"><a href="#标记压缩-Mark–Compact-算法" class="headerlink" title="标记压缩(Mark–Compact)算法"></a>标记压缩(Mark–Compact)算法</h4><p><img src="/JAVA/JVM%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/wps16.jpg" alt="img"> </p>
<p>​                                                             图：标记压缩算法</p>
<p>标记清除算法和标记压缩算法非常相同，但是标记压缩算法在标记清除算法之上解决内存碎片化。</p>
<p>优点：解决内存碎片化问题。也消除了复制算法当中，内存减半的高额代价</p>
<p>缺点：<strong>效率低</strong>，压缩阶段，由于移动了可用对象，需要去更新引用。</p>
<h4 id="标记清除压缩-Mark-Sweep-Compact-算法"><a href="#标记清除压缩-Mark-Sweep-Compact-算法" class="headerlink" title="标记清除压缩(Mark-Sweep-Compact)算法"></a>标记清除压缩(Mark-Sweep-Compact)算法</h4><p>标记清除压缩(Mark-Sweep-Compact)算法是标记清除算法和标记压缩算法的结合算法。其原理和标记清除算法一致，只不过会在多次GC后，进行一次Compact操作！</p>
<h3 id="垃圾回收器"><a href="#垃圾回收器" class="headerlink" title="垃圾回收器"></a>垃圾回收器</h3><h4 id="串行回收和并行回收"><a href="#串行回收和并行回收" class="headerlink" title="串行回收和并行回收"></a>串行回收和并行回收</h4><p>串行回收: JDK1.5前的默认算法。<strong>缺点是只有一个线程</strong>，执行垃圾回收时程序停止的时间比较长。</p>
<p>并行回收: 多个线程执行垃圾回收适合于高吞吐量的系统，<strong>回收时系统会停止运行。</strong></p>
<h4 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h4><p>串行收集器是最古老，最稳定以及效率高的收集器，是一个单线程的收集器，在进行垃圾收集时候，必须暂停其他所有的工作线程（Stop The World）直到它收集结束。</p>
<p>特点：CPU利用率最高，停顿时间即用户等待时间比较长。</p>
<p>适用场景：小型应用。</p>
<p>通过JVM参数-XX:+UseSerialGC可以使用串行垃圾回收器。</p>
<h4 id="Parallel-New收集器"><a href="#Parallel-New收集器" class="headerlink" title="Parallel New收集器"></a>Parallel New收集器</h4><p>Parallel New收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代采用复制算法、老年代采用标记-压缩。</p>
<p>参数控制：-XX:+UseParNewGC  使用ParNew收集器</p>
<p>-XX:ParallelGCThreads 限制线程数量</p>
<h4 id="Parallel-Scavenge收集器-jdk1-8默认"><a href="#Parallel-Scavenge收集器-jdk1-8默认" class="headerlink" title="Parallel Scavenge收集器(jdk1.8默认)"></a>Parallel Scavenge收集器(jdk1.8默认)</h4><p>Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩，采用多线程来通过扫描并压缩堆。</p>
<p>特点：停顿时间短，回收效率高，对吞吐量要求高。</p>
<p>适用场景：大型应用，科学计算，大规模数据采集等。</p>
<p>通过JVM参数 -XX:+UseParallelGC 打开并发标记扫描垃圾回收器。</p>
<h4 id="cms收集器"><a href="#cms收集器" class="headerlink" title="cms收集器"></a>cms收集器</h4><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括： </p>
<p>l初始标记（CMS initial mark）</p>
<p>l并发标记（CMS concurrent mark）</p>
<p>l并发预处理</p>
<p>l重新标记（CMS remark）</p>
<p>l并发清除（CMS concurrent sweep）</p>
<p>l并发重置</p>
<p>其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 </p>
<p>由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。</p>
<p>优点:并发收集、低停顿 </p>
<p>缺点：产生大量空间碎片、并发阶段会降低吞吐量</p>
<p>采用“标记-清除”算法实现，使用多线程的算法去扫描堆，对发现未使用的对象进行回收。</p>
<p>特点：响应时间优先，减少垃圾收集停顿时间。</p>
<p>适应场景：大型服务器等。</p>
<p>通过JVM参数 -XX:+UseConcMarkSweepGC设置</p>
<h4 id="G1收集器-jdk1-9默认"><a href="#G1收集器-jdk1-9默认" class="headerlink" title="G1收集器(jdk1.9默认)"></a>G1收集器(jdk1.9默认)</h4><p>G1（Garbage First）是一个横跨新生代和老年代的垃圾回收器。实际上，它已经打乱了前面所说的堆结构，直接将堆分成极其多个区域。每个区域都可以充当 Eden 区、Survivor 区或者老年代中的一个。它采用的是标记-压缩算法，而且和 CMS 一样都能够在应用程序运行过程中并发地进行垃圾回收。G1 能够针对每个细分的区域来进行垃圾回收。在选择进行垃圾回收的区域时，它会优先回收死亡对象较多的区域。这也是 G1 名字的由来。</p>
<p>特点：支持很大的堆，高吞吐量</p>
<p>–支持多CPU和垃圾回收线程</p>
<p>–在主线程暂停的情况下，使用并行收集</p>
<p>–在主线程运行的情况下，使用并发收集</p>
<p>实时目标：可配置在N毫秒内最多只占用M毫秒的时间进行垃圾回收</p>
<p>通过JVM参数 -XX:+UseG1GC 使用G1垃圾回收器</p>
<p>注意:  并发是指一个处理器同时处理多个任务。 </p>
<p>并行是指多个处理器或者是多核的处理器同时处理多个不同的任务。 </p>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>针对新生代的垃圾回收器共有三个：Serial，Parallel Scavenge 和 Parallel New。这三个采用的都是标记-复制算法。其中，Serial 是一个单线程的，Parallel New 可以看成 Serial 的多线程版本。Parallel Scavenge 和 Parallel New 类似，但更加注重吞吐率。此外，Parallel Scavenge 不能与 CMS 一起使用。</p>
<p>针对老年代的垃圾回收器也有三个：Serial Old 和 Parallel Old，以及 CMS。Serial Old 和 Parallel Old 都是标记-压缩算法。同样，前者是单线程的，而后者可以看成前者的多线程版本。</p>
<p>CMS 采用的是标记-清除算法，并且是并发的。除了少数几个操作需要 Stop-the-world 之外，它可以在应用程序运行过程中进行垃圾回收。在并发收集失败的情况下，Java 虚拟机会使用其他两个压缩型垃圾回收器进行一次垃圾回收。由于 G1 的出现，CMS 在 Java 9 中已被废弃。</p>
]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Java Web入门</title>
    <url>/JAVA/Java-Web%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>该阶段的学习目标是了解javaWEB的整个技术体系，熟悉JavaWEB前后台交互的实现方式及相关原理。明确未来大数据所分析数据的来源以及分析完成后的数据如何实现页面可视化.</p>
<h1 id="JavaWeb的技术体系"><a href="#JavaWeb的技术体系" class="headerlink" title="JavaWeb的技术体系"></a>JavaWeb的技术体系</h1><p><img src="/JAVA/Java-Web%E5%85%A5%E9%97%A8/wps1-1600839866149.jpg" alt="wps1"></p>
<h1 id="前端三剑客"><a href="#前端三剑客" class="headerlink" title="前端三剑客"></a>前端三剑客</h1><h2 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h2><h3 id="HTML是什么"><a href="#HTML是什么" class="headerlink" title="HTML是什么?"></a>HTML是什么?</h3><p>HTML指的超文本标记语言(Hyper Text Markup Language)，是一种用来描述网页的语言。超文本指的是除了可以包含文字之外，还可以包含图片、链接、音乐、视频、程序等内容。</p>
<h3 id="HTML网页的组成"><a href="#HTML网页的组成" class="headerlink" title="HTML网页的组成:"></a>HTML网页的组成:</h3><p><img src="/JAVA/Java-Web%E5%85%A5%E9%97%A8/wps2-1600839896755.jpg" alt="wps2"></p>
<h3 id="常用的HTML标签"><a href="#常用的HTML标签" class="headerlink" title="常用的HTML标签"></a>常用的HTML标签</h3><ol>
<li><p>html 根标记</p>
</li>
<li><p>head 头标记</p>
</li>
<li><p>body 体标记</p>
</li>
<li><p>a 超链接</p>
</li>
<li><p>form 表单</p>
</li>
<li><p>table 表格</p>
</li>
<li><p>h1~h6 标题</p>
</li>
<li><p>br 换行</p>
</li>
<li><p>img 图片</p>
</li>
</ol>
<h3 id="一个基本结构的HTML页面"><a href="#一个基本结构的HTML页面" class="headerlink" title="一个基本结构的HTML页面"></a>一个基本结构的HTML页面</h3><p><img src="/JAVA/Java-Web%E5%85%A5%E9%97%A8/wps3-1600839917690.jpg" alt="wps3"></p>
<h3 id="登录页面的示例"><a href="#登录页面的示例" class="headerlink" title="登录页面的示例"></a>登录页面的示例</h3><p><img src="/JAVA/Java-Web%E5%85%A5%E9%97%A8/wps4-1600839920672.jpg" alt="wps4"></p>
<h2 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h2><h3 id="CSS是什么"><a href="#CSS是什么" class="headerlink" title="CSS是什么"></a>CSS是什么</h3><p>CSS 指层叠样式表 (Cascading Style<br>Sheets)。主要用来设置网页中元素的样式。如边框，颜色，位置等…</p>
<p>CSS即可以写在HTML中，也可以写在元素的style属性里面，还可以写在.css外部文件里然后引入到页面</p>
<h3 id="编写位置"><a href="#编写位置" class="headerlink" title="编写位置"></a>编写位置</h3><p>1)  写在元素的style属性里面</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;p style="color:red;font-size:16px;"&gt; Hello CSS &lt;/p&gt;</span><br></pre></td></tr></table></figure>

<p>2)  写在HTML head标签中的style标签中</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;style type="text/css"&gt;                            </span><br><span class="line"><span class="selector-tag">p</span> &#123;                                                    </span><br><span class="line"><span class="attribute">color</span>: blue;                                          </span><br><span class="line"><span class="attribute">background-color</span>: yellow;                            </span><br><span class="line">&#125;                                                     </span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure>



<p>3)  写在外部的CSS文件中</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;link rel="stylesheet" type="text/css" href="style.css" &gt;</span><br></pre></td></tr></table></figure>

<h3 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h3><p>1)  标签(元素)选择器</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">p</span> &#123;                 </span><br><span class="line"><span class="attribute">color</span>:red;          </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2)  Id选择器</p>
<p>按照元素的id选中相应的元素，使用#id值</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="selector-id">#abc</span> &#123;                                             </span><br><span class="line"> <span class="attribute">color</span>:red;                                         </span><br><span class="line"> &#125;                                                  </span><br><span class="line"> &lt;p id="abc"&gt;大家好&lt;/p&gt;</span><br></pre></td></tr></table></figure>

<p>3)  class类选择器</p>
<p>按照元素的类名选中相应的元素，使用.class值</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.foot</span> &#123;                                                   </span><br><span class="line"><span class="attribute">color</span>:red;                                                   </span><br><span class="line">&#125;                                                           </span><br><span class="line">&lt;p class="foot"&gt;你好&lt;/p&gt;                                </span><br><span class="line">&lt;b class="foot"&gt;你也好&lt;/b&gt;</span><br></pre></td></tr></table></figure>

<h3 id="常用样式"><a href="#常用样式" class="headerlink" title="常用样式"></a>常用样式</h3><table>
<thead>
<tr>
<th>color</th>
<th>设置颜色</th>
<th>颜色可以写颜色名如：black, blue, red, green, white, yellow等颜色也可以写rgb值和十六进制表示值：如rgb(255,0,0)，#00F6DE，如果写十六进制值必须加#</th>
</tr>
</thead>
<tbody><tr>
<td>background-color</td>
<td>设置背景色</td>
<td>同上</td>
</tr>
<tr>
<td>width/height</td>
<td>设置宽度/高度</td>
<td>可以写像素值：19px；也可以写百分比值：20%；</td>
</tr>
<tr>
<td>font-size</td>
<td>设置字体大小</td>
<td>写像素值：19px；</td>
</tr>
<tr>
<td>text-align</td>
<td>设置文本位置</td>
<td>左中右  leftcengerright</td>
</tr>
<tr>
<td>border</td>
<td>设置边框</td>
<td>例如:黑色1像素实线边框border: 1px solid black;</td>
</tr>
<tr>
<td>margin-left/margin-right/margin-top/margin-bottom</td>
<td>设置左外边距/右外边距/上外边距/下外边距</td>
<td>例如: div居中 margin-left: <strong>auto</strong>;margin-right: <strong>auto</strong>;</td>
</tr>
</tbody></table>
<h3 id="布局"><a href="#布局" class="headerlink" title="布局"></a>布局</h3><p>1)  浮动</p>
<p>浮动的框可以向左或向右移动，直到它的外边缘碰到包含框或另一个浮动框的边框为止</p>
<ul>
<li><p>未使用浮动的li</p>
<p><img src="/JAVA/Java-Web%E5%85%A5%E9%97%A8/wps5-1600840254003.jpg" alt="wps5"></p>
</li>
<li><p>向左浮动的li</p>
</li>
</ul>
<p><img src="/JAVA/Java-Web%E5%85%A5%E9%97%A8/wps6-1600840261957.jpg" alt="wps6"></p>
<ul>
<li>代码</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"    </span><br><span class="line">"http://www.w3.org/TR/html4/loose.dtd"&gt;                                                                                                </span><br><span class="line">&lt;<span class="selector-tag">html</span>&gt;                                                                                                                               </span><br><span class="line">&lt;<span class="selector-tag">head</span>&gt;                                                                                                                               </span><br><span class="line">&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;                                      </span><br><span class="line">&lt;title&gt;Insert title here&lt;/title&gt;                                 </span><br><span class="line"></span><br><span class="line">&lt;style type="text/css"&gt;                                          </span><br><span class="line"></span><br><span class="line"><span class="selector-tag">div</span> &#123;                                                               </span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid black;                                           </span><br><span class="line"><span class="attribute">width</span>: <span class="number">800px</span>;                                                       </span><br><span class="line"><span class="attribute">height</span>: <span class="number">30px</span>;                                                       </span><br><span class="line">&#125;                                                                    </span><br><span class="line"><span class="selector-tag">ul</span> &#123;                                                                </span><br><span class="line"><span class="attribute">list-style</span>: none;                                                   </span><br><span class="line"><span class="attribute">margin</span>: <span class="number">0px</span>;                                                       </span><br><span class="line">&#125;                                                                   </span><br><span class="line"><span class="selector-tag">li</span> &#123;                                                                </span><br><span class="line"><span class="attribute">float</span>: left;                                                       </span><br><span class="line"><span class="attribute">background-color</span>: <span class="number">#ddffcc</span>;                                         </span><br><span class="line"><span class="attribute">text-align</span>: center;                                                 </span><br><span class="line"><span class="attribute">width </span>: <span class="number">80px</span>;                                                       </span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid black;                                           </span><br><span class="line"><span class="attribute">width</span>: <span class="number">80px</span>;                                                       </span><br><span class="line">&#125;                                                                                                                                     </span><br><span class="line">&lt;/style&gt;                                                           </span><br><span class="line">                                                                    </span><br><span class="line">&lt;/head&gt;</span><br></pre></td></tr></table></figure>



<h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><h3 id="什么是JavaScript"><a href="#什么是JavaScript" class="headerlink" title="什么是JavaScript"></a>什么是JavaScript</h3><p>JavaScript是一门客户端脚本语言，主要运行在浏览器中，浏览器中负责运行JavaScript脚本代码的程序叫JavaScript引擎</p>
<h3 id="编写位置-1"><a href="#编写位置-1" class="headerlink" title="编写位置"></a>编写位置</h3><p>js需要包括在<script>标签中，这个标签可以出现在页面的任何位置,一般建议写在HTML<br>head标签中.</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;<span class="selector-tag">head</span>&gt;                          </span><br><span class="line">                                  </span><br><span class="line">&lt;script type="text/javascript"&gt; </span><br><span class="line">                                  </span><br><span class="line">//Js 代码                         </span><br><span class="line">                                  </span><br><span class="line">&lt;/script&gt;                       </span><br><span class="line">                                  </span><br><span class="line">&lt;/head&gt;</span><br></pre></td></tr></table></figure>



<h3 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h3><p>1)  变量</p>
<ul>
<li><p>声明：使用var 如：var x=65; var y="你好";</p>
<p>1.变量的声明不需要指定数据类型，可以接受所有的数据类型</p>
<p>2.变量名区分大小写，abc和aBc是两个不同的变量</p>
</li>
<li><p>赋值：x=44; x="abc"; x=new Date();</p>
<p>1.变量可以接受任何值</p>
<p>2.声明和赋值也可同时进行 如var x="abc";</p>
</li>
</ul>
<p>2)  函数</p>
<ul>
<li>声明：使用function关键字，没有指定返回值一说！参数列表也没有指定参数类型一说，因为js所有类型都使用var来声明</li>
</ul>
<ol>
<li>函数在js中也是一种对象，可以将函数的引用赋值给变量</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var abc&#x3D;function(a,b)&#123; return a+b;&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>为函数起个名字叫add</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function add(a,b)&#123; return a+b;&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>调用</li>
</ul>
<ol>
<li><p>如果是声明方式第一种的，使用变量名+()的方式进行调用 abc(1,2);</p>
</li>
<li><p>如果是声明方式第二种的，直接使用函数名调用 add(1,2);</p>
</li>
</ol>
<p><strong>注意：js调用函数的时候不会检查参数列表，所以js中没有重载一说，add(1,2)；add(1)；add(1,"abc")；add("abc")；add(1,"666",true);add()；都是调用的同一个方法。</strong></p>
<p>3)  对象</p>
<ul>
<li>创建</li>
</ul>
<ol>
<li><p>var obj = new Object();</p>
</li>
<li><p>var obj = {};</p>
</li>
</ol>
<ul>
<li>为对象的属性赋值</li>
</ul>
<ol>
<li>动态添加</li>
</ol>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">var obj = new Object();                               </span><br><span class="line">                                                      </span><br><span class="line">obj.name = "张三";                                    </span><br><span class="line">                                                      </span><br><span class="line">obj.age = 18;                                         </span><br><span class="line">                                                      </span><br><span class="line">obj.work = function()&#123; //对象的属性可以赋值为一个函数 </span><br><span class="line">                                                      </span><br><span class="line"><span class="selector-tag">alert</span>("我在工作！");                                  </span><br><span class="line">                                                      </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>创建时指定</li>
</ol>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">obj = &#123;            </span><br><span class="line">                   </span><br><span class="line"><span class="selector-tag">name</span><span class="selector-pseudo">:"</span>张三",       </span><br><span class="line">                   </span><br><span class="line"><span class="selector-tag">age</span><span class="selector-pseudo">:18</span>,            </span><br><span class="line">                   </span><br><span class="line"><span class="selector-tag">work</span><span class="selector-pseudo">:function()</span>&#123;   </span><br><span class="line">                   </span><br><span class="line">alert("我在工作"); </span><br><span class="line">                   </span><br><span class="line">&#125;                  </span><br><span class="line">                   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>使用对象的属性,直接以.的方式来访问</li>
</ul>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">alert</span>(<span class="selector-tag">obj</span><span class="selector-class">.name</span>) </span><br><span class="line">                 </span><br><span class="line"><span class="selector-tag">obj</span><span class="selector-class">.work</span>();</span><br></pre></td></tr></table></figure>

<h3 id="其他语法"><a href="#其他语法" class="headerlink" title="其他语法"></a>其他语法</h3><p>java中的for，while，if-else，switch，try-catch，break，continue，以及各种运算符，在js中也是按照同样的方式使用的。这里不再赘述。</p>
<h3 id="JavaScript事件"><a href="#JavaScript事件" class="headerlink" title="JavaScript事件"></a>JavaScript事件</h3><p>1)  什么是事件</p>
<p>事件就是浏览器或者用户交互时触发的行为。比如按钮点击，表单提交，鼠标滑动等等。</p>
<p>2)  事件的分类</p>
<ul>
<li><p>系统事件：如：文档加载完成。</p>
</li>
<li><p>用户事件：如：鼠标移入移出，单击双击等。</p>
</li>
</ul>
<p>3)  事件的触发</p>
<ul>
<li><p>系统事件会由系统触发，如window.onload事件，用户事件由用户行为触发如click事件。</p>
</li>
<li><p>常见的系统事件</p>
<blockquote>
<p>window.onload</p>
</blockquote>
</li>
<li><p>常见的用户事件</p>
<blockquote>
<p>onclick当用户点击某个对象时调用的事件句柄。</p>
<p>onblur 元素失去焦点。</p>
<p>onfocus元素获得焦点。</p>
<p>onchange 域的内容被改变。</p>
<p>onkeydown某个键盘按键被按下</p>
<p>onmouseover 鼠标移入</p>
<p>onmouseout 鼠标移出</p>
</blockquote>
</li>
</ul>
<p>4)  事件响应</p>
<ul>
<li><p>我们希望某个事件发生的时候我们可以做一些事情。这个称为事件的响应，比如用户点击了一个按钮，我弹出一个框告诉用户，你成功的点击了这个按钮。事件触发后我们要执行的方法称为响应函数。</p>
</li>
<li><p>响应函数与事件关联</p>
</li>
</ul>
<ol>
<li>直接为事件绑定响应函数</li>
</ol>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">window.onload = function()&#123; </span><br><span class="line">                            </span><br><span class="line"><span class="selector-tag">alert</span>("文档加载完成了！")   </span><br><span class="line">                            </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>使用标签的事件属性</li>
</ol>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;a href=”atbiubiu.com” onclick=”biubiu()”&gt;red0819&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">在&lt;script&gt;&lt;/script&gt;中定义这个函数</span><br><span class="line"></span><br><span class="line">&lt;script type=”text/javascript”&gt;</span><br><span class="line">		<span class="selector-tag">function</span> <span class="selector-tag">gotobiubiu</span>()&#123;</span><br><span class="line">			alert(“biubiubiu”)</span><br><span class="line">        &#125;</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure>



<h3 id="3-2-6-DOM"><a href="#3-2-6-DOM" class="headerlink" title="3.2.6 DOM"></a>3.2.6 DOM</h3><p>1)  什么是DOM</p>
<p>Document Object Model(文档对象模型)，我们浏览器把整个网页会当成一个大的对象，利用面向对象的方式操作网页内容。</p>
<p>2)  DOM模型</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps7-1600840978736.jpg" alt="wps7"></p>
<p>3)  document对象</p>
<p>document对象是window对象的一个属性，代表当前整个HTML文档，将这个文档抽象成了document对象，包含了整个dom树的所有内容。</p>
<p>其本质是window.document，而window.可以省略。直接使用document</p>
<p>4)  DOM操作</p>
<ul>
<li><p>查询元素</p>
<table>
<thead>
<tr>
<th>根据id值查询</th>
<th>document.getElementById(“id值”)</th>
<th>一个具体的元素节点</th>
</tr>
</thead>
<tbody><tr>
<td>根据标签名查询</td>
<td>document.getElementsByTagName(“标签名”)</td>
<td>元素节点数组</td>
</tr>
<tr>
<td>根据name属性值查询</td>
<td>document.getElementsByName(“name值”)</td>
<td>元素节点数组</td>
</tr>
</tbody></table>
</li>
<li><p>属性操作</p>
<table>
<thead>
<tr>
<th>读取属性值</th>
<th>元素对象.属性名</th>
</tr>
</thead>
<tbody><tr>
<td>修改属性值</td>
<td>元素对象.属性名=新的属性名</td>
</tr>
</tbody></table>
</li>
<li><p>文本操作</p>
<table>
<thead>
<tr>
<th>获取元素内容</th>
<th>元素.innerHTML</th>
</tr>
</thead>
<tbody><tr>
<td>给元素内容赋值</td>
<td>元素.innerHTML=值</td>
</tr>
</tbody></table>
</li>
</ul>
<h1 id="服务端技术"><a href="#服务端技术" class="headerlink" title="服务端技术"></a>服务端技术</h1><h2 id="Web服务器"><a href="#Web服务器" class="headerlink" title="Web服务器"></a>Web服务器</h2><h3 id="常见的Web服务器"><a href="#常见的Web服务器" class="headerlink" title="常见的Web服务器"></a>常见的Web服务器</h3><p>1)  Web服务器主要用来接收客户端发送的请求和响应客户端请求。</p>
<p>2)  <strong>Tomcat（Apache）</strong>：当前应用最广的JavaWeb服务器；</p>
<p>3)  JBoss（Redhat红帽）：支持JavaEE，应用比较广EJB容器 --><br>    SSH轻量级的框架代替</p>
<p>4)  GlassFish（Orcale）：Oracle开发JavaWeb服务器，应用不是很广；</p>
<p>5)  Resin（Caucho）：支持JavaEE，应用越来越广；</p>
<p>6)  Weblogic（Orcale）：要钱的！支持JavaEE，适合大型项目；</p>
<p>7)  Websphere（IBM）：要钱的！支持JavaEE，适合大型项目</p>
<h3 id="Tomcat服务器的安装及配置"><a href="#Tomcat服务器的安装及配置" class="headerlink" title="Tomcat服务器的安装及配置"></a>Tomcat服务器的安装及配置</h3><p>1)  将Tomcat的安装包解压到磁盘的任意位置(非中文无空格)</p>
<p>2)  Tomcat服务的目录结构</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps8-1600841074556.jpg" alt="wps8"></p>
<h3 id="在IDEA中配置tomcat"><a href="#在IDEA中配置tomcat" class="headerlink" title="在IDEA中配置tomcat"></a>在IDEA中配置tomcat</h3><p><img src="Java-Web%E5%85%A5%E9%97%A8/wps9-1600841080786.jpg" alt="wps9"></p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps10-1600841090021.jpg" alt="wps10"></p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps11-1600841100755.jpg" alt="wps11"></p>
<h2 id="web-Module"><a href="#web-Module" class="headerlink" title="web Module"></a>web Module</h2><h3 id="创建web-Module"><a href="#创建web-Module" class="headerlink" title="创建web Module"></a>创建web Module</h3><p><img src="Java-Web%E5%85%A5%E9%97%A8/wps12-1600841116380.jpg" alt="wps12"></p>
<h3 id="部署web-Module"><a href="#部署web-Module" class="headerlink" title="部署web Module"></a>部署web Module</h3><p><img src="Java-Web%E5%85%A5%E9%97%A8/wps13-1600841127938.jpg" alt="wps13"></p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps14-1600841135150.jpg" alt="wps14"></p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps15-1600841143231.jpg" alt="wps15"></p>
<h2 id="Servlet"><a href="#Servlet" class="headerlink" title="Servlet"></a>Servlet</h2><h3 id="什么是Servlet"><a href="#什么是Servlet" class="headerlink" title="什么是Servlet"></a>什么是Servlet</h3><p>1)  Servlet是Sun公司制定的一套技术标准，包含与Web应用相关的一系列接口，是Web应用实现方式的宏观解决方案。而具体的Servlet容器负责提供标准的实现。</p>
<p>2)  Servlet作为服务器端的一个组件，它的本意是"服务器端的小程序"。Servlet的实例对象由Servlet容器负责创建；Servlet的方法由容器在特定情况下调用；Servlet容器会在Web应用卸载时销毁Servlet对象的实例。</p>
<p>3) <strong><font color="red"> 简单可以理解为Servlet就是用来处理客户端的请求与响应客户端。</font></strong></p>
<blockquote>
<p>Tomcat也算是Servlet的容器</p>
<p>有一个请求就有一个Servlet来处理和响应</p>
</blockquote>
<h3 id="Servlet开发规则"><a href="#Servlet开发规则" class="headerlink" title="Servlet开发规则"></a>Servlet开发规则</h3><p>1)  实际编码通过继承HttpServlet来完成Servlet的开发</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps16-1600841288466.jpg" alt="wps16"></p>
<h3 id="Servlet类的相关方法"><a href="#Servlet类的相关方法" class="headerlink" title="Servlet类的相关方法"></a>Servlet类的相关方法</h3><p>1)  doGet方法 Servlet中用于处理get请求的方法</p>
<blockquote>
<p>GET:提交的数据会暴露在地址栏中，不安全，还有对提交数据的大小支持比较小（大概4kb？）</p>
<p>POST:提交的数据不会暴露到地址栏中，比较安全，对提交的数据大小理论上没有限制</p>
<p>但是不能说post就绝对安全，因为所有传递的信息都有可能被拦截</p>
</blockquote>
<p>2)  doPost方法 Servlet中用于处理post请求的方法</p>
<blockquote>
<p>常用的请求方式GET和POST</p>
<p>如果是get方式传来的请求就用doget处理</p>
</blockquote>
<p>3)  service方法 </p>
<ol>
<li><p>在Servlet的顶层实现中，在service方法中调用的具体的doGet或者是doPost</p>
</li>
<li><p>在实际开发Servlet的过程中，可以选择重写doGet以及doPost 或者<br>直接重写service方法来处理请求。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;form action=”地址（http://localhost:8080/Moudle名/文件夹名/网址名）” method=”post”&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<blockquote>
<p>用来调用doget和dopost方法</p>
<p>一个请求进入服务器先进入service 然后在service中选择再POST还是GET</p>
<p>两种方式：</p>
<ol>
<li><p>重写get和post方法</p>
</li>
<li><p>只重写service</p>
</li>
</ol>
</blockquote>
<h3 id="Servlet在web-xml中的配置"><a href="#Servlet在web-xml中的配置" class="headerlink" title="Servlet在web.xml中的配置"></a>Servlet在web.xml中的配置</h3><p><img src="Java-Web%E5%85%A5%E9%97%A8/wps17-1600841414434.jpg" alt="wps17"></p>
<h2 id="获取请求参数值"><a href="#获取请求参数值" class="headerlink" title=" 获取请求参数值"></a> 获取请求参数值</h2><blockquote>
<p>自定义的Servlet，需要继承HttpServlet（Tomcat）</p>
<p>目的：处理客户端的响应</p>
</blockquote>
<p>1)  HttpServletRequest</p>
<ol>
<li><p>该接口是ServletRequest接口的子接口，封装了HTTP请求的相关信息，由Servlet容器创建其实现类对象并传入service(ServletRequest<br>req, ServletResponse<br>res)方法中。以下我们所说的HttpServletRequest对象指的是容器提供的HttpServletRequest实现类对象。</p>
</li>
<li><p>HttpServletRequest对象的主要功能有：</p>
</li>
</ol>
<p>​                 获取请求参数</p>
<p>​                 在请求域中绑定数据</p>
<p>​                 将请求转发给另外一个URL地址 [转发]</p>
<blockquote>
<p>自定义的Servlet，需要继承HttpServlet（Tomcat）</p>
<p>目的：处理客户端的响应</p>
</blockquote>
<h2 id="响应结果"><a href="#响应结果" class="headerlink" title="响应结果"></a>响应结果</h2><p>1)  HttpServletResponse</p>
<ol>
<li><p>该接口是ServletResponse接口的子接口，封装了HTTP响应的相关信息，由Servlet容器创建其实现类对象并传入service(ServletRequest req, ServletResponse res)方法中。以下我们所说的HttpServletResponse对象指的是容器提供的HttpServletResponse实现类对象。</p>
</li>
<li><p>主要功能:</p>
<p>使用PrintWriter对象向浏览器输出数据</p>
<p>实现请求的重定向[重定向]</p>
</li>
</ol>
<h2 id="具体登录功能的实现步骤"><a href="#具体登录功能的实现步骤" class="headerlink" title="具体登录功能的实现步骤"></a>具体登录功能的实现步骤</h2><p>1)  在登录页面中录入用户名和密码，点击登录按钮提交登录请求</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps18-1600841515877.jpg" alt="wps18"></p>
<p>2)  在LoginServlet中通过request对象获取到页面表单提交的用户名和密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;获取用户输入的用户名和密码</span><br><span class="line">String username &#x3D;  req.getParameter(&quot;username&quot;);</span><br><span class="line">String password &#x3D;  req.getParameter(&quot;password&quot;);</span><br></pre></td></tr></table></figure>

<p>3)  调用Dao对象，将用户提交的用户名和密码与数据库的用户表的数据进行匹配</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">UserDao dao &#x3D; new UserDaoJdbcImpl();</span><br><span class="line">User user &#x3D; dao.findUserByUsernameAndPassword(username, password);</span><br></pre></td></tr></table></figure>

<p>4)  得出结果，完成响应.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;设置响应头信息</span><br><span class="line">resp.setContentType(&quot;text&#x2F;html;charset&#x3D;utf-8&quot;);</span><br><span class="line">&#x2F;&#x2F;获取输出流</span><br><span class="line">PrintWriter  out &#x3D; resp.getWriter();</span><br><span class="line">if(user &#x3D;&#x3D; null ) &#123;</span><br><span class="line">	&#x2F;&#x2F;登录失败</span><br><span class="line">	out.println(&quot;登录失败！！！！&quot;);</span><br><span class="line">&#125;else &#123;</span><br><span class="line">	&#x2F;&#x2F;登录成功</span><br><span class="line">	out.println(&quot;登录成功！！！！&quot;);</span><br><span class="line">	&#125;</span><br><span class="line">out.close();</span><br></pre></td></tr></table></figure>

<h2 id="请求重定向-redirect"><a href="#请求重定向-redirect" class="headerlink" title="请求重定向 redirect"></a>请求重定向 redirect</h2><p>1)  Servlet接收到浏览器端请求并处理完成后，给浏览器端一个特殊的响应，这个特殊的响应要求浏览器去请求一个新的资源，整个过程中浏览器端会发出两次请求，且浏览器地址栏会改变为新资源的地址。</p>
<p>2)  重定向的情况下，原Servlet和目标资源之间就不能共享请求域数据了</p>
<p>3)  实现重定向的API</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps19-1600841566738.jpg" alt="wps19"></p>
<p>4) 登录失败后重定向到登录页面</p>
<h2 id="JSP页面"><a href="#JSP页面" class="headerlink" title="JSP页面"></a>JSP页面</h2><p>1)  JSP全称Java Server<br>    Pages，顾名思义就是运行在java服务器中的页面，也就是在我们JavaWeb中的动态页面，其本质就是一个Servlet。</p>
<p>2)  其本身是一个动态网页技术标准，它的主要构成有HTML网页代码、Java代码片段、JSP标签几部分组成，后缀是.jsp</p>
<p>3)  相比于Servlet，JSP更加善于处理显示页面，而Servlet跟擅长处理业务逻辑，两种技术各有专长，所以一般我们会将Servlet和JSP结合使用，Servlet负责业务，JSP负责显示。</p>
<p>4)  一般情况下，<br>    都是Servlet处理完的数据，转发到JSP，JSP负责显示数据的工作</p>
<p>5)  JSP的基本语法:</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps20-1600841575927.jpg" alt="wps20"></p>
<p>6)  JSP的脚本元素</p>
<ol>
<li>脚本片段是嵌入到JSP中Java代码段，格式以<%开头，%>结尾，两个%号之间就可以编写Java代码了</li>
</ol>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps21-1600841593811.jpg" alt="wps21"></p>
<p>7)  JSP的表达式</p>
<ol>
<li>JSP表达式用来直接将Java变量输出到页面中，格式以<%=开头，以%>结尾，中间是我们要输出的内容</li>
</ol>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps22-1600841610339.jpg" alt="wps22"></p>
<p>8)  JSP的内置对象</p>
<ol>
<li><p>out（JspWriter）：相当于response.getWriter()获取的对象，用于在页面中显示信息。</p>
</li>
<li><p>config（ServletConfig）：对应Servlet中的ServletConfig对象。</p>
</li>
<li><p>page（Object）：对应当前Servlet对象，实际上就是this。</p>
</li>
<li><p>pageContext（PageContext）：当前页面的上下文，也是一个域对象。</p>
</li>
<li><p>exception（Throwable）：错误页面中异常对象</p>
</li>
<li><p>request（HttpServletRequest）：HttpServletRequest对象</p>
</li>
<li><p>response（HttpServletResponse）：HttpServletResponse对象</p>
</li>
<li><p>application（ServletContext）：ServletContext对象</p>
</li>
<li><p>session（HttpSession）：HttpSession对象</p>
</li>
</ol>
<p>9)  EL表达式</p>
<ol>
<li><p>EL是JSP内置的表达式语言，用以访问页面的上下文以及不同作用域中的对象<br>，取得对象属性的值，或执行简单的运算或判断操作。EL在得到某个数据时，会自动进行数据类型的转换。</p>
</li>
<li><p><strong>EL表达式用于代替JSP表达式(<%= %>)在页面中做输出操作。</strong></p>
</li>
<li><p>EL表达式仅仅用来读取数据，而不能对数据进行修改。</p>
</li>
<li><p>使用EL表达式输出数据时，<strong>如果有则输出数据，如果为null则什么也不输出。</strong></p>
</li>
<li><p><strong>EL表达式的语法:</strong></p>
</li>
</ol>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps23-1600841656675.jpg" alt="wps23"></p>
<ol start="6">
<li><p>EL取值的四个域:</p>
<p>pageScope</p>
<p>requestScope</p>
<p>sessionScope</p>
<p>applicationScope</p>
</li>
</ol>
<h2 id="请求转发"><a href="#请求转发" class="headerlink" title="请求转发"></a>请求转发</h2><p>1)  Servlet接收到浏览器端请求后，进行一定的处理，先不进行响应，而是在服务器端内部"转发"给其他Servlet程序继续处理。在这种情况下浏览器端只发出了一次请求，浏览器地址栏不会发生变化，用户也感知不到请求被转发了。</p>
<p>2)  转发请求的Servlet和目标Servlet共享同一个request对象。</p>
<p>3)  实现转发的API</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps24-1600841687798.jpg" alt="wps24"></p>
<h2 id="重定向与转发的区别"><a href="#重定向与转发的区别" class="headerlink" title="重定向与转发的区别"></a>重定向与转发的区别</h2><table>
<thead>
<tr>
<th></th>
<th>转发</th>
<th>重定向</th>
</tr>
</thead>
<tbody><tr>
<td>浏览器地址栏</td>
<td>不改变</td>
<td>改变</td>
</tr>
<tr>
<td>发送请求次数</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>能否共享request对象数据</td>
<td>能</td>
<td>否</td>
</tr>
<tr>
<td>目标资源：WEB-INF下的资源</td>
<td>能访问</td>
<td>不能访问</td>
</tr>
<tr>
<td>Request中绑定的数据是否可以传递</td>
<td>能</td>
<td>不能</td>
</tr>
</tbody></table>
<hr>
<h2 id="页面中-错误提示的功能效果"><a href="#页面中-错误提示的功能效果" class="headerlink" title="页面中`错误提示的功能效果"></a>页面中`错误提示的功能效果</h2><p><img src="Java-Web%E5%85%A5%E9%97%A8/wps25-1600841822594.jpg" alt="wps25"></p>
<h2 id="Cookie-了解"><a href="#Cookie-了解" class="headerlink" title="Cookie[了解]"></a>Cookie[了解]</h2><p>1)  HTTP是无状态协议，服务器不能记录浏览器的访问状态，也就是说服务器不能区分中两次请求是否由一个客户端发出。这样的设计严重阻碍的Web程序的设计。如：在我们进行网购时，买了一条裤子，又买了一个手机。由于http协议是无状态的，如果不通过其他手段，服务器是不能知道用户到底买了什么。而Cookie就是解决方案之一。</p>
<p>2)  Cookie实际上就是服务器保存在浏览器上的一段信息。浏览器有了Cookie之后，每次向服务器发送请求时都会同时将该信息发送给服务器，服务器收到请求后，就可以根据该信息处理请求。</p>
<p>3)  Cookie的用途</p>
<blockquote>
<p>网上商城购物车</p>
<p>用户登录状态的保持</p>
</blockquote>
<p>4)  Cookie的限制性</p>
<pre><code>1.  Cookie作为请求或响应报文发送，无形中增加了网络流量。

2.  Cookie是明文传送的安全性差。

3.  各个浏览器对Cookie有限制，使用上有局限</code></pre><p>5)  Cookie的具体使用</p>
<ol>
<li>创建cookie</li>
</ol>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps26-1600841840922.jpg" alt="wps26"></p>
<ol start="2">
<li>读取cookie</li>
</ol>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps27-1600841845047.jpg" alt="wps27"></p>
<h2 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h2><p>1)  使用Cookie有一个非常大的局限，就是如果Cookie很多，则无形的增加了客户端与服务端的数据传输量。而且由于浏览器对Cookie数量的限制，注定我们不能再Cookie中保存过多的信息，于是Session出现。</p>
<p>2)  Session的作用就是在服务器端保存一些用户的数据，然后传递给用户一个名字为JSESSIONID的Cookie，这个JESSIONID对应这个服务器中的一个Session对象，通过它就可以获取到保存用户信息的Session。</p>
<p>3)  Session的工作原理</p>
<ol>
<li><p>Session的创建时机是在request.getSession()方法第一次被调用时。</p>
</li>
<li><p>Session被创建后，同时还会有一个名为JSESSIONID的Cookie被创建。</p>
</li>
<li><p>这个Cookie的默认时效就是当前会话。</p>
</li>
<li><p>简单来说，Session机制也是依赖于Cookie来实现的</p>
</li>
</ol>
<p>4)  Session的具体使用</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps28-1600841853884.jpg" alt="wps28"></p>
<p>5)  Session的时效问题</p>
<p>Session默认有效时间为30分钟，可以在服务器的web.xml配置中修改.</p>
<h2 id="登录成功跳转主页面显示登录用户"><a href="#登录成功跳转主页面显示登录用户" class="headerlink" title="登录成功跳转主页面显示登录用户"></a>登录成功跳转主页面显示登录用户</h2><h1 id="异步请求Ajax"><a href="#异步请求Ajax" class="headerlink" title="异步请求Ajax"></a>异步请求Ajax</h1><h2 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h2><p>1)  AJAX 是 Asynchronous JavaScript And XML<br>    的简称。直译为，异步的JS和XML。</p>
<p>2)  AJAX的实际意义是，不发生页面跳转、异步载入内容并改写页面内容的技术。</p>
<p>3)  AJAX也可以简单的理解为通过JS向服务器发送请求。</p>
<h2 id="异步处理"><a href="#异步处理" class="headerlink" title="异步处理"></a>异步处理</h2><p>1)  同步处理</p>
<p>AJAX出现之前，我们访问互联网时一般都是同步请求，也就是当我们通过一个页面向<br>服务器发送一个请求时，在服务器响应结束之前，我们的整个页面是不能操作的，也就<br>是直观上来看他是卡主不动的。</p>
<p>这就带来了非常糟糕的用户体验。首先，同步请求时，用户只能等待服务器的响应，而<br>不能做任何操作。其次，如果请求时间过长可能会给用户一个卡死的感觉。最后，同步<br>请求的最大缺点就是即使整个页面中只有一小部分内容发生改变我们也要刷新整个页<br>面。</p>
<p>2)  异步处理</p>
<p>而异步处理指的是我们在浏览网页的同时，通过AJAX向服务器发送请求，发送请求的过程中我们浏览网页的行为并不会收到任何影响，甚至主观上感知不到在向服务器发送请求。当服务器正常响应请求后，响应信息会直接发送到AJAX中，AJAX可以根据服务器响应的内容做一些操作。</p>
<p>使用AJAX的异步请求基本上完美的解决了同步请求带来的问题。首先，发送请求时不会影响到用户的正常访问。其次，即使请求时间过长，用户不会有任何感知。最后，AJAX可以根据服务器的响应信息局部的修改页面，而不需要整个页面刷新。</p>
<h2 id="异步请求对象"><a href="#异步请求对象" class="headerlink" title="异步请求对象"></a>异步请求对象</h2><p>1)  XMLHttpRequest对象是AJAX中非常重要的对象，所有的AJAX操作都是基于该对象的。</p>
<p>XMLHttpRequest对象用来封装请求报文，我们向服务器发送的请求信息全部都需要封装到该对象中。</p>
<p>这里需要稍微注意一下，XMLHttpRequest对象并没有成为标准，但是现在的主流浏览器都支持该对象，而一些如IE6的老版本浏览器中的创建方式有一些区别，但是问题不大。</p>
<h3 id="XMLHttpRequest对象的获取"><a href="#XMLHttpRequest对象的获取" class="headerlink" title="XMLHttpRequest对象的获取"></a>XMLHttpRequest对象的获取</h3><p><img src="Java-Web%E5%85%A5%E9%97%A8/wps29-1600841892166.jpg" alt="wps29"></p>
<h3 id="XMLHttpRequest对象的方法"><a href="#XMLHttpRequest对象的方法" class="headerlink" title="XMLHttpRequest对象的方法"></a>XMLHttpRequest对象的方法</h3><h4 id="open-method-url-async"><a href="#open-method-url-async" class="headerlink" title="open(method,url,async)"></a>open(method,url,async)</h4><p>open()用于设置请求的基本信息，接收三个参数。</p>
<ol>
<li><p>method</p>
<ul>
<li><p>请求的方法：get或post</p>
</li>
<li><p>接收一个字符串</p>
</li>
</ul>
</li>
<li><p>url</p>
<ul>
<li>请求的地址，接收一个字符串</li>
</ul>
</li>
<li><p>Assync</p>
<ul>
<li><p>发送的请求是否为异步请求，接收一个布尔值。</p>
</li>
<li><p>true 是异步请求</p>
</li>
<li><p>false 不是异步请求（同步请求）</p>
</li>
</ul>
</li>
</ol>
<h4 id="send-string"><a href="#send-string" class="headerlink" title="send(string)"></a>send(string)</h4><p>send()用于将请求发送给服务器，可以接收一个参数</p>
<ol>
<li><p>string参数</p>
<ul>
<li><p>该参数只在发送post请求时需要。</p>
</li>
<li><p>string参数用于设置请求体</p>
</li>
</ul>
</li>
</ol>
<h4 id="setRequestHeader-header-value"><a href="#setRequestHeader-header-value" class="headerlink" title="setRequestHeader(header,value)"></a>setRequestHeader(header,value)</h4><p>用于设置请求头</p>
<ol>
<li><p>header参数</p>
<ul>
<li>字符串类型，要设置的请求头的名字</li>
</ul>
</li>
<li><p>value参数</p>
<ul>
<li>字符串类型，要设置的请求头的值</li>
</ul>
</li>
</ol>
<h3 id="XMLHttpRequest对象的属性"><a href="#XMLHttpRequest对象的属性" class="headerlink" title="XMLHttpRequest对象的属性"></a>XMLHttpRequest对象的属性</h3><h4 id="readyState"><a href="#readyState" class="headerlink" title="readyState"></a>readyState</h4><ol>
<li><p>描述XMLHttpRequest的状态</p>
</li>
<li><p>一共有五种状态分别对应了五个数字：</p>
<ul>
<li><p>0 ：请求尚未初始化，open()尚未被调用</p>
</li>
<li><p>1 ：服务器连接已建立，send()尚未被调用</p>
</li>
<li><p>2 ：请求已接收，服务器尚未响应</p>
</li>
<li><p>3 ：请求已处理，正在接收服务器发送的响应</p>
</li>
<li><p>4 ：请求已处理完毕，且响应已就绪。</p>
</li>
</ul>
</li>
</ol>
<h4 id="status"><a href="#status" class="headerlink" title="status"></a>status</h4><p> 请求的响应码：</p>
<p> 200 响应成功</p>
<p> 404 页面未找到</p>
<p> 500 服务器内部错误 </p>
<h4 id="readyState-1"><a href="#readyState-1" class="headerlink" title="readyState"></a>readyState</h4><ol>
<li><p>请求的响应码</p>
<ul>
<li><p>200 响应成功</p>
</li>
<li><p>404 页面为找到</p>
</li>
<li><p>500 服务器内部错误</p>
</li>
</ul>
</li>
</ol>
<h4 id="onreadystatechange"><a href="#onreadystatechange" class="headerlink" title="onreadystatechange"></a>onreadystatechange</h4><p>① 该属性需要指向一个函数</p>
<p>② 该函数会在readyState属性发生改变时被调用</p>
<h4 id="responseText"><a href="#responseText" class="headerlink" title="responseText"></a>responseText</h4><p>① 获得字符串形式的响应数据。</p>
<h4 id="responseXML"><a href="#responseXML" class="headerlink" title="responseXML"></a>responseXML</h4><p>① 获得 XML 形式的响应数据。</p>
<h4 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h4><p><img src="Java-Web%E5%85%A5%E9%97%A8/wps30-1600842323838.jpg" alt="wps30"></p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps31-1600842396367.jpg" alt="wps31"></p>
<h2 id="JQuery对异步的支持"><a href="#JQuery对异步的支持" class="headerlink" title="JQuery对异步的支持 "></a>JQuery对异步的支持 </h2><p>1) JQuery是当前比较主流的 JavaScript 库，封装了很多预定义的对象和实现函数，帮助使用者建立有高难度交互的页面，并且兼容大部分主流</p>
<p>的浏览器.</p>
<p>JQuery对同样提供了对Ajax的支持，可以更加方便快速的进行Ajax的开发，相关的方法有$.get   $.post  $.ajax等.</p>
<p>JQuery的对象的本质就是dom对象的数组/集合</p>
<p>2) JQuery对象与dom对象的相互转换</p>
<p>JS转JQuery:  var  jObj = $(dObj);</p>
<p>JQuery转JS:  var  dObj = jObj[0]  或者  var dObj = jObj.get(0) </p>
<p>3) $.get方法</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps32-1600842423552.jpg" alt="wps32"></p>
<p>4)  $.post方法</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps33-1600842429806.jpg" alt="wps33"></p>
<p>5)  $.ajax方法</p>
<p>jQuery 底层 AJAX 实现。简单易用的高层实现见 $.get, $.post 等。$.ajax() 返回其创建的 XMLHttpRequest 对象。大多数情况下你无需直接操作该函数，除非你需要操作不常用的选项，以获得更多的灵活性。</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps34-1600842450769.jpg" alt="wps34"></p>
<p>6)  具体的示例代码</p>
<p><img src="Java-Web%E5%85%A5%E9%97%A8/wps35-1600842459196.jpg" alt="wps35"></p>
<h2 id="基于异步请求校验用户名"><a href="#基于异步请求校验用户名" class="headerlink" title="基于异步请求校验用户名"></a>基于异步请求校验用户名</h2><h1 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h1><h2 id="JSON-简介"><a href="#JSON-简介" class="headerlink" title=" JSON 简介"></a> JSON 简介</h2><ol>
<li><p>1) AJAX一开始使用的时XML的数据格式，XML的数据格式非常简单清晰，容易编写，但是由于XML中包含了过多的标签，以及十分复杂的结构，解析起来也相对复杂，所以目前来讲，AJAX中已经几乎不使用XML来发送数据了。取而代之的是一项新的技术JSON。</p>
<p>2) JSON是JavaScript Object Notation 的缩写，是JS提供的一种数据交换格式。</p>
<p>3) JSON对象本质上就是一个JS对象，但是这个对象比较特殊，它可以直接转换为字符串，在不同语言中进行传递，通过工具又可以转换为其他语言中的对象。</p>
<p>4) 例，有如下一个JSON对象：</p>
<p>① {“name”:”sunwukong” , ”age”:18 , ”address”:”beijing” }</p>
<p>② 这个对象中有三个属性name、age和address</p>
<p>③ 如果将该对象使用单引号引起了，那么他就变成了一个字符串</p>
<p>④ ‘{“name”:”sunwukong” , ”age”:18 , ”address”:”beijing” }’</p>
<p>⑤ 变成字符串后有一个好处，就是可以在不同语言之间传递。</p>
<p>⑥ 比如，将JSON作为一个字符串发送给Servlet，在Java中就可以把JSON字符串转换为一个Java对象。</p>
</li>
</ol>
<h2 id="JSON通过6种数据类型来表示"><a href="#JSON通过6种数据类型来表示" class="headerlink" title="JSON通过6种数据类型来表示"></a>JSON通过6种数据类型来表示</h2><p>1) 字符串</p>
<blockquote>
<p>l 例子：”字符串”</p>
<p>l 注意：不能使用单引号</p>
</blockquote>
<p>2) 数字：</p>
<blockquote>
<p>l 例子：123.4</p>
</blockquote>
<p>3) 布尔值：</p>
<blockquote>
<p>l 例子：true、false</p>
</blockquote>
<p>4) null值:</p>
<blockquote>
<p>l 例子：null</p>
</blockquote>
<p>5) 对象</p>
<blockquote>
<p>l 例子：{“name”:”sunwukong”, ”age”:18}</p>
</blockquote>
<p>6) 数组</p>
<blockquote>
<p>l 例子：[1,”str”,true]在JS中操作JSON</p>
</blockquote>
<p>1)  创建JSON对象</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">-   var json = &#123;"name1":"value1","name2":"value2" ,</span><br><span class="line">    "name3":[1,"str",true]&#125;;</span><br><span class="line"></span><br><span class="line">-   var json = [&#123;"name1":"value1"&#125;,&#123;"name2":"value2"&#125;];</span><br></pre></td></tr></table></figure>

<p>2)  JSON对象转换为JSON字符串</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">-   JSON.stringify(JSON对象)</span><br></pre></td></tr></table></figure>

<p>3)  JSON字符串转换为JSON对象</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">-   JSON.parse(JSON字符串)</span><br></pre></td></tr></table></figure>

<h2 id="在Java中操作JSON"><a href="#在Java中操作JSON" class="headerlink" title="在Java中操作JSON"></a>在Java中操作JSON</h2><p>1) 在Java中可以从文件中读取JSON字符串，也可以是客户端发送的JSON字符串，所以第一个问题，我们先来看如何将一个JSON字符串转换成一个Java对象。</p>
<p>2) 首先解析JSON字符串我们需要导入第三方的工具，目前主流的解析JSON的工具大概有三种json-lib、jackson、gson。三种解析工具相比较json-lib的使用复杂，且效率较差。而Jackson和gson解析效率较高。使用简单，这里我们以gson为例讲解。</p>
<p>3) Gson是Google公司出品的解析JSON工具，使用简单，解析性能好。</p>
<p>4) Gson中解析JSON的核心是Gson的类，解析操作都是通过该类实例进行。</p>
<p>5) JSON字符串转换为对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">String json &#x3D; &quot;&#123;\&quot;name\&quot;:\&quot;张三\&quot;,\&quot;age\&quot;:18&#125;&quot;;</span><br><span class="line">Gson gson &#x3D; new Gson();</span><br><span class="line">&#x2F;&#x2F;转换为集合</span><br><span class="line">Map&lt;String,Object&gt; stuMap &#x3D; gson.fromJson(json, Map.class);</span><br><span class="line">&#x2F;&#x2F;如果编写了相应的类也可以转换为指定对象</span><br><span class="line">Student fromJson &#x3D; gson.fromJson(json, Student.class);</span><br></pre></td></tr></table></figure>

<p>6)  对象转换为JSON字符串</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Student stu = <span class="keyword">new</span> Student(<span class="string">"李四"</span>, <span class="number">23</span>);</span><br><span class="line">Gson gson = <span class="keyword">new</span> Gson();</span><br><span class="line"><span class="comment">//&#123;"name":"李四","age":23&#125;</span></span><br><span class="line">String json = gson.toJson(stu);</span><br><span class="line">		</span><br><span class="line">Map&lt;String , Object&gt; map = <span class="keyword">new</span> HashMap&lt;String, Object&gt;();</span><br><span class="line">map.put(<span class="string">"name"</span>, <span class="string">"孙悟空"</span>);</span><br><span class="line">map.put(<span class="string">"age"</span>, <span class="number">30</span>);</span><br><span class="line"><span class="comment">//&#123;"age":30,"name":"孙悟空"&#125;</span></span><br><span class="line">String json2 = gson.toJson(map);</span><br><span class="line">		</span><br><span class="line">List&lt;Student&gt; list = <span class="keyword">new</span> ArrayList&lt;Student&gt;();</span><br><span class="line">list.add(<span class="keyword">new</span> Student(<span class="string">"八戒"</span>, <span class="number">18</span>));</span><br><span class="line">list.add(<span class="keyword">new</span> Student(<span class="string">"沙僧"</span>, <span class="number">28</span>));</span><br><span class="line">list.add(<span class="keyword">new</span> Student(<span class="string">"唐僧"</span>, <span class="number">38</span>));</span><br><span class="line"><span class="comment">//[&#123;"name":"八戒","age":18&#125;,&#123;"name":"沙僧","age":28&#125;,</span></span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"唐僧"</span>,<span class="string">"age"</span>:<span class="number">38</span>&#125;]</span><br><span class="line">String json3 = gson.toJson(list);		</span><br><span class="line">     <span class="comment">// 如果将一个数组格式的json字符串转换成java对象需要用到</span></span><br><span class="line">     <span class="comment">//Gson提供的一个匿名内部类： TypeToken</span></span><br><span class="line">	TypeToken tk= <span class="keyword">new</span> TypeToken&lt;List&lt;User&gt;&gt;()&#123;&#125;;</span><br><span class="line">	List&lt;User&gt; list2 = gson.fromJson(json,tk.getType());</span><br><span class="line">	System.out.println(list2.get(<span class="number">0</span>));</span><br></pre></td></tr></table></figure>



<h2 id="JQuery-异步请求返回JSON数据"><a href="#JQuery-异步请求返回JSON数据" class="headerlink" title="JQuery 异步请求返回JSON数据"></a>JQuery 异步请求返回JSON数据</h2><p>1)  Servlet 返回json数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doGet</span><span class="params">(HttpServletRequest request, HttpServletResponse response)</span> <span class="keyword">throws</span> ServletException, IOException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		List&lt;Employee&gt; emps = <span class="keyword">new</span> EmployeeDaoJdbcImpl().getAllEmps();</span><br><span class="line">		Gson gson = <span class="keyword">new</span> Gson();</span><br><span class="line">		String jsonStr = gson.toJson(emps);</span><br><span class="line">		response.setContentType(<span class="string">"text/html;charset=utf-8"</span>);</span><br><span class="line">		PrintWriter out = response.getWriter();</span><br><span class="line">		out.println(jsonStr);</span><br><span class="line">		out.close();</span><br></pre></td></tr></table></figure>

<p>2)  页面中处理 json数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">getJsonStr</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="comment">//通过JQuery发送异步请求， 将所有的员工信息通过json的格式返回</span></span><br><span class="line"> $.ajax(&#123;</span><br><span class="line">	url:<span class="string">"getEmpsJsonStr"</span>,</span><br><span class="line">	type:<span class="string">"post"</span>,</span><br><span class="line">	dataType:<span class="string">"json"</span>,</span><br><span class="line">	success:function(data)&#123;  <span class="comment">// 会直接将后台返回的json字符串转换成js对象</span></span><br><span class="line">		<span class="keyword">var</span> str = <span class="string">"&lt;tr&gt;&lt;th&gt;Id&lt;/th&gt;&lt;th&gt;LastName&lt;/th&gt;&lt;th&gt;Email&lt;/th&gt;&lt;th&gt;Gender&lt;/th&gt;&lt;/tr&gt;"</span>;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">var</span> i= <span class="number">0</span> ;i &lt;data.length;i++)&#123;</span><br><span class="line">			<span class="keyword">var</span> emp = data[i];</span><br><span class="line">			str+=<span class="string">"&lt;tr align='center'&gt;&lt;td&gt;"</span></span><br><span class="line">                 +emp.id+</span><br><span class="line">                 <span class="string">"&lt;/td&gt;&lt;td&gt;"</span></span><br><span class="line">                 +emp.lastName+</span><br><span class="line">                 <span class="string">"&lt;/td&gt;&lt;td&gt;"</span></span><br><span class="line">                 +emp.email+</span><br><span class="line">                 <span class="string">"&lt;/td&gt;&lt;td&gt;"</span></span><br><span class="line">                 +emp.gender+</span><br><span class="line">                 <span class="string">"&lt;/td&gt;&lt;/tr&gt;"</span>	</span><br><span class="line">		&#125;	</span><br><span class="line">				$(<span class="string">"#tb"</span>).html(str);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

</script></p>]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>javaweb</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka基础（一）</title>
    <url>/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Kafka概述"><a href="#Kafka概述" class="headerlink" title="Kafka概述"></a>Kafka概述</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Kafka是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。</p>
<h2 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h2><p>1.2.1传统消息队列的应用场景</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps78.png" alt="img"></p>
<p>使用消息队列的好处</p>
<p>1）解耦</p>
<p>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p>
<p>2）可恢复性</p>
<p>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p>
<p>3）缓冲</p>
<p>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p>
<p>4）灵活性 &amp; 峰值处理能力</p>
<p>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p>
<p>5）异步通信</p>
<p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p>
<p>1.2.2消息队列的两种模式</p>
<p>（1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）</p>
<p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。</p>
<p>消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/image-20200923160439670.png" alt="image-20200923160439670"></p>
<p>（2）发布/订阅模式（一对多，消费者消费数据之后不会清除消息）</p>
<p>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/image-20200923160453224.png" alt="image-20200923160453224"></p>
<h2 id="Kafka基础架构"><a href="#Kafka基础架构" class="headerlink" title="Kafka基础架构"></a>Kafka基础架构</h2><p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps91.png" alt="img"></p>
<p>1）Producer ：消息生产者，就是向kafka broker发消息的客户端；</p>
<p>2）Consumer ：消息消费者，向kafka broker取消息的客户端；</p>
<p>3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p>
<p>4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</p>
<p>5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</p>
<p>6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</p>
<p>7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</p>
<p>8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</p>
<p>9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</p>
<h1 id="Kafka快速入门"><a href="#Kafka快速入门" class="headerlink" title="Kafka快速入门"></a>Kafka快速入门</h1><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><p>jar包下载</p>
<p><a href="http://kafka.apache.org/downloads.html" target="_blank" rel="noopener">http://kafka.apache.org/downloads.html</a></p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/image-20200923160526719.png" alt="image-20200923160526719"></p>
<p>2.1.3集群部署</p>
<p>1）解压安装包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 software]$ tar -zxvf kafka_2.11-0.11.0.0.tgz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure>

<p>2）修改解压后的文件名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 module]$ mv kafka_2.11-0.11.0.0&#x2F; kafka</span><br></pre></td></tr></table></figure>

<p>3）在/opt/module/kafka目录下创建logs文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ mkdir logs</span><br></pre></td></tr></table></figure>

<p>4）修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ cd config&#x2F;</span><br><span class="line">[red@hadoop102 config]$ vi server.properties</span><br></pre></td></tr></table></figure>

<p>输入以下内容：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复</span></span><br><span class="line"></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#删除topic功能使能</span></span><br><span class="line"></span><br><span class="line"><span class="meta">delete.topic.enable</span>=<span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line"></span><br><span class="line"><span class="meta">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用来处理磁盘IO的线程数量</span></span><br><span class="line"></span><br><span class="line"><span class="meta">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line"></span><br><span class="line"><span class="meta">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line"></span><br><span class="line"><span class="meta">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line"></span><br><span class="line"><span class="meta">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#kafka运行日志存放的路径</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log.dirs</span>=<span class="string">/opt/module/kafka/logs</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#topic在当前broker上的分区个数</span></span><br><span class="line"></span><br><span class="line"><span class="meta">num.partitions</span>=<span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用来恢复和清理data下数据的线程数量</span></span><br><span class="line"></span><br><span class="line"><span class="meta">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置连接Zookeeper集群地址</span></span><br><span class="line"></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">hadoop102:2181,hadoop103:2181,hadoop104:2181</span></span><br></pre></td></tr></table></figure>

<p>5）配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 module]$ sudo vi &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line"></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 module]$ source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>

<p>6）分发安装包</p>
<p>​    注意：分发之后记得配置其他机器的环境变量</p>
<p>7）分别在hadoop103和hadoop104上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p>
<p>​    注：broker.id不得重复</p>
<p>8）启动集群</p>
<p>依次在hadoop102、hadoop103、hadoop104节点上启动kafka</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line">[red@hadoop103 kafka]$ bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line">[red@hadoop104 kafka]$ bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br></pre></td></tr></table></figure>

<p>9）关闭集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-server-stop.sh stop</span><br><span class="line"></span><br><span class="line">[red@hadoop103 kafka]$ bin&#x2F;kafka-server-stop.sh stop</span><br><span class="line"></span><br><span class="line">[red@hadoop104 kafka]$ bin&#x2F;kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure>

<h2 id="Kafka命令行操作"><a href="#Kafka命令行操作" class="headerlink" title="Kafka命令行操作"></a>Kafka命令行操作</h2><p>1）查看当前服务器中的所有topic</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --list</span><br></pre></td></tr></table></figure>

<p>2）创建topic</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first</span><br></pre></td></tr></table></figure>

<p>选项说明：</p>
<blockquote>
<p>–topic 定义topic名</p>
<p>–replication-factor  定义副本数</p>
<p>–partitions  定义分区数</p>
</blockquote>
<p>3）删除topic</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first</span><br></pre></td></tr></table></figure>

<p>需要server.properties中设置delete.topic.enable=true否则只是标记删除。</p>
<p>4）发送消息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-console-producer.sh --broker-list hadoop102:9092 --topic first</span><br><span class="line"></span><br><span class="line">&gt;hello world</span><br><span class="line"></span><br><span class="line">&gt;red  red</span><br></pre></td></tr></table></figure>

<p>5）消费消息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line"></span><br><span class="line">--zookeeper hadoop102:2181 --topic first</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line"></span><br><span class="line">--bootstrap-server hadoop102:9092 --topic first</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line"></span><br><span class="line">--bootstrap-server hadoop102:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>

<blockquote>
<p>–from-beginning：会把主题中以往所有的数据都读取出来。</p>
</blockquote>
<p>6）查看某个Topic的详情</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first</span><br></pre></td></tr></table></figure>

<p>7）修改分区数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6</span><br></pre></td></tr></table></figure>



<h1 id="Kafka架构深入"><a href="#Kafka架构深入" class="headerlink" title="Kafka架构深入"></a>Kafka架构深入</h1><h2 id="Kafka工作流程及文件存储机制"><a href="#Kafka工作流程及文件存储机制" class="headerlink" title="Kafka工作流程及文件存储机制"></a>Kafka工作流程及文件存储机制</h2><p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps92.png" alt="img"></p>
<p>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p>
<p>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps118.png" alt="img"></p>
<p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p>
<blockquote>
<p>00000000000000000000.index</p>
<p>00000000000000000000.log</p>
<p>00000000000000170410.index</p>
<p>00000000000000170410.log</p>
<p>00000000000000239430.index</p>
<p>00000000000000239430.log</p>
</blockquote>
<p>index和log文件以当前segment的第一条消息的offset命名。下图为index文件和log文件的结构示意图。</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps133.png" alt="img"></p>
<p>“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</p>
<h2 id="Kafka生产者"><a href="#Kafka生产者" class="headerlink" title="Kafka生产者"></a>Kafka生产者</h2><h3 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h3><p>1）分区的原因</p>
<p>（1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p>
<p>（2）可以提高并发，因为可以以Partition为单位读写了。</p>
<p>2）分区的原则</p>
<p>我们需要将producer发送的数据封装成一个ProducerRecord对象。</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/image-20200923161400221.png" alt="image-20200923161400221"></p>
<p>（1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值；</p>
<p>（2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；</p>
<p>（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。</p>
<h3 id="数据可靠性保证"><a href="#数据可靠性保证" class="headerlink" title="数据可靠性保证"></a>数据可靠性保证</h3><p>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps134.png" alt="img"></p>
<p>1）副本数据同步策略</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>半数以上完成同步，就发送ack</td>
<td>延迟低</td>
<td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td>
</tr>
<tr>
<td>全部完成同步，才发送ack</td>
<td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td>
<td>延迟高</td>
</tr>
</tbody></table>
<p>Kafka选择了第二种方案，原因如下：</p>
<p>1.同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p>
<p>2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p>
<p>2）ISR</p>
<p>​    采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p>
<p>​    Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</p>
<p>3）ack应答机制</p>
<p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p>
<p>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p>
<p>acks参数配置：</p>
<p>acks：</p>
<p>0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；</p>
<p>1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps135.png" alt="img"></p>
<p>-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps136.png" alt="img"></p>
<p>4）故障处理细节</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps137.png" alt="img"></p>
<p>LEO：指的是每个副本最大的offset；</p>
<p>HW：指的是消费者能见到的最大的offset，ISR队列中最小的LEO。</p>
<p>（1）follower故障</p>
<p>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</p>
<p>（2）leader故障</p>
<p>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</p>
<p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>
<h2 id="Kafka消费者"><a href="#Kafka消费者" class="headerlink" title="Kafka消费者"></a>Kafka消费者</h2><h3 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h3><p>consumer采用pull（拉）模式从broker中读取数据。</p>
<p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p>
<p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</p>
<h3 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a>分区分配策略</h3><p>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</p>
<p>Kafka有两种分配策略，一是RoundRobin，一是Range。</p>
<p>1）RoundRobin</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps138.png" alt="img"></p>
<p>2）Range</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps139.png" alt="img"></p>
<h3 id="offset的维护"><a href="#offset的维护" class="headerlink" title="offset的维护"></a>offset的维护</h3><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p>
<p> <img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/image-20200923161454891.png" alt="image-20200923161454891"></p>
<p>Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</p>
<p>1）修改配置文件consumer.properties</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">exclude.internal.topics</span>=<span class="string">false</span></span><br></pre></td></tr></table></figure>

<p>2）读取offset</p>
<p>0.11.0.0之前版本:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter&quot; --consumer.config config&#x2F;consumer.properties --from-beginning</span><br></pre></td></tr></table></figure>

<p>0.11.0.0之后版本(含):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --consumer.config config&#x2F;consumer.properties --from-beginning</span><br></pre></td></tr></table></figure>



<h3 id="消费者组案例"><a href="#消费者组案例" class="headerlink" title="消费者组案例"></a>消费者组案例</h3><p>1）需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。</p>
<p>2）案例实操</p>
<p>​    （1）在hadoop102、hadoop103上修改/opt/module/kafka/config/consumer.properties配置文件中的group.id属性为任意组名。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop103 config]$ vi consumer.properties</span><br><span class="line"></span><br><span class="line">group.id&#x3D;red</span><br></pre></td></tr></table></figure>

<p>​    （2）在hadoop102、hadoop103上分别启动消费者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line"></span><br><span class="line">--zookeeper hadoop102:2181 --topic first --consumer.config config&#x2F;consumer.properties</span><br><span class="line"></span><br><span class="line">[red@hadoop103 kafka]$ bin&#x2F;kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first --consumer.config config&#x2F;consumer.properties</span><br></pre></td></tr></table></figure>

<p>​    （3）在hadoop104上启动生产者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop104 kafka]$ bin&#x2F;kafka-console-producer.sh \</span><br><span class="line"></span><br><span class="line">--broker-list hadoop102:9092 --topic first</span><br><span class="line"></span><br><span class="line">&gt;hello world</span><br></pre></td></tr></table></figure>

<p>​    （4）查看hadoop102和hadoop103的接收者。</p>
<p>​     同一时刻只有一个消费者接收到消息。</p>
<h2 id="Kafka-高效读写数据"><a href="#Kafka-高效读写数据" class="headerlink" title="Kafka 高效读写数据"></a>Kafka 高效读写数据</h2><p>1）顺序写磁盘</p>
<p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p>
<p>2）零复制技术</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps140.png" alt="img"></p>
<h2 id="Zookeeper在Kafka中的作用"><a href="#Zookeeper在Kafka中的作用" class="headerlink" title="Zookeeper在Kafka中的作用"></a>Zookeeper在Kafka中的作用</h2><p>Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p>
<p>Controller的管理工作都是依赖于Zookeeper的。</p>
<p>​    以下为partition的leader选举过程：</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps152.png" alt="img"></p>
<h1 id="Kafka-API"><a href="#Kafka-API" class="headerlink" title="Kafka API"></a>Kafka API</h1><h2 id="Producer-API"><a href="#Producer-API" class="headerlink" title="Producer API"></a>Producer API</h2><h3 id="消息发送流程"><a href="#消息发送流程" class="headerlink" title="消息发送流程"></a>消息发送流程</h3><p>Kafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps179.png" alt="img"></p>
<p>相关参数：</p>
<p>batch.size：只有数据积累到batch.size之后，sender才会发送数据。</p>
<p>linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p>
<h3 id="异步发送API"><a href="#异步发送API" class="headerlink" title="异步发送API"></a>异步发送API</h3><p>1）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编写代码</p>
<p>需要用到的类：</p>
<blockquote>
<p>KafkaProducer：需要创建一个生产者对象，用来发送数据</p>
<p>ProducerConfig：获取所需的一系列配置参数</p>
<p>ProducerRecord：每条数据都要封装成一个ProducerRecord对象</p>
</blockquote>
<p>1.不带回调函数的API</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka集群，broker-list</span></span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">//重试次数</span></span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"retries"</span>, <span class="number">1</span>); </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">//批次大小</span></span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>); </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">//等待时间</span></span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>); </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">      producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"first"</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    producer.close();</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2.带回调函数的API</p>
<p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p>
<p>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line"></span><br><span class="line">   props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line"></span><br><span class="line">   props.put(<span class="string">"retries"</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line"></span><br><span class="line">   props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line"></span><br><span class="line">   props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line"></span><br><span class="line">   props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">   props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">   Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">      producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"first"</span>, Integer.toString(i), Integer.toString(i)), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//回调函数，该方法会在Producer收到ack时调用，为异步调用</span></span><br><span class="line">       <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">           System.out.println(<span class="string">"success-&gt;"</span> + metadata.offset());</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           exception.printStackTrace();</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">   &#125;</span><br><span class="line">    producer.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h3><p>​    同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</p>
<p>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方发即可。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"retries"</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">​    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">​      producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"first"</span>, Integer.toString(i), Integer.toString(i))).get();</span><br><span class="line"></span><br><span class="line">​    &#125;</span><br><span class="line"></span><br><span class="line">​    producer.close();</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h2><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p>
<p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p>
<p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p>
<h3 id="自动提交offset"><a href="#自动提交offset" class="headerlink" title="自动提交offset"></a>自动提交offset</h3><p>1）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编写代码</p>
<p>需要用到的类：</p>
<blockquote>
<p>KafkaConsumer：需要创建一个消费者对象，用来消费数据</p>
<p>ConsumerConfig：获取所需的一系列配置参数</p>
<p>ConsuemrRecord：每条数据都要封装成一个ConsumerRecord对象</p>
</blockquote>
<p><strong><font color="red">为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。</font></strong> </p>
<p>自动提交offset的相关参数：</p>
<blockquote>
<p>enable.auto.commit：是否开启自动提交offset功能</p>
<p>auto.commit.interval.ms：自动提交offset的时间间隔</p>
</blockquote>
<p>以下为自动提交offset的代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">​    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">​    consumer.subscribe(Arrays.asList(<span class="string">"first"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">​    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">​      ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">​      <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line"></span><br><span class="line">​        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line"></span><br><span class="line">​    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h3><p>虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</p>
<p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p>
<p>1）同步提交offset</p>
<p>由于同步提交offset有失败重试机制，故更加可靠，以下为同步提交offset的示例。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka.consumer;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomComsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//Kafka集群</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>); </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>); </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);<span class="comment">//关闭自动提交offset</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    consumer.subscribe(Arrays.asList(<span class="string">"first"</span>));<span class="comment">//消费者订阅主题</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//消费者拉取数据</span></span><br><span class="line"></span><br><span class="line">​      ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>); </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​      <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​      &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//同步提交，当前线程会阻塞直到offset提交成功</span></span><br><span class="line"></span><br><span class="line">​      consumer.commitSync();</span><br><span class="line"></span><br><span class="line">​    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）异步提交offset</p>
<p>虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</p>
<p>以下为异步提交offset的示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka.consumer;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    <span class="comment">//Kafka集群</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>); </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    <span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>); </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    <span class="comment">//关闭自动提交offset</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">​    consumer.subscribe(Arrays.asList(<span class="string">"first"</span>));<span class="comment">//消费者订阅主题</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">​      ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据</span></span><br><span class="line"></span><br><span class="line">​      <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line"></span><br><span class="line">​        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line"></span><br><span class="line">​      &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//异步提交</span></span><br><span class="line"></span><br><span class="line">​      consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line"></span><br><span class="line">​        <span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​          <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;</span><br><span class="line"></span><br><span class="line">​            System.err.println(<span class="string">"Commit failed for"</span> + offsets);</span><br><span class="line"></span><br><span class="line">​          &#125;</span><br><span class="line"></span><br><span class="line">​        &#125;</span><br><span class="line"></span><br><span class="line">​      &#125;); </span><br><span class="line"></span><br><span class="line">​    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>3）</strong> 数据漏消费和重复消费分析</p>
<p>无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。</p>
<h3 id="自定义存储offset"><a href="#自定义存储offset" class="headerlink" title="自定义存储offset"></a>自定义存储offset</h3><p>Kafka 0.9版本之前，offset存储在zookeeper，0.9版本及之后，默认将offset存储在Kafka的一个内置的topic中。除此之外，Kafka还可以选择自定义存储offset。</p>
<p>offset的维护是相当繁琐的，因为需要考虑到消费者的Rebalace。</p>
<p>当有新的消费者加入消费者组、已有的消费者推出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做Rebalance。</p>
<p>消费者发生Rebalance之后，每个消费者消费的分区就会发生变化。因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的offset位置继续消费。</p>
<p>要实现自定义存储offset，需要借助ConsumerRebalanceListener，以下为示例代码，其中提交和获取offset的方法，需要根据所选的offset存储系统自行实现。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka.consumer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;TopicPartition, Long&gt; currentOffset = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//创建配置信息</span></span><br><span class="line"></span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line"><span class="comment">//Kafka集群</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//关闭自动提交offset</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line"></span><br><span class="line">​    <span class="comment">//Key和Value的反序列化类</span></span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">​    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">​    <span class="comment">//创建一个消费者</span></span><br><span class="line"></span><br><span class="line">​    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">​    <span class="comment">//消费者订阅主题</span></span><br><span class="line"></span><br><span class="line">​    consumer.subscribe(Arrays.asList(<span class="string">"first"</span>), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​      <span class="comment">//该方法会在Rebalance之前调用</span></span><br><span class="line"></span><br><span class="line">​      <span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​        commitOffset(currentOffset);</span><br><span class="line"></span><br><span class="line">​      &#125;</span><br><span class="line"></span><br><span class="line">​      <span class="comment">//该方法会在Rebalance之后调用</span></span><br><span class="line"></span><br><span class="line">​      <span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​        currentOffset.clear();</span><br><span class="line"></span><br><span class="line">​        <span class="keyword">for</span> (TopicPartition partition : partitions) &#123;</span><br><span class="line"></span><br><span class="line">​          consumer.seek(partition, getOffset(partition));<span class="comment">//定位到最近提交的offset位置继续消费</span></span><br><span class="line"></span><br><span class="line">​        &#125;</span><br><span class="line"></span><br><span class="line">​      &#125;</span><br><span class="line"></span><br><span class="line">​    &#125;);</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">​      ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据</span></span><br><span class="line"></span><br><span class="line">​      <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line"></span><br><span class="line">​        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line"></span><br><span class="line">​        currentOffset.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.partition()), record.offset());</span><br><span class="line"></span><br><span class="line">​      &#125;</span><br><span class="line"></span><br><span class="line">​      commitOffset(currentOffset);<span class="comment">//异步提交</span></span><br><span class="line"></span><br><span class="line">​    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取某分区的最新offset</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getOffset</span><span class="params">(TopicPartition partition)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//提交该消费者所有分区的offset</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">commitOffset</span><span class="params">(Map&lt;TopicPartition, Long&gt; currentOffset)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="自定义Interceptor"><a href="#自定义Interceptor" class="headerlink" title="自定义Interceptor"></a>自定义Interceptor</h2><h3 id="拦截器原理"><a href="#拦截器原理" class="headerlink" title="拦截器原理"></a>拦截器原理</h3><p>Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。</p>
<p>对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：</p>
<p>（1）configure(configs)</p>
<p>获取配置信息和初始化数据时调用。</p>
<p>（2）onSend(ProducerRecord)：</p>
<p>该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。</p>
<p>（3）onAcknowledgement(RecordMetadata, Exception)：</p>
<p>该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用。并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。</p>
<p>（4）close：</p>
<p>关闭interceptor，主要用于执行一些资源清理工作</p>
<p>如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。</p>
<h3 id="拦截器案例"><a href="#拦截器案例" class="headerlink" title="拦截器案例"></a>拦截器案例</h3><p>1）需求：</p>
<p>实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</p>
<p>2）案例实操</p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/wps200.png" alt="img"></p>
<p>（1）增加时间戳拦截器</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​		<span class="comment">// 创建一个新的record，把时间戳写入消息体的最前部</span></span><br><span class="line"></span><br><span class="line">​		<span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(),</span><br><span class="line"></span><br><span class="line">​				System.currentTimeMillis() + <span class="string">","</span> + record.value().toString());</span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CounterInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> errorCounter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> successCounter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​		</span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​		 <span class="keyword">return</span> record;</span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​		<span class="comment">// 统计成功和失败的次数</span></span><br><span class="line"></span><br><span class="line">​    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line"></span><br><span class="line">​      successCounter++;</span><br><span class="line"></span><br><span class="line">​    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">​      errorCounter++;</span><br><span class="line"></span><br><span class="line">​    &#125;</span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">​    <span class="comment">// 保存结果</span></span><br><span class="line"></span><br><span class="line">​    System.out.println(<span class="string">"Successful sent: "</span> + successCounter);</span><br><span class="line"></span><br><span class="line">​    System.out.println(<span class="string">"Failed sent: "</span> + errorCounter);</span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）producer主程序</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InterceptorProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">​	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">​		<span class="comment">// 1 设置配置信息</span></span><br><span class="line"></span><br><span class="line">​		Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">​		props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);</span><br><span class="line"></span><br><span class="line">​		props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line"></span><br><span class="line">​		props.put(<span class="string">"retries"</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">​		props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">​		props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">​		props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">​		props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">​		props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">​		</span><br><span class="line"></span><br><span class="line">​		<span class="comment">// 2 构建拦截链</span></span><br><span class="line"></span><br><span class="line">​		List&lt;String&gt; interceptors = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">​		interceptors.add(<span class="string">"com.red.kafka.interceptor.TimeInterceptor"</span>); 	interceptors.add(<span class="string">"com.red.kafka.interceptor.CounterInterceptor"</span>); </span><br><span class="line"></span><br><span class="line">​		props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</span><br><span class="line"></span><br><span class="line">​		 </span><br><span class="line"></span><br><span class="line">​		String topic = <span class="string">"first"</span>;</span><br><span class="line"></span><br><span class="line">​		Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">​		</span><br><span class="line"></span><br><span class="line">​		<span class="comment">// 3 发送消息</span></span><br><span class="line"></span><br><span class="line">​		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">​			</span><br><span class="line"></span><br><span class="line">​		  ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(topic, <span class="string">"message"</span> + i);</span><br><span class="line"></span><br><span class="line">​		  producer.send(record);</span><br><span class="line"></span><br><span class="line">​		&#125;</span><br><span class="line"></span><br><span class="line">​		 </span><br><span class="line"></span><br><span class="line">​		<span class="comment">// 4 一定要关闭producer，这样才会调用interceptor的close方法</span></span><br><span class="line"></span><br><span class="line">​		producer.close();</span><br><span class="line"></span><br><span class="line">​	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）测试</p>
<p>（1）在kafka上启动消费者，然后运行客户端java程序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka]$ bin&#x2F;kafka-console-consumer.sh \</span><br><span class="line"></span><br><span class="line">--bootstrap-server hadoop102:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>

<blockquote>
<p>1501904047034,message0</p>
<p>1501904047225,message1</p>
<p>1501904047230,message2</p>
<p>1501904047234,message3</p>
<p>1501904047236,message4</p>
<p>1501904047240,message5</p>
<p>1501904047243,message6</p>
<p>1501904047246,message7</p>
<p>1501904047249,message8</p>
<p>1501904047252,message9</p>
</blockquote>
<h1 id="Kafka监控"><a href="#Kafka监控" class="headerlink" title="Kafka监控"></a>Kafka监控</h1><h2 id="KafkaManager"><a href="#KafkaManager" class="headerlink" title="KafkaManager"></a>KafkaManager</h2><p>1.上传压缩包kafka-manager-1.3.3.15.zip到集群</p>
<p>2.解压到/opt/module</p>
<p>3.修改配置文件conf/application.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kafka-manager.zkhosts="kafka-manager-zookeeper:2181"</span><br></pre></td></tr></table></figure>

<p>修改为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kafka-manager.zkhosts="hadoop102:2181,hadoop103:2181,hadoop104:2181"</span><br></pre></td></tr></table></figure>

<p>4.启动kafka-manager</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;kafka-manager</span><br></pre></td></tr></table></figure>

<p>5.登录hadoop102:9000页面查看详细信息</p>
<h2 id="KafkaMonitor"><a href="#KafkaMonitor" class="headerlink" title="KafkaMonitor"></a>KafkaMonitor</h2><p>1.上传jar包KafkaOffsetMonitor-assembly-0.2.0.jar到集群</p>
<p>2.在/opt/module/下创建kafka-offset-console文件夹</p>
<p>3.将上传的jar包放入刚创建的目录下</p>
<p>4.在/opt/module/kafka-offset-console目录下创建启动脚本start.sh，内容如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">java -Xms512M -Xmx512M -Xss1024K -XX:PermSize=256M -XX:MaxPermSize=512M -cp KafkaOffsetMonitor-assembly-0.2.0.jar \</span><br><span class="line"></span><br><span class="line">com.quantifind.kafka.offsetapp.OffsetGetterWeb \</span><br><span class="line"></span><br><span class="line">--zk hadoop102:2181,hadoop103:2181,hadoop104:2181 \</span><br><span class="line"></span><br><span class="line">--port 8086 \</span><br><span class="line"></span><br><span class="line">--refresh 10.seconds \</span><br><span class="line"></span><br><span class="line">--retain 7.days  1&gt;mobile-logs/stdout.log  2&gt;mobile-logs/stderr.log &amp;</span><br></pre></td></tr></table></figure>

<p>5.在/opt/module/kafka-offset-console目录下创建mobile-logs文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;opt&#x2F;module&#x2F;kafka-offset-console&#x2F;mobile-logs</span><br></pre></td></tr></table></figure>

<p>6.启动KafkaMonitor</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;start.sh</span><br></pre></td></tr></table></figure>

<p>7.登录页面hadoop102:8086端口查看详情</p>
<h2 id="Kafka-Eagle"><a href="#Kafka-Eagle" class="headerlink" title="Kafka Eagle"></a>Kafka Eagle</h2><p>1.修改kafka启动命令</p>
<p>修改kafka-server-start.sh命令中</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx1G -Xms1G"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<p>为</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<p><font color="red">注意：修改之后在启动Kafka之前要分发之其他节点</font></p>
<p>2.上传压缩包kafka-eagle-bin-1.3.7.tar.gz到集群/opt/software目录</p>
<p>3.解压到本地</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 software]$ tar -zxvf kafka-eagle-bin-1.3.7.tar.gz</span><br></pre></td></tr></table></figure>

<p>4.进入刚才解压的目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka-eagle-bin-1.3.7]$ ll</span><br></pre></td></tr></table></figure>

<blockquote>
<p>总用量 82932</p>
<p>-rw-rw-r–. 1 red red 84920710 8月  13 23:00 kafka-eagle-web-1.3.7-bin.tar.gz</p>
</blockquote>
<p>5.将kafka-eagle-web-1.3.7-bin.tar.gz解压至/opt/module</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 kafka-eagle-bin-1.3.7]$ tar -zxvf kafka-eagle-web-1.3.7-bin.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure>

<p>6.修改名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 module]$ mv kafka-eagle-web-1.3.7&#x2F; eagle</span><br></pre></td></tr></table></figure>

<p>7.给启动文件执行权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 eagle]$ cd bin&#x2F;</span><br><span class="line"></span><br><span class="line">[red@hadoop102 bin]$ ll</span><br></pre></td></tr></table></figure>

<blockquote>
<p>总用量 12</p>
<p>-rw-r–r–. 1 red red 1848 8月  22 2017 ke.bat</p>
<p>-rw-r–r–. 1 red red 7190 7月  30 20:12 ke.sh</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 bin]$ chmod 777 ke.sh</span><br></pre></td></tr></table></figure>

<p>8.修改配置文件</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># multi zookeeper&amp;kafka cluster list</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="meta">kafka.eagle.zk.cluster.alias</span>=<span class="string">cluster1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">cluster1.zk.list</span>=<span class="string">hadoop102:2181,hadoop103:2181,hadoop104:2181</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka offset storage</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="meta">cluster1.kafka.eagle.offset.storage</span>=<span class="string">kafka</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># enable kafka metrics</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="meta">kafka.eagle.metrics.charts</span>=<span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="meta">kafka.eagle.sql.fix.error</span>=<span class="string">false</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka jdbc driver address</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"></span><br><span class="line"><span class="meta">kafka.eagle.driver</span>=<span class="string">com.mysql.jdbc.Driver</span></span><br><span class="line"></span><br><span class="line"><span class="meta">kafka.eagle.url</span>=<span class="string">jdbc:mysql://hadoop102:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"></span><br><span class="line"><span class="meta">kafka.eagle.username</span>=<span class="string">root</span></span><br><span class="line"></span><br><span class="line"><span class="meta">kafka.eagle.password</span>=<span class="string">000000</span></span><br></pre></td></tr></table></figure>



<p>9.添加环境变量</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">export</span> <span class="string">KE_HOME=/opt/module/eagle</span></span><br><span class="line"></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$KE_HOME/bin</span></span><br></pre></td></tr></table></figure>

<p>注意：source /etc/profile</p>
<p>10.启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[red@hadoop102 eagle]$ bin&#x2F;ke.sh start</span><br></pre></td></tr></table></figure>

<blockquote>
<p>… …</p>
<p>… …</p>
<p>*Kafka Eagle Service has started success.</p>
<p>* Welcome, Now you can visit ‘<a href="http://192.168.9.102:8048/ke&#39;" target="_blank" rel="noopener">http://192.168.9.102:8048/ke&#39;</a></p>
<p>* Account:admin ,Password:123456</p>
<hr>
<p>* <Usage> ke.sh [start|status|stop|restart|stats] </Usage></p>
<p>* <Usage> <a href="https://www.kafka-eagle.org/" target="_blank" rel="noopener">https://www.kafka-eagle.org/</a> </Usage></p>
</blockquote>
<p>[red@hadoop102 eagle]$</p>
<p><font color="red">注意：启动之前需要先启动ZK以及KAFKA</font></p>
<p>11.登录页面查看监控数据</p>
<p><a href="http://192.168.9.102:8048/ke" target="_blank" rel="noopener">http://192.168.9.102:8048/ke</a></p>
<p><img src="/bigdata/Kafka%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/image-20200923164007494.png" alt="image-20200923164007494"></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>消息中间件</title>
    <url>/uncategorized/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="中间件"><a href="#中间件" class="headerlink" title="中间件"></a>中间件</h1><p>非底层操作系统也非顶层业务系统，不能直接给用户使用也不能为用户带来直接价值。是一类连接软件组件和应用的计算机软件，它包括一组服务。以便于运行在一台或多台机器上的多个软件通过网络进行交互。该技术所提供的互操作性，推动了一致分布式体系架构的演进，该架构通常用于<strong>支持并简化</strong>那些复杂的分布式应用程序，它包括web服务器、事务监控器和消息队列软件。（摘抄至百度百科）</p>
<p>间件=平台+通信  </p>
<p>只有用于分布式系统中才能叫中间件，同时也把它与支撑软件和实用软件区分开来。</p>
<h2 id="基本特点"><a href="#基本特点" class="headerlink" title="基本特点"></a>基本特点</h2><p>1、满足大量应用的需要 </p>
<p>2、运行于多种硬件和OS平台  </p>
<p>3、支持分布式计算，提供跨网络、硬件和OS平台的透明性的应用或服务的交互功能  </p>
<p>4、<strong>支持标准的协议</strong>  </p>
<p>5、<strong>支持标准的接口</strong></p>
<p>（摘抄至百度百科）</p>
<h1 id="消息中间件"><a href="#消息中间件" class="headerlink" title="消息中间件"></a>消息中间件</h1><p>消息中间件是基于队列与消息传递技术，在网络环境中为应用系统提供<strong>同步或异步</strong>、可靠的消息传输的支撑性软件系统。</p>
<p> 关注于数据的发送和接受，利用高效可靠的异步消息传递机制集成分布式系统。</p>
<h2 id="JMS"><a href="#JMS" class="headerlink" title="JMS"></a>JMS</h2><p>  Java消息服务（Java Message Service）即JMS，是一个Java平台中关于面向消息中间件的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。</p>
<h2 id="AMQP"><a href="#AMQP" class="headerlink" title="AMQP"></a>AMQP</h2><p>  AMQP(advanced message queuing protocol)是一个提供统一消息服务的应用层标准协议，基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等 条件的限制。</p>
<h2 id="ActiveMQ"><a href="#ActiveMQ" class="headerlink" title="ActiveMQ"></a>ActiveMQ</h2><p>  ActiveMQ是Apache公司出品，最流行的，能力强劲的开源消息总线。ActiveMQ是一个完全支持JMS1.1和J2EE1.4规范的JMS Provider实现，尽管JMS规范出台已经是很久的事情了，但是JMS在当今的J2EE应用中间仍然扮演者特殊的地位。</p>
<p> 特性：</p>
<p>  多种语言和协议编写客户端。语言：Java,C,C++,C#,Rubt,Perl,Python,PHP。应用协议：OpenWire,Stomp REST,WS Notification,XMPP,AMQP</p>
<p>  完全支持JMS1.1和J2EE 1.4规范（持久化，XA消息，事务）</p>
<p>  虚拟主题、组合目的、镜像队列</p>
<p>  消息持久化</p>
<h2 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h2><p>  RabbitMQ是一个开源的AMQP实现，服务端采用Erlang语言编写。用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。</p>
<p> 特性：</p>
<p>  支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript等</p>
<p>  AMQP的完整实现（vhost、Exchange、Binding、Routing Key等）</p>
<p>  事务支持/发布确认</p>
<p>  消息持久化</p>
<p>  主要用于金融行业，对安全性稳定性要求极高！</p>
<h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><p><img src="/uncategorized/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/20190416142714631.png" alt="img"></p>
<h1 id="JMS相关概念"><a href="#JMS相关概念" class="headerlink" title="JMS相关概念"></a><strong>JMS相关概念</strong></h1><p> 提供者：实现JMS规范的消息中间件服务器</p>
<p> 客户端：发送或接收消息的应用程序</p>
<p> 生产者/发布者：创建并发送消息的客户端</p>
<p> 消费者/订阅者：接收并处理消息的客户端</p>
<p> 消息：应用程序之间传递的数据内容</p>
<p> 消息模式：在客户端之间传递消息的方式，JMS中定义了主题和队列两种模式</p>
<p><strong>队列模式：</strong></p>
<p> 客户端包括生产者和消费者</p>
<p> 队列中的消息只能被一个消费者消费</p>
<p> 消费者可以随时消费队列中的消息</p>
<p><img src="/uncategorized/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/20190416143704996.png" alt="img"></p>
<p><strong>主题模型</strong></p>
<p> 客户端包括发布者和订阅者</p>
<p> 主题中的消息被所有订阅者消费 </p>
<p> 消费者不能消费订阅之前就发送到主题中的消息</p>
<p><img src="/uncategorized/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/20190416143934195.png" alt="img"></p>
<h1 id="JMS编码接口"><a href="#JMS编码接口" class="headerlink" title="JMS编码接口"></a><strong>JMS编码接口</strong></h1><p> ConnectionFactory 用于创建连接到消息中间件的连接工厂</p>
<p> Connection 代表了应用程序和消息服务器之间的通信链路</p>
<p> Destination 指消息发布和接收的地点，包括队列或主题</p>
<p> Session 表示一个但县城的上下文，用于发送和接收消息</p>
<p> MessageConsumer 由会话创建，用于接收发送到目标的消息</p>
<p> MessageProducer 由会话创建，用于发送消息到目标</p>
<p> Message 是在消费者和生产者之间传送的对象，消息头，一组消息属性，一个消息体</p>
]]></content>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
</search>
