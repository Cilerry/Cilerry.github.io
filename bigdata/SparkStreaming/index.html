<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>SparkStreaming | RedLeavesBlog</title><meta name="keywords" content="SparkStreaming"><meta name="author" content="Snow Monster"><meta name="copyright" content="Snow Monster"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="概述Spark Streaming是微批次处理方式，批处理间隔是Spark Streaming是的核心概念和关键参数。 Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行 Spark Streaming是什么Spark流使得构建可扩展的容错流应用程序变得更加容易。 Spark Streaming用于流式数据的处理。Spark S">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkStreaming">
<meta property="og:url" content="http://www.red0819.top/bigdata/SparkStreaming/index.html">
<meta property="og:site_name" content="RedLeavesBlog">
<meta property="og:description" content="概述Spark Streaming是微批次处理方式，批处理间隔是Spark Streaming是的核心概念和关键参数。 Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行 Spark Streaming是什么Spark流使得构建可扩展的容错流应用程序变得更加容易。 Spark Streaming用于流式数据的处理。Spark S">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://www.red0819.top/wordPic/spark-logo.jpg">
<meta property="article:published_time" content="2020-03-11T08:45:00.000Z">
<meta property="article:modified_time" content="2020-09-15T17:33:57.073Z">
<meta property="article:author" content="Snow Monster">
<meta property="article:tag" content="SparkStreaming">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.red0819.top/wordPic/spark-logo.jpg"><link rel="shortcut icon" href="/img/logo.png"><link rel="canonical" href="http://www.red0819.top/bigdata/SparkStreaming/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="yandex-verification" content="{&quot;theme_color&quot;:{&quot;enable&quot;:true,&quot;main&quot;:&quot;\t#191970&quot;,&quot;paginator&quot;:&quot;#00c4b6&quot;,&quot;button_hover&quot;:&quot;#FF7242&quot;,&quot;text_selection&quot;:&quot;#00c4b6&quot;,&quot;link_color&quot;:&quot;#99a9bf&quot;,&quot;meta_color&quot;:&quot;#858585&quot;,&quot;hr_color&quot;:&quot;#A4D8FA&quot;,&quot;code_foreground&quot;:&quot;#F47466&quot;,&quot;code_background&quot;:&quot;rgba(27, 31, 35, .05)&quot;,&quot;toc_color&quot;:&quot;#00c4b6&quot;,&quot;blockquote_padding_color&quot;:&quot;#49b1f5&quot;,&quot;blockquote_background_color&quot;:&quot;#49b1f5&quot;}}"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '4.2.1',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-09-16 01:33:57'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
  }
}

var autoChangeMode = 'false'
var t = saveToLocal.get('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (saveToLocal.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="RedLeavesBlog" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/AF67D2ECB3B0F647193CD90F94294167.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#概述"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming是什么"><span class="toc-number">1.1.</span> <span class="toc-text">Spark Streaming是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming的特点"><span class="toc-number">1.2.</span> <span class="toc-text">Spark Streaming的特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming架构"><span class="toc-number">1.3.</span> <span class="toc-text">Spark Streaming架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#架构图"><span class="toc-number">1.3.1.</span> <span class="toc-text">架构图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#背压机制"><span class="toc-number">1.3.2.</span> <span class="toc-text">背压机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-1-5以前版本"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">Spark 1.5以前版本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-1-5以后版本"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">Spark 1.5以后版本</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dstream入门"><span class="toc-number">2.</span> <span class="toc-text">Dstream入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#WordCount案例实操"><span class="toc-number">2.1.</span> <span class="toc-text">WordCount案例实操</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WordCount解析"><span class="toc-number">2.2.</span> <span class="toc-text">WordCount解析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DStream创建"><span class="toc-number">3.</span> <span class="toc-text">DStream创建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD队列"><span class="toc-number">3.1.</span> <span class="toc-text">RDD队列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Queue"><span class="toc-number">3.1.1.</span> <span class="toc-text">Queue()</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#用法及说明"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">用法及说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#案例实操"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#file"><span class="toc-number">3.1.2.</span> <span class="toc-text">file()</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#用法及说明-1"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">用法及说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#案例实操-1"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自定义数据源"><span class="toc-number">3.2.</span> <span class="toc-text">自定义数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#用法及说明-2"><span class="toc-number">3.2.1.</span> <span class="toc-text">用法及说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#案例实操-2"><span class="toc-number">3.2.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka数据源"><span class="toc-number">3.3.</span> <span class="toc-text">Kafka数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#版本选型"><span class="toc-number">3.3.1.</span> <span class="toc-text">版本选型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-0-8-Receiver模式"><span class="toc-number">3.3.2.</span> <span class="toc-text">Kafka 0-8 Receiver模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-0-8-Direct模式"><span class="toc-number">3.3.3.</span> <span class="toc-text">Kafka 0-8 Direct模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#自动维护offset"><span class="toc-number">3.3.3.1.</span> <span class="toc-text">自动维护offset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#手动维护offset"><span class="toc-number">3.3.3.2.</span> <span class="toc-text">手动维护offset</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-0-10-Direct模式"><span class="toc-number">3.3.4.</span> <span class="toc-text">Kafka 0-10 Direct模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消费Kafka数据模式总结"><span class="toc-number">3.3.5.</span> <span class="toc-text">消费Kafka数据模式总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0-8"><span class="toc-number">3.3.5.1.</span> <span class="toc-text">0-8</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#ReceiverAPI"><span class="toc-number">3.3.5.1.1.</span> <span class="toc-text">ReceiverAPI</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#DirectAPI"><span class="toc-number">3.3.5.1.2.</span> <span class="toc-text">DirectAPI</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0-10-DirectAPI"><span class="toc-number">3.3.5.2.</span> <span class="toc-text">0-10 DirectAPI</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(/wordPic/spark-logo.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">RedLeavesBlog</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">SparkStreaming</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2020-03-11T08:45:00.000Z" title="undefined 2020-03-11 16:45:00">2020-03-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/bigdata/">大数据</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>28分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p><img src="/2020/03/11/SparkStreaming/SparkStreaming-logo.jpg" alt="SparkStreaming-logo"></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Spark Streaming是微批次处理方式，批处理间隔是Spark Streaming是的核心概念和关键参数。</p>
<p>Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行</p>
<h2 id="Spark-Streaming是什么"><a href="#Spark-Streaming是什么" class="headerlink" title="Spark Streaming是什么"></a>Spark Streaming是什么</h2><p>Spark流使得构建可扩展的容错流应用程序变得更加容易。</p>
<p>Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming%E5%9B%BE.jpg" alt="SparkStreaming图"></p>
<p>和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。<strong>在内部，每个时间区间收到的数据都作为 RDD 存在，而DStream是由这些RDD所组成的序列(因此得名“离散化”)。</strong></p>
<p>离散流反义词就是连续流。</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming%E5%9B%BE%E8%A7%A3.png" alt="SparkStreaming图解"></p>
<h2 id="Spark-Streaming的特点"><a href="#Spark-Streaming的特点" class="headerlink" title="Spark Streaming的特点"></a>Spark Streaming的特点</h2><p><strong>易用</strong></p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming5.jpg" alt="SparkStreaming5"></p>
<p><strong>容错</strong></p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming6.jpg" alt="SparkStreaming6"></p>
<p> <strong>易整合到Spark体系</strong><img src="/2020/03/11/SparkStreaming/SparkStreaming7.jpg" alt="SparkStreaming7"></p>
<h2 id="Spark-Streaming架构"><a href="#Spark-Streaming架构" class="headerlink" title="Spark Streaming架构"></a>Spark Streaming架构</h2><p>最基本的架构：底层就是spark-core。数据采集和封装之后传给Driver，Driver拿到相应的RDD，再形成一个一个Task然后传给Executor执行。。</p>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p> 整体架构图</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming8.jpg" alt="SparkStreaming8"></p>
<p>​                                        spark1.5之前</p>
<p><img src="/2020/03/11/SparkStreaming/spark1.5%E4%B9%8B%E5%90%8E%E6%9E%B6%E6%9E%84.png" alt="spark1.5之后架构"></p>
<p>​                                       spark1.5之后</p>
<p>SparkStreaming架构图</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming9.jpg" alt="SparkStreaming9"></p>
<h3 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h3><p>背压(back pressure)机制主要用于解决流处理系统中，业务流量在短时间内剧增，造成巨大的流量毛刺，数据流入速度远高于数据处理速度，对流处理系统构成巨大的负载压力的问题。</p>
<p>如果不能处理流量毛刺或者持续的数据过高速率输入，可能导致Executor端出现OOM的情况或者任务崩溃。</p>
<h4 id="Spark-1-5以前版本"><a href="#Spark-1-5以前版本" class="headerlink" title="Spark 1.5以前版本"></a>Spark 1.5以前版本</h4><p>用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现（限制每个receiver没每秒最大可以接收的数据量）。此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。</p>
<p>direct-approach方式接收数据，可以配置 “spark.streaming.kafka.maxRatePerPartition”参数来限制每个kafka分区最多读取的数据量。</p>
<p>缺点：</p>
<p>​          1、实现需要进行压测，来设置最大值。参数的设置必须合理，如果集群处理能力高于配置的速率，则会造成资源的浪费。</p>
<p>​          2、参数需要手动设置，设置过后必须重启streaming服务。</p>
<h4 id="Spark-1-5以后版本"><a href="#Spark-1-5以后版本" class="headerlink" title="Spark 1.5以后版本"></a>Spark 1.5以后版本</h4><p>为了更好的协调数据接收速率与资源处理能力，1.5版本开始Spark Streaming可以动态控制数据接收速率来适配集群数据处理能力（能够根据当前数据量以及集群状态来预估下个批次最优速率）。背压机制（即Spark Streaming Backpressure）: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。</p>
<p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。</p>
<p><strong><u>以下背压机制spark1.5之后流程以及配置均摘抄自：<a href="https://blog.csdn.net/may_fly/article/details/103922862" target="_blank" rel="noopener">https://blog.csdn.net/may_fly/article/details/103922862</a></u></strong></p>
<p>新版具体流程如下：<img src="/2020/03/11/SparkStreaming/SparkStreaming3.png" alt="SparkStreaming3"></p>
<p>新版的背压机制主要通过<code>RateController</code>组件来实现。<code>RateController</code>继承了接口<code>StreamingListener</code>并实现了<code>onBatchCompleted</code>方法。</p>
<p>结合direct-approach方式的源码来理解</p>
<ol>
<li>首先创建一个kafka流。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>,<span class="type">String</span>,<span class="type">StringDecoder</span>,<span class="type">StringDecoder</span>,(<span class="type">String</span>,<span class="type">String</span>)](streamingContext, kafkaParams, getOffsets(topics,kc,kafkaParams),messageHandler)</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>

<ol>
<li>createDirectStream方法创建并返回一个DirectKafkaInputDStream对象</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create an input stream that directly pulls messages from Kafka Brokers</span></span><br><span class="line"><span class="comment">   * without using any receiver. This stream can guarantee that each message</span></span><br><span class="line"><span class="comment">   * from Kafka is included in transformations exactly once (see points below).</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Points to note:</span></span><br><span class="line"><span class="comment">   *  - No receivers: This stream does not use any receiver. It directly queries Kafka</span></span><br><span class="line"><span class="comment">   *  - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked</span></span><br><span class="line"><span class="comment">   *    by the stream itself. For interoperability with Kafka monitoring tools that depend on</span></span><br><span class="line"><span class="comment">   *    Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.</span></span><br><span class="line"><span class="comment">   *    You can access the offsets used in each batch from the generated RDDs (see</span></span><br><span class="line"><span class="comment">   *    [[org.apache.spark.streaming.kafka.HasOffsetRanges]]).</span></span><br><span class="line"><span class="comment">   *  - Failure Recovery: To recover from driver failures, you have to enable checkpointing</span></span><br><span class="line"><span class="comment">   *    in the `StreamingContext`. The information on consumed offset can be</span></span><br><span class="line"><span class="comment">   *    recovered from the checkpoint. See the programming guide for details (constraints, etc.).</span></span><br><span class="line"><span class="comment">   *  - End-to-end semantics: This stream ensures that every records is effectively received and</span></span><br><span class="line"><span class="comment">   *    transformed exactly once, but gives no guarantees on whether the transformed data are</span></span><br><span class="line"><span class="comment">   *    outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure</span></span><br><span class="line"><span class="comment">   *    that the output operation is idempotent, or use transactions to output records atomically.</span></span><br><span class="line"><span class="comment">   *    See the programming guide for more details.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param ssc StreamingContext object</span></span><br><span class="line"><span class="comment">   * @param kafkaParams Kafka &lt;a href="http://kafka.apache.org/documentation.html#configuration"&gt;</span></span><br><span class="line"><span class="comment">   *    configuration parameters&lt;/a&gt;. Requires "metadata.broker.list" or "bootstrap.servers"</span></span><br><span class="line"><span class="comment">   *    to be set with Kafka broker(s) (NOT zookeeper servers) specified in</span></span><br><span class="line"><span class="comment">   *    host1:port1,host2:port2 form.</span></span><br><span class="line"><span class="comment">   * @param fromOffsets Per-topic/partition Kafka offsets defining the (inclusive)</span></span><br><span class="line"><span class="comment">   *    starting point of the stream</span></span><br><span class="line"><span class="comment">   * @param messageHandler Function for translating each message and metadata into the desired type</span></span><br><span class="line"><span class="comment">   * @tparam K type of Kafka message key</span></span><br><span class="line"><span class="comment">   * @tparam V type of Kafka message value</span></span><br><span class="line"><span class="comment">   * @tparam KD type of Kafka message key decoder</span></span><br><span class="line"><span class="comment">   * @tparam VD type of Kafka message value decoder</span></span><br><span class="line"><span class="comment">   * @tparam R type returned by messageHandler</span></span><br><span class="line"><span class="comment">   * @return DStream of R</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDirectStream</span></span>[</span><br><span class="line">    <span class="type">K</span>: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">V</span>: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">KD</span> &lt;: <span class="type">Decoder</span>[<span class="type">K</span>]: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">VD</span> &lt;: <span class="type">Decoder</span>[<span class="type">V</span>]: <span class="type">ClassTag</span>,</span><br><span class="line">    <span class="type">R</span>: <span class="type">ClassTag</span>] (</span><br><span class="line">      ssc: <span class="type">StreamingContext</span>,</span><br><span class="line">      kafkaParams: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">      fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>],</span><br><span class="line">      messageHandler: <span class="type">MessageAndMetadata</span>[<span class="type">K</span>, <span class="type">V</span>] =&gt; <span class="type">R</span></span><br><span class="line">  ): <span class="type">InputDStream</span>[<span class="type">R</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanedHandler = ssc.sc.clean(messageHandler)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DirectKafkaInputDStream</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">KD</span>, <span class="type">VD</span>, <span class="type">R</span>](</span><br><span class="line">      ssc, kafkaParams, fromOffsets, cleanedHandler)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152</span></span><br></pre></td></tr></table></figure>

<ol>
<li>DirectKafkaInputDStream类继承了抽象类InputDStream，并重载了rateController方法。创建了DirectKafkaRateController类，并传入了一个速率估计类。如果设置RateController.isBackPressureEnabled为true也就是开启背压则开始计算下一次的最优速率</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Asynchronously maintains &amp; sends new rate limits to the receiver through the receiver tracker.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span>[streaming] <span class="keyword">val</span> rateController: <span class="type">Option</span>[<span class="type">RateController</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">RateController</span>.isBackPressureEnabled(ssc.conf)) &#123;</span><br><span class="line">      <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">DirectKafkaRateController</span>(id,</span><br><span class="line">        <span class="type">RateEstimator</span>.create(ssc.conf, context.graph.batchDuration)))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">1234567891011</span></span><br></pre></td></tr></table></figure>

<ol>
<li>DirectKafkaRateController内部实现了一个私有类来计算速率，publish方法使用lambda表达式调用了RateController中唯一一个公有的方法onBatchCompleted获取计算结果</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A RateController to retrieve the rate from RateEstimator.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[streaming] <span class="class"><span class="keyword">class</span> <span class="title">DirectKafkaRateController</span>(<span class="params">id: <span class="type">Int</span>, estimator: <span class="type">RateEstimator</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">RateController</span>(<span class="params">id, estimator</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">publish</span></span>(rate: <span class="type">Long</span>): <span class="type">Unit</span> = ()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">1234567</span></span><br></pre></td></tr></table></figure>

<ol>
<li>onBatchCompleted获取三个时间一个数据量：处理结束时间，处理时间，等待时间，当前处理数据量，并调用computeAndPublish方法计算下次最优的数据量</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBatchCompleted</span></span>(batchCompleted: <span class="type">StreamingListenerBatchCompleted</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> elements = batchCompleted.batchInfo.streamIdToInputInfo</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">      processingEnd &lt;- batchCompleted.batchInfo.processingEndTime</span><br><span class="line">      workDelay &lt;- batchCompleted.batchInfo.processingDelay</span><br><span class="line">      waitDelay &lt;- batchCompleted.batchInfo.schedulingDelay</span><br><span class="line">      elems &lt;- elements.get(streamUID).map(_.numRecords)</span><br><span class="line">    &#125; computeAndPublish(processingEnd, elems, workDelay, waitDelay)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">12345678910</span></span><br></pre></td></tr></table></figure>

<ol>
<li>computeAndPublish调用rateEstimator.compute方法计算速率</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Compute the new rate limit and publish it asynchronously.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">computeAndPublish</span></span>(time: <span class="type">Long</span>, elems: <span class="type">Long</span>, workDelay: <span class="type">Long</span>, waitDelay: <span class="type">Long</span>): <span class="type">Unit</span> =</span><br><span class="line">    <span class="type">Future</span>[<span class="type">Unit</span>] &#123;</span><br><span class="line">      <span class="keyword">val</span> newRate = rateEstimator.compute(time, elems, workDelay, waitDelay)</span><br><span class="line">      newRate.foreach &#123; s =&gt;</span><br><span class="line">        rateLimit.set(s.toLong)</span><br><span class="line">        publish(getLatestRate())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Computes the number of records the stream attached to this `RateEstimator`</span></span><br><span class="line"><span class="comment">   * should ingest per second, given an update on the size and completion</span></span><br><span class="line"><span class="comment">   * times of the latest batch.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param time The timestamp of the current batch interval that just finished</span></span><br><span class="line"><span class="comment">   * @param elements The number of records that were processed in this batch</span></span><br><span class="line"><span class="comment">   * @param processingDelay The time in ms that took for the job to complete</span></span><br><span class="line"><span class="comment">   * @param schedulingDelay The time in ms that the job spent in the scheduling queue</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>,</span><br><span class="line">      elements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>,</span><br><span class="line">      schedulingDelay: <span class="type">Long</span>): <span class="type">Option</span>[<span class="type">Double</span>]</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627</span></span><br></pre></td></tr></table></figure>

<ol>
<li>compute方法的具体实现，需要来看3中<code>RateEstimator.create(ssc.conf, context.graph.batchDuration)))</code>传入的RateEstimator类。由源码可知，默认调用pid速率估计器，是 <code>RateEstimator</code>的唯一实现 ，具体计算逻辑要看pid速率估计器的compute方法</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RateEstimator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new `RateEstimator` based on the value of</span></span><br><span class="line"><span class="comment">   * `spark.streaming.backpressure.rateEstimator`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * The only known and acceptable estimator right now is `pid`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return An instance of RateEstimator</span></span><br><span class="line"><span class="comment">   * @throws IllegalArgumentException if the configured RateEstimator is not `pid`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(conf: <span class="type">SparkConf</span>, batchInterval: <span class="type">Duration</span>): <span class="type">RateEstimator</span> =</span><br><span class="line">    conf.get(<span class="string">"spark.streaming.backpressure.rateEstimator"</span>, <span class="string">"pid"</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"pid"</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> proportional = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.proportional"</span>, <span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">val</span> integral = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.integral"</span>, <span class="number">0.2</span>)</span><br><span class="line">        <span class="keyword">val</span> derived = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.derived"</span>, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">val</span> minRate = conf.getDouble(<span class="string">"spark.streaming.backpressure.pid.minRate"</span>, <span class="number">100</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PIDRateEstimator</span>(batchInterval.milliseconds, proportional, integral, derived, minRate)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> estimator =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Unknown rate estimator: <span class="subst">$estimator</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="number">123456789101112131415161718192021222324</span></span><br></pre></td></tr></table></figure>

<ol>
<li>pid速率估计器的compute方法如下。具体流程不再细述，有时间举个例子推一下</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      numElements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      schedulingDelay: <span class="type">Long</span> <span class="comment">// in milliseconds</span></span><br><span class="line">    ): <span class="type">Option</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    logTrace(<span class="string">s"\ntime = <span class="subst">$time</span>, # records = <span class="subst">$numElements</span>, "</span> +</span><br><span class="line">      <span class="string">s"processing time = <span class="subst">$processingDelay</span>, scheduling delay = <span class="subst">$schedulingDelay</span>"</span>)</span><br><span class="line">    <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (time &gt; latestTime &amp;&amp; numElements &gt; <span class="number">0</span> &amp;&amp; processingDelay &gt; <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in seconds, should be close to batchDuration</span></span><br><span class="line">        <span class="keyword">val</span> delaySinceUpdate = (time - latestTime).toDouble / <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> processingRate = numElements.toDouble / processingDelay * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// In our system `error` is the difference between the desired rate and the measured rate</span></span><br><span class="line">        <span class="comment">// based on the latest batch information. We consider the desired rate to be latest rate,</span></span><br><span class="line">        <span class="comment">// which is what this estimator calculated for the previous batch.</span></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> error = latestRate - processingRate</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The error integral, based on schedulingDelay as an indicator for accumulated errors.</span></span><br><span class="line">        <span class="comment">// A scheduling delay s corresponds to s * processingRate overflowing elements. Those</span></span><br><span class="line">        <span class="comment">// are elements that couldn't be processed in previous batches, leading to this delay.</span></span><br><span class="line">        <span class="comment">// In the following, we assume the processingRate didn't change too much.</span></span><br><span class="line">        <span class="comment">// From the number of overflowing elements we can calculate the rate at which they would be</span></span><br><span class="line">        <span class="comment">// processed by dividing it by the batch interval. This rate is our "historical" error,</span></span><br><span class="line">        <span class="comment">// or integral part, since if we subtracted this rate from the previous "calculated rate",</span></span><br><span class="line">        <span class="comment">// there wouldn't have been any overflowing elements, and the scheduling delay would have</span></span><br><span class="line">        <span class="comment">// been zero.</span></span><br><span class="line">        <span class="comment">// (in elements/second)</span></span><br><span class="line">        <span class="keyword">val</span> historicalError = schedulingDelay.toDouble * processingRate / batchIntervalMillis</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/(second ^ 2)</span></span><br><span class="line">        <span class="keyword">val</span> dError = (error - latestError) / delaySinceUpdate</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> newRate = (latestRate - proportional * error -</span><br><span class="line">                                    integral * historicalError -</span><br><span class="line">                                    derivative * dError).max(minRate)</span><br><span class="line">        logTrace(<span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">            | latestRate = $latestRate, error = $error</span></span><br><span class="line"><span class="string">            | latestError = $latestError, historicalError = $historicalError</span></span><br><span class="line"><span class="string">            | delaySinceUpdate = $delaySinceUpdate, dError = $dError</span></span><br><span class="line"><span class="string">            "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        latestTime = time</span><br><span class="line">        <span class="keyword">if</span> (firstRun) &#123;</span><br><span class="line">          latestRate = processingRate</span><br><span class="line">          latestError = <span class="number">0</span>D</span><br><span class="line">          firstRun = <span class="literal">false</span></span><br><span class="line">          logTrace(<span class="string">"First run, rate estimation skipped"</span>)</span><br><span class="line">          <span class="type">None</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          latestRate = newRate</span><br><span class="line">          latestError = error</span><br><span class="line">          logTrace(<span class="string">s"New rate = <span class="subst">$newRate</span>"</span>)</span><br><span class="line">          <span class="type">Some</span>(newRate)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logTrace(<span class="string">"Rate estimation skipped"</span>)</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566</span></span><br></pre></td></tr></table></figure>

<ol>
<li>虽然通过这个公式计算出了一个速率，但最终的速率并不一定是计算出的结果。由代码可知，如果设置了参数spark.streaming.kafka.maxRatePerPartition，则每个分区所取数据最大量为计算出的结果以及设置参数的最小值，否则直接使用计算出的值</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span>[streaming] <span class="function"><span class="keyword">def</span> <span class="title">maxMessagesPerPartition</span></span>(</span><br><span class="line">      offsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>]): <span class="type">Option</span>[<span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> estimatedRateLimit = rateController.map(_.getLatestRate())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate a per-partition rate limit based on current lag</span></span><br><span class="line">    <span class="keyword">val</span> effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ &gt; <span class="number">0</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(rate) =&gt;</span><br><span class="line">        <span class="keyword">val</span> lagPerPartition = offsets.map &#123; <span class="keyword">case</span> (tp, offset) =&gt;</span><br><span class="line">          tp -&gt; <span class="type">Math</span>.max(offset - currentOffsets(tp), <span class="number">0</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> totalLag = lagPerPartition.values.sum</span><br><span class="line"></span><br><span class="line">        lagPerPartition.map &#123; <span class="keyword">case</span> (tp, lag) =&gt;</span><br><span class="line">          <span class="keyword">val</span> backpressureRate = <span class="type">Math</span>.round(lag / totalLag.toFloat * rate)</span><br><span class="line">          tp -&gt; (<span class="keyword">if</span> (maxRateLimitPerPartition &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">Math</span>.min(backpressureRate, maxRateLimitPerPartition)&#125; <span class="keyword">else</span> backpressureRate)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; offsets.map &#123; <span class="keyword">case</span> (tp, offset) =&gt; tp -&gt; maxRateLimitPerPartition &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> maxRateLimitPerPartition: <span class="type">Long</span> = context.sparkContext.getConf.getLong(</span><br><span class="line">      <span class="string">"spark.streaming.kafka.maxRatePerPartition"</span>, <span class="number">0</span>)</span><br><span class="line"><span class="number">12345678910111213141516171819202122</span></span><br></pre></td></tr></table></figure>

<p>一些相关的参数：</p>
<ol>
<li>开启背压机制：设置<strong>spark.streaming.backpressure.enabled</strong> 为true，默认为false</li>
<li>启用反压机制时每个接收器接收第一批数据的初始最大速率。默认值没有设置 <strong>spark.streaming.backpressure.initialRate</strong></li>
<li>速率估算器类，默认值为 pid ，目前 Spark 只支持这个，大家可以根据自己的需要实现 <strong>spark.streaming.backpressure.rateEstimator</strong></li>
<li>用于响应错误的权重（最后批次和当前批次之间的更改）。默认值为1，只能设置成非负值。<em>weight for response to “error” (change between last batch and this batch)</em> <strong>spark.streaming.backpressure.pid.proportional</strong></li>
<li>错误积累的响应权重，具有抑制作用（有效阻尼）。默认值为 0.2 ，只能设置成非负值。<em>weight for the response to the accumulation of error. This has a dampening effect.</em> <strong>spark.streaming.backpressure.pid.integral</strong></li>
<li>对错误趋势的响应权重。 这可能会引起 batch size 的波动，可以帮助快速增加/减少容量。默认值为0，只能设置成非负值。<em>weight for the response to the trend in error. This can cause arbitrary/noise-induced fluctuations in batch size, but can also help react quickly to increased/reduced capacity.</em> <strong>spark.streaming.backpressure.pid.derived</strong></li>
<li>可以估算的最低费率是多少。默认值为 100，只能设置成非负值。 <strong>spark.streaming.backpressure.pid.minRate</strong></li>
</ol>
<p>参考：</p>
<p><a href="https://blog.csdn.net/wangpei1949/article/details/90727805" target="_blank" rel="noopener">https://blog.csdn.net/wangpei1949/article/details/90727805</a></p>
<p><a href="https://blog.csdn.net/zengxiaosen/article/details/72822869" target="_blank" rel="noopener">https://blog.csdn.net/zengxiaosen/article/details/72822869</a></p>
<p><a href="https://www.cnblogs.com/barrenlake/p/5349949.html" target="_blank" rel="noopener">https://www.cnblogs.com/barrenlake/p/5349949.html</a></p>
<p><a href="https://www.iteblog.com/archives/2323.html?from=related" target="_blank" rel="noopener">https://www.iteblog.com/archives/2323.html?from=related</a></p>
<h1 id="Dstream入门"><a href="#Dstream入门" class="headerlink" title="Dstream入门"></a>Dstream入门</h1><h2 id="WordCount案例实操"><a href="#WordCount案例实操" class="headerlink" title="WordCount案例实操"></a>WordCount案例实操</h2><p>需求：使用netcat工具向9999端口不断的发送数据，通过SparkStreaming读取端口数据并统计不同单词出现的次数</p>
<p>1) 添加依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2) 编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Wordcount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Todo 1.配置对象</span></span><br><span class="line">    <span class="comment">//初始化Spark配置信息</span></span><br><span class="line">    <span class="comment">//Spark Streaming需要单独一个节点来接收数据，所以Spark Streaming 至少需要两个节点才能运行（local至少要两个节点）</span></span><br><span class="line">    <span class="keyword">val</span> sparkconf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Todo 2.环境对象</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    class StreamingContext private[streaming]加了包权限（私有） 所以主构不能用</span></span><br><span class="line"><span class="comment">    def this(sparkContext: SparkContext, batchDuration: Duration) 辅助构建方法可用（Spark配置信息，批处理持续时间）</span></span><br><span class="line"><span class="comment">    case class Duration (private val millis: Long)样例类 直接用 但是不方便 要自己算毫秒</span></span><br><span class="line"><span class="comment">    new StreamingContext(sparkconf , Duration(1000 * 3))</span></span><br><span class="line"><span class="comment">    所以直接使用伴生对象Seconds()</span></span><br><span class="line"><span class="comment">     object Seconds &#123;</span></span><br><span class="line"><span class="comment">        def apply(seconds: Long): Duration = new Duration(seconds * 1000)</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 初始化SparkStreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkconf , <span class="type">Seconds</span>(<span class="number">3</span>)) <span class="comment">// 创建对象的第二个参数表示数据的采集周期</span></span><br><span class="line">    <span class="comment">//Todo 3.数据处理</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 从数据源采集数据</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 //内存和磁盘有两个序列化副本（socketStream默认的存储级别）</span></span><br><span class="line"><span class="comment">    ReceiverInputDStream[String]  Receiver:接收器 Input:输入  DStream:离散化流   */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> socketDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span> , <span class="number">9999</span> )</span><br><span class="line">    <span class="comment">//TODO 将采集数据进行WordCount的处理</span></span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = socketDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在控制台上打印结果</span></span><br><span class="line"><span class="comment">/*    可以看出底层还是RDD</span></span><br><span class="line"><span class="comment">def print(num: Int): Unit = ssc.withScope &#123;</span></span><br><span class="line"><span class="comment">      def foreachFunc: (RDD[T], Time) =&gt; Unit = &#123;</span></span><br><span class="line"><span class="comment">        (rdd: RDD[T], time: Time) =&gt; &#123;</span></span><br><span class="line"><span class="comment">          val firstNum = rdd.take(num + 1)</span></span><br><span class="line"><span class="comment">          // scalastyle:off println</span></span><br><span class="line"><span class="comment">          println("-------------------------------------------")</span></span><br><span class="line"><span class="comment">          println(s"Time: $time")</span></span><br><span class="line"><span class="comment">          println("-------------------------------------------")</span></span><br><span class="line"><span class="comment">          firstNum.take(num).foreach(println)</span></span><br><span class="line"><span class="comment">          if (firstNum.length &gt; num) println("...")</span></span><br><span class="line"><span class="comment">          println()</span></span><br><span class="line"><span class="comment">          // scalastyle:on println</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Todo 4.开启连接环境</span></span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">    和spark、scala不同的是：最后并不关闭连接环境（除非程序升级或者出现故障的时候，因为数据采集是要7*24）</span></span><br><span class="line"><span class="comment">    并且不能让driver程序结束，需要让driver程序等待,等待数据处理的停止或异常时，才会继续执行</span></span><br><span class="line"><span class="comment">      def awaitTermination() &#123;</span></span><br><span class="line"><span class="comment">        waiter.waitForStopOrError()  //等待停止或者异常，如果没有停止和异常现象 程序不结束</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    ssc.start();</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3) 启动程序并通过netcat发送数据：</p>
<blockquote>
<p>nc -lk 9999</p>
<p>hello red</p>
<p>hello world</p>
</blockquote>
<h2 id="WordCount解析"><a href="#WordCount解析" class="headerlink" title="WordCount解析"></a>WordCount解析</h2><p>Discretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，<strong>DStream是一系列连续的RDD来表示</strong>。<u>每个RDD含有一段时间间隔内的数据（段时间内所有数据在一个RDD里）。</u><img src="/2020/03/11/SparkStreaming/SparkStreaming10.jpg" alt="SparkStreaming10"></p>
<p>对数据的操作也是按照RDD为单位来进行的</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming11.jpg" alt="SparkStreaming11"></p>
<p>计算过程由Spark Engine来完成</p>
<p><img src="/2020/03/11/SparkStreaming/SparkStreaming12.jpg" alt="SparkStreaming12"></p>
<h1 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h1><h2 id="RDD队列"><a href="#RDD队列" class="headerlink" title="RDD队列"></a>RDD队列</h2><h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue()"></a>Queue()</h3><h4 id="用法及说明"><a href="#用法及说明" class="headerlink" title="用法及说明"></a>用法及说明</h4><p><font color="red">测试过程</font>中，可以通过使用ssc.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。</p>
<h4 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h4><p>需求：循环创建几个RDD，将RDD放入队列。通过SparkStream创建Dstream，计算WordCount</p>
<p>1) 编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDStream</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.初始化Spark配置信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDDStream"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.初始化SparkStreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.创建RDD队列</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.创建QueueInputDStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.处理队列中的RDD数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.打印结果</span></span><br><span class="line"></span><br><span class="line">  reducedStream.print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.启动任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">//8.循环创建并向RDD队列中放入RDD</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line"></span><br><span class="line">   rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">   <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2) 结果展示</p>
<blockquote>
<p>-——————————————</p>
<p>Time: 1539075280000 ms</p>
<p>-——————————————</p>
<p>(4,60)</p>
<p>(0,60)</p>
<p>(6,60)</p>
<p>(8,60)</p>
<p>(2,60)</p>
<p>(1,60)</p>
<p>(3,60)</p>
<p>(7,60)</p>
<p>(9,60)</p>
<p>(5,60)</p>
<p>-——————————————</p>
<p>Time: 1539075284000 ms</p>
<p>-——————————————</p>
<p>(4,60)</p>
<p>(0,60)</p>
<p>(6,60)</p>
<p>(8,60)</p>
<p>(2,60)</p>
<p>(1,60)</p>
<p>(3,60)</p>
<p>(7,60)</p>
<p>(9,60)</p>
<p>(5,60)</p>
<p>-——————————————</p>
<p>Time: 1539075288000 ms</p>
<p>-——————————————</p>
<p>(4,30)</p>
<p>(0,30)</p>
<p>(6,30)</p>
<p>(8,30)</p>
<p>(2,30)</p>
<p>(1,30)</p>
<p>(3,30)</p>
<p>(7,30)</p>
<p>(9,30)</p>
<p>(5,30)</p>
<p>-——————————————</p>
<p>Time: 1539075292000 ms</p>
<p>-——————————————</p>
</blockquote>
<h3 id="file"><a href="#file" class="headerlink" title="file()"></a>file()</h3><h4 id="用法及说明-1"><a href="#用法及说明-1" class="headerlink" title="用法及说明"></a>用法及说明</h4><p><font color="red">测试过程</font>中，可以通过使用ssc.textFileStream(“in”)来创建DStream，监控文件夹的变化。</p>
<p>从文件夹中读取新的文件数据（拽过去的文件可能读不到），功能不稳定 ，所以不推荐使用</p>
<p>flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume</p>
<h4 id="案例实操-1"><a href="#案例实操-1" class="headerlink" title="案例实操"></a>案例实操</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming03_DStream_File</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 数据处理</span></span><br><span class="line">    <span class="comment">// 从文件夹中读取新的文件数据，功能不稳定 ，所以不推荐使用</span></span><br><span class="line">    <span class="comment">// flume更加专业，所以生产环境，监控文件或目录的变化，采集数据都使用flume</span></span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">DStream</span>[<span class="type">String</span>] = ssc.textFileStream(<span class="string">"in"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map( (_, <span class="number">1</span>) )</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭连接环境</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>






<h2 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h2><h3 id="用法及说明-2"><a href="#用法及说明-2" class="headerlink" title="用法及说明"></a>用法及说明</h3><p>需要继承Receiver，并实现onStart、onStop方法来自定义数据源采集。</p>
<h3 id="案例实操-2"><a href="#案例实操-2" class="headerlink" title="案例实操"></a>案例实操</h3><p>需求：自定义数据源，实现监控某个端口号，获取该端口号内容。</p>
<p>1) 自定义数据源</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  自定义数据采集器</span></span><br><span class="line"><span class="comment">    自定义数据采集器</span></span><br><span class="line"><span class="comment">    模仿spark自带的socket采集器</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  abstract class Receiver[T](val storageLevel: StorageLevel) extends Serializable</span></span><br><span class="line"><span class="comment">  StorageLevel :存储级别 MEMORY_ONLY DISK_ONLY MEMORY_AND_DISK</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  步骤： 1. 继承Receiver ,设定泛型（采集数据的类型）, 传递参数</span></span><br><span class="line"><span class="comment">         2. 重写方法</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyRecevier</span>(<span class="params">host : <span class="type">String</span> , port : <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>)</span>&#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   socketTextStream:  socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)</span></span><br><span class="line"><span class="comment">    socketStream：  new SocketInputDStream[T](this, hostname, port, converter, storageLevel)</span></span><br><span class="line"><span class="comment">    SocketInputDStream:  new SocketReceiver(host, port, bytesToObjects, storageLevel)</span></span><br><span class="line"><span class="comment">    SocketReceiver：extends Receiver[T](storageLevel) with Logging</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> socket: <span class="type">Socket</span> = _</span><br><span class="line">    <span class="comment">// SocketReceiver：socket.getInputStream()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">      <span class="comment">//我们需要字符串，所以将字节流转换为缓冲字符流</span></span><br><span class="line">      <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">InputStreamReader</span>(</span><br><span class="line">          <span class="comment">//获取从网络中传递来的数据（字节流）</span></span><br><span class="line">          socket.getInputStream,</span><br><span class="line">          <span class="string">"UTF-8"</span></span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      <span class="keyword">var</span> s:<span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">     <span class="comment">/*</span></span><br><span class="line"><span class="comment">      这个“s = reader.readLine())!= null”语句是错误的，因为在网络编程中获取的数据是没有null的概念</span></span><br><span class="line"><span class="comment">      文件读取时，如果读到结束的时候，获取的结果为null（文件读取这样是对的）</span></span><br><span class="line"><span class="comment">      但是在网络中我可以现在传递一些数据  过一段就再传一次，所以null是无法判断的</span></span><br><span class="line"><span class="comment">      网络编程中，需要明确告知服务器，客户端不再传数据，需要发送特殊的指令</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      <span class="keyword">while</span>(( s = reader.readLine())!= <span class="literal">null</span>)&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需要发送特殊的指令</span></span><br><span class="line">        <span class="keyword">if</span>(s != <span class="string">"-END-"</span>)&#123;</span><br><span class="line">          <span class="comment">//采集到数据后，进行封装(存储)</span></span><br><span class="line">              store(s)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          <span class="comment">// stop</span></span><br><span class="line">          <span class="comment">// close</span></span><br><span class="line">          <span class="comment">// 重启</span></span><br><span class="line">          <span class="comment">//restart("")</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 启动采集器</span></span><br><span class="line">    <span class="comment">// 采集 &amp; 封装</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      socket = <span class="keyword">new</span> <span class="type">Socket</span>(host , port)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"Socket Receiver"</span>) &#123;</span><br><span class="line">        setDaemon(<span class="literal">true</span>)<span class="comment">//守护线程</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">          receive() &#125;</span><br><span class="line">      &#125;.start()<span class="comment">//start()会回调run()方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (socket != <span class="literal">null</span>)&#123;</span><br><span class="line">        socket.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>2) 使用自定义的数据源采集数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamig_DIY</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span>  <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"DIY采集器"</span>)</span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// TODO 数据处理</span></span><br><span class="line">    <span class="comment">// 自定义数据采集器</span></span><br><span class="line">    <span class="keyword">val</span> myDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.receiverStream(<span class="keyword">new</span> <span class="type">MyRecevier</span>(<span class="string">"localhost"</span> , <span class="number">9999</span>))</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">DStream</span>[<span class="type">String</span>] = myDS.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToOneDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordDS.map((_ , <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordToCountDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOneDS.reduceByKey(_+_)</span><br><span class="line">    wordToCountDS.print()</span><br><span class="line">    <span class="comment">// TODO 开启连接环境</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h2 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h2><h3 id="版本选型"><a href="#版本选型" class="headerlink" title="版本选型"></a>版本选型</h3><p><strong>ReceiverAPI</strong>：需要一个专门的Executor去接收数据，然后发送给其他的Executor做计算。存在的问题，接收数据的Executor和计算的Executor速度会有所不同，特别在接收数据的Executor速度大于计算的Executor速度，会导致计算数据的节点内存溢出。</p>
<p><strong>DirectAPI</strong>：是由计算的Executor来主动消费Kafka的数据，速度由自身控制。</p>
<p> <img src="/2020/03/11/SparkStreaming/SparkStreaming13.jpg" alt="SparkStreaming13"></p>
<h3 id="Kafka-0-8-Receiver模式"><a href="#Kafka-0-8-Receiver模式" class="headerlink" title="Kafka 0-8 Receiver模式"></a>Kafka 0-8 Receiver模式</h3><p>1） 需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.red.kafka</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Kafka</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span>  <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"DIY采集器"</span>)</span><br><span class="line">    <span class="comment">// TODO 环境对象</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf , <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// TODO 数据处理 - 读取Kafka数据创建DStream(基于Receive方式)</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    def createStream(</span></span><br><span class="line"><span class="comment">                      ssc : org.apache.spark.streaming.StreamingContext,</span></span><br><span class="line"><span class="comment">                      zkQuorum : scala.Predef.String, //zookeeper</span></span><br><span class="line"><span class="comment">                      groupId : scala.Predef.String, //消费者组</span></span><br><span class="line"><span class="comment">                      topics : scala.Predef.Map[scala.Predef.String, scala.Int],//分区数</span></span><br><span class="line"><span class="comment">                      storageLevel : org.apache.spark.storage.StorageLevel = &#123;  compiled code  &#125;</span></span><br><span class="line"><span class="comment">                    ) : org.apache.spark.streaming.dstream.ReceiverInputDStream[scala.Tuple2[scala.Predef.String, scala.Predef.String]] = &#123;  compiled code  &#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaDS: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="string">"linux1:2181,linux2:2181,linux3:2181"</span>,</span><br><span class="line">      <span class="string">"red0819"</span>,</span><br><span class="line">      <span class="type">Map</span>(<span class="string">"red0819"</span> -&gt; <span class="number">3</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// Kafka消息传递的时候以k-v对</span></span><br><span class="line">    <span class="comment">// k - 传值的时候提供的，默认为null,主要用于分区</span></span><br><span class="line">    <span class="comment">// v - message</span></span><br><span class="line">    kafkaDS.map((_._2)).print()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Kafka-0-8-Direct模式"><a href="#Kafka-0-8-Direct模式" class="headerlink" title="Kafka 0-8 Direct模式"></a>Kafka 0-8 Direct模式</h3><p>1）需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<h4 id="自动维护offset"><a href="#自动维护offset" class="headerlink" title="自动维护offset"></a>自动维护offset</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIAuto02</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> getSSC1: () =&gt; <span class="type">StreamingContext</span> = () =&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">getSSC</span></span>: <span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//设置CK</span></span><br><span class="line"></span><br><span class="line">  ssc.checkpoint(<span class="string">"./ck2"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.定义Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.读取Kafka数据</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc,</span><br><span class="line"></span><br><span class="line">   kafkaPara,</span><br><span class="line"></span><br><span class="line">   <span class="type">Set</span>(<span class="string">"red"</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.计算WordCount</span></span><br><span class="line"></span><br><span class="line">  kafkaDStream.map(_._2)</span><br><span class="line"></span><br><span class="line">   .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">   .print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.返回数据</span></span><br><span class="line"></span><br><span class="line">  ssc</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取SSC</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck2"</span>, () =&gt; getSSC)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="手动维护offset"><a href="#手动维护offset" class="headerlink" title="手动维护offset"></a>手动维护offset</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.common.<span class="type">TopicAndPartition</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.message.<span class="type">MessageAndMetadata</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.&#123;<span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">OffsetRange</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIHandler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.获取上一次启动最后保留的Offset=&gt;getOffset(MySQL)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>] = <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>](<span class="type">TopicAndPartition</span>(<span class="string">"red"</span>, <span class="number">0</span>) -&gt; <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.读取Kafka数据创建DStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">String</span>] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>, <span class="type">String</span>](ssc,</span><br><span class="line"></span><br><span class="line">   kafkaPara,</span><br><span class="line"></span><br><span class="line">   fromOffsets,</span><br><span class="line"></span><br><span class="line">   (m: <span class="type">MessageAndMetadata</span>[<span class="type">String</span>, <span class="type">String</span>]) =&gt; m.message())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.创建一个数组用于存放当前消费数据的offset信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> offsetRanges = <span class="type">Array</span>.empty[<span class="type">OffsetRange</span>]</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.获取当前消费数据的offset信息</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordToCountDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = kafkaDStream.transform &#123; rdd =&gt;</span><br><span class="line"></span><br><span class="line">   offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">   rdd</span><br><span class="line"></span><br><span class="line">  &#125;.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//8.打印Offset信息</span></span><br><span class="line"></span><br><span class="line">  wordToCountDStream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span>:<span class="subst">$&#123;o.partition&#125;</span>:<span class="subst">$&#123;o.fromOffset&#125;</span>:<span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//9.开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Kafka-0-10-Direct模式"><a href="#Kafka-0-10-Direct模式" class="headerlink" title="Kafka 0-10 Direct模式"></a>Kafka 0-10 Direct模式</h3><p>1）需求：通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算，最终打印到控制台。</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPI</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//1.创建SparkConf</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.创建StreamingContext</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//3.定义Kafka参数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"red"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line"></span><br><span class="line">   <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line"></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//4.读取Kafka数据创建DStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line"></span><br><span class="line">   <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line"></span><br><span class="line">   <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"red"</span>), kafkaPara))</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//5.将每条消息的KV取出</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> valueDStream: <span class="type">DStream</span>[<span class="type">String</span>] = kafkaDStream.map(record =&gt; record.value())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//6.计算WordCount</span></span><br><span class="line"></span><br><span class="line">  valueDStream.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">   .map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">   .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">   .print()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="comment">//7.开启任务</span></span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line"></span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="消费Kafka数据模式总结"><a href="#消费Kafka数据模式总结" class="headerlink" title="消费Kafka数据模式总结"></a>消费Kafka数据模式总结</h3><h4 id="0-8"><a href="#0-8" class="headerlink" title="0-8"></a>0-8</h4><h5 id="ReceiverAPI"><a href="#ReceiverAPI" class="headerlink" title="ReceiverAPI"></a>ReceiverAPI</h5><p>1) 专门的Executor读取数据，速度不统一</p>
<p>  数据丢失：预写日志开启</p>
<p>2) 跨机器传输数据</p>
<p>3) Executor读取数据通过多个线程的方式，想要增加并行度，则需要多个流union</p>
<p>4) offset存储在zookeeper中</p>
<h5 id="DirectAPI"><a href="#DirectAPI" class="headerlink" title="DirectAPI"></a>DirectAPI</h5><p>1) Executor读取数据并计算</p>
<p>2) 增加Executor个数来增加消费的并行度</p>
<p>3) offset存储</p>
<p>a. CheckPoint(getActiveOrCreate方式创建StreamingContext)</p>
<p>从checkpoint中读取数据偏移量（不推荐使用）</p>
<blockquote>
<p>理由：</p>
<p>​        checkpoint还保存了计算逻辑，不适合扩展功能<br>​                checkpoint会延续计算，但是可能会压垮内存<br>​                checkpoint一般的存储路径为HDFS，所以会导致小文件过多</p>
</blockquote>
<p>b. 手动维护(有事务的存储系统)</p>
<p>4) 获取offset必须在第一个调用的算子中：</p>
<p>offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</p>
<h4 id="0-10-DirectAPI"><a href="#0-10-DirectAPI" class="headerlink" title="0-10 DirectAPI"></a>0-10 DirectAPI</h4><p>1) Executor读取数据并计算</p>
<p>2) 增加Executor个数来增加消费的并行度</p>
<p>3) offset存储</p>
<p>a. __consumer_offsets系统主题中</p>
<p>b. 手动维护(有事务的存储系统)</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Snow Monster</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.red0819.top/bigdata/SparkStreaming/">http://www.red0819.top/bigdata/SparkStreaming/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.red0819.top" target="_blank">RedLeavesBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/SparkStreaming/">SparkStreaming</a></div><div class="post_share"><div class="social-share" data-image="/wordPic/spark-logo.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/bigdata/Oozie%E7%AE%80%E4%BB%8B/"><img class="prev-cover" src="/wordPic/oozie-logo.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Oozie简介</div></div></a></div><div class="next-post pull-right"><a href="/bigdata/SparkStreaming-2/"><img class="next-cover" src="/wordPic/spark-logo.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">SparkStreaming-2</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/bigdata/SparkStreaming-2/" title="SparkStreaming-2"><img class="cover" src="/wordPic/spark-logo.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-11</div><div class="title">SparkStreaming-2</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer" style="background-image: url(/wordPic/spark-logo.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2017 - 2020 By Snow Monster</div><div class="framework-info"><span>框架 </span><a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener">Butterfly</a></div><div class="footer_custom_text">努力充实自己大脑中，欢迎投喂知识！</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const initData = {
      el: '#vcomment',
      appId: 'BlwHhRwsSAJShGHGJeX8d6qB-gzGzoHsz',
      appKey: 'fGlkNKWjBKmGkMSND7jvpsRI',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }

    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script></div></body></html>